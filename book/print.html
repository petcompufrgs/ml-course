<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Curso de Machine Learning</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="capa.html">Capa</a></li><li class="chapter-item expanded affix "><a href="agradecimentos.html">Agradecimentos</a></li><li class="chapter-item expanded "><a href="introducao.html"><strong aria-hidden="true">1.</strong> Introdução</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-1/1/1-1.html"><strong aria-hidden="true">1.1.</strong> Visão Geral</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-1/1/1-1-1.html"><strong aria-hidden="true">1.1.1.</strong> O que é Machine Learning?</a></li><li class="chapter-item expanded "><a href="parte-1/1/1-1-2.html"><strong aria-hidden="true">1.1.2.</strong> O que é inteligência?</a></li><li class="chapter-item expanded "><a href="parte-1/1/1-1-3.html"><strong aria-hidden="true">1.1.3.</strong> Supervised Learning</a></li><li class="chapter-item expanded "><a href="parte-1/1/1-1-4.html"><strong aria-hidden="true">1.1.4.</strong> Unsupervised Learning</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="supervised-learning.html"><strong aria-hidden="true">2.</strong> Supervised Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/1/2-1.html"><strong aria-hidden="true">2.1.</strong> Representação de modelos</a></li><li class="chapter-item expanded "><a href="parte-2/2/2-2.html"><strong aria-hidden="true">2.2.</strong> Regressão linear</a></li><li class="chapter-item expanded "><a href="parte-2/3/2-3.html"><strong aria-hidden="true">2.3.</strong> Função Custo (Cost Function)</a></li><li class="chapter-item expanded "><a href="parte-2/4/2-4.html"><strong aria-hidden="true">2.4.</strong> Gradiente Descendente (Gradient Descent)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/4/2-4-1.html"><strong aria-hidden="true">2.4.1.</strong> Algoritmo Gradiente Descendente</a></li><li class="chapter-item expanded "><a href="parte-2/4/2-4-2.html"><strong aria-hidden="true">2.4.2.</strong> Gradiente Descendente para Regressão Linear</a></li></ol></li><li class="chapter-item expanded "><a href="parte-2/5/2-5.html"><strong aria-hidden="true">2.5.</strong> Álgebra Linear</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/5/2-5-1.html"><strong aria-hidden="true">2.5.1.</strong> Matrizes e vetores</a></li><li class="chapter-item expanded "><a href="parte-2/5/2-5-2.html"><strong aria-hidden="true">2.5.2.</strong> Operações básicas com matrizes e vetores</a></li><li class="chapter-item expanded "><a href="parte-2/5/2-5-3.html"><strong aria-hidden="true">2.5.3.</strong> Multipĺicação entre matrizes e vetores</a></li><li class="chapter-item expanded "><a href="parte-2/5/2-5-4.html"><strong aria-hidden="true">2.5.4.</strong> Multiplicação entre duas matrizes</a></li><li class="chapter-item expanded "><a href="parte-2/5/2-5-5.html"><strong aria-hidden="true">2.5.5.</strong> Matriz identidade, inversa e transposta</a></li></ol></li><li class="chapter-item expanded "><a href="parte-2/6/2-6.html"><strong aria-hidden="true">2.6.</strong> Regressão Linear com múltiplas variáveis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/6/2-6-1.html"><strong aria-hidden="true">2.6.1.</strong> Múltiplos parâmetros de entrada</a></li><li class="chapter-item expanded "><a href="parte-2/6/2-6-2.html"><strong aria-hidden="true">2.6.2.</strong> Gradiente Descendente com múltiplas variáveis</a></li></ol></li><li class="chapter-item expanded "><a href="parte-2/7/2-7.html"><strong aria-hidden="true">2.7.</strong> Equação Normal (Normal Equation)</a></li><li class="chapter-item expanded "><a href="parte-2/8/2-8.html"><strong aria-hidden="true">2.8.</strong> Classificação</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/8/2-8-1.html"><strong aria-hidden="true">2.8.1.</strong> Problemas de calssificação</a></li><li class="chapter-item expanded "><a href="parte-2/8/2-8-2.html"><strong aria-hidden="true">2.8.2.</strong> Representação da hipótese</a></li><li class="chapter-item expanded "><a href="parte-2/8/2-8-3.html"><strong aria-hidden="true">2.8.3.</strong> Limites de decisão (Decision Boundary)</a></li><li class="chapter-item expanded "><a href="parte-2/8/2-8-4.html"><strong aria-hidden="true">2.8.4.</strong> Classificação Multiclasse</a></li></ol></li><li class="chapter-item expanded "><a href="parte-2/9/2-9.html"><strong aria-hidden="true">2.9.</strong> Regressão Logística (Logistic Regression)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/9/2-9-1.html"><strong aria-hidden="true">2.9.1.</strong> Função Custo (Cost Function)</a></li><li class="chapter-item expanded "><a href="parte-2/9/2-9-2.html"><strong aria-hidden="true">2.9.2.</strong> Gradiente Descendente para Regressão Logística</a></li></ol></li><li class="chapter-item expanded "><a href="parte-2/10/2-10.html"><strong aria-hidden="true">2.10.</strong> Underfitting e overfitting</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-2/10/2-10-1.html"><strong aria-hidden="true">2.10.1.</strong> Função custo em casos de overfitting</a></li><li class="chapter-item expanded "><a href="parte-2/10/2-10-2.html"><strong aria-hidden="true">2.10.2.</strong> Regressão linear regularizada</a></li><li class="chapter-item expanded "><a href="parte-2/10/2-10-3.html"><strong aria-hidden="true">2.10.3.</strong> Regressão logística regularizada</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="redes-neurais.html"><strong aria-hidden="true">3.</strong> Redes Neurais</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/1/3-1.html"><strong aria-hidden="true">3.1.</strong> Redes Neurais: Representação</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/1/3-1-1.html"><strong aria-hidden="true">3.1.1.</strong> Definição básica</a></li><li class="chapter-item expanded "><a href="parte-3/1/3-1-2.html"><strong aria-hidden="true">3.1.2.</strong> Hipótese não-linear</a></li><li class="chapter-item expanded "><a href="parte-3/1/3-1-3.html"><strong aria-hidden="true">3.1.3.</strong> Os Neurônios e o Cérebro</a></li><li class="chapter-item expanded "><a href="parte-3/1/3-1-4.html"><strong aria-hidden="true">3.1.4.</strong> Representação do modelo</a></li><li class="chapter-item expanded "><a href="parte-3/1/3-1-5.html"><strong aria-hidden="true">3.1.5.</strong> Aplicações</a></li><li class="chapter-item expanded "><a href="parte-3/1/3-1-6.html"><strong aria-hidden="true">3.1.6.</strong> Classificação Multiclasse</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/2/3-2.html"><strong aria-hidden="true">3.2.</strong> Redes Neurais: Aprendizado</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/2/3-2-1.html"><strong aria-hidden="true">3.2.1.</strong> Estrutura básica</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-1.html"><strong aria-hidden="true">3.2.1.1.</strong> Camada linear</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-2.html"><strong aria-hidden="true">3.2.1.2.</strong> Camada de ativação: Sigmoid layer</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-3.html"><strong aria-hidden="true">3.2.1.3.</strong> Cross entropy loss</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-4.html"><strong aria-hidden="true">3.2.1.4.</strong> Camada de ativação: Softmax layer</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-5.html"><strong aria-hidden="true">3.2.1.5.</strong> Camada de ativação: Rectified Linear layer (ReLU)</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-1-6.html"><strong aria-hidden="true">3.2.1.6.</strong> Camada de ativação: Hyperbolic tangent</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-2.html"><strong aria-hidden="true">3.2.2.</strong> Função Custo (Cost Function)</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-3.html"><strong aria-hidden="true">3.2.3.</strong> Backpropagation Algorithm</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/2/3-2-3-1.html"><strong aria-hidden="true">3.2.3.1.</strong> Forward pass</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-3-2.html"><strong aria-hidden="true">3.2.3.2.</strong> Backward pass</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-3-3.html"><strong aria-hidden="true">3.2.3.3.</strong> Algoritmo</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-4.html"><strong aria-hidden="true">3.2.4.</strong> Otimizadores</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-5.html"><strong aria-hidden="true">3.2.5.</strong> Verificação do gradiente</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-6.html"><strong aria-hidden="true">3.2.6.</strong> Inicialização aleatória</a></li><li class="chapter-item expanded "><a href="parte-3/2/3-2-7.html"><strong aria-hidden="true">3.2.7.</strong> Organização do conhecimento</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/3/3-3.html"><strong aria-hidden="true">3.3.</strong> Aplicação de algoritmos de machine learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/3/3-3-1.html"><strong aria-hidden="true">3.3.1.</strong> Valoração de algoritmos de aprendizagem</a></li><li class="chapter-item expanded "><a href="parte-3/3/3-3-2.html"><strong aria-hidden="true">3.3.2.</strong> Curvas de aprendizado</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-3/3/3-3-2-1.html"><strong aria-hidden="true">3.3.2.1.</strong> High Bias</a></li><li class="chapter-item expanded "><a href="parte-3/3/3-3-2-2.html"><strong aria-hidden="true">3.3.2.2.</strong> High Variance</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/3/3-3-3.html"><strong aria-hidden="true">3.3.3.</strong> Decisões a serem tomadas</a></li><li class="chapter-item expanded "><a href="parte-3/3/3-3-4.html"><strong aria-hidden="true">3.3.4.</strong> Diagnosticando Redes Neurais</a></li><li class="chapter-item expanded "><a href="parte-3/3/3-3-5.html"><strong aria-hidden="true">3.3.5.</strong> Support Vector Machines (SVMs)</a></li></ol></li><li class="chapter-item expanded "><a href="parte-3/4/3-4.html"><strong aria-hidden="true">3.4.</strong> Naive Bayes</a></li></ol></li><li class="chapter-item expanded "><a href="unsupervised-learning.html"><strong aria-hidden="true">4.</strong> Unsupervised Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/1/4-1.html"><strong aria-hidden="true">4.1.</strong> Clustering</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/1/4-1-1.html"><strong aria-hidden="true">4.1.1.</strong> K-Means Algorithm</a></li><li class="chapter-item expanded "><a href="parte-4/1/4-1-2.html"><strong aria-hidden="true">4.1.2.</strong> Otimização</a></li><li class="chapter-item expanded "><a href="parte-4/1/4-1-3.html"><strong aria-hidden="true">4.1.3.</strong> Inicialização</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/2/4-2.html"><strong aria-hidden="true">4.2.</strong> K-Nearest Neighbors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/2/4-2-1.html"><strong aria-hidden="true">4.2.1.</strong> Algoritmo</a></li><li class="chapter-item expanded "><a href="parte-4/2/4-2-2.html"><strong aria-hidden="true">4.2.2.</strong> Escolhendo o valor correto para K</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/3/4-3.html"><strong aria-hidden="true">4.3.</strong> Redução de dimensionalidade</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/3/4-3-1.html"><strong aria-hidden="true">4.3.1.</strong> Compressão de dados</a></li><li class="chapter-item expanded "><a href="parte-4/3/4-3-2.html"><strong aria-hidden="true">4.3.2.</strong> Visualização</a></li><li class="chapter-item expanded "><a href="parte-4/3/4-3-3.html"><strong aria-hidden="true">4.3.3.</strong> Ánalise do componente principal (PCA)</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/4/4-4.html"><strong aria-hidden="true">4.4.</strong> Aprendizado por reforço (Reinforcement learning)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/4/4-4-1.html"><strong aria-hidden="true">4.4.1.</strong> Visão geral</a></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-2.html"><strong aria-hidden="true">4.4.2.</strong> Exploration e Exploitation</a></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-3.html"><strong aria-hidden="true">4.4.3.</strong> Markov Process</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/4/4-4-3-1.html"><strong aria-hidden="true">4.4.3.1.</strong> Propriedade de Markov</a></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-3-2.html"><strong aria-hidden="true">4.4.3.2.</strong> Cadeia de Markov</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-4.html"><strong aria-hidden="true">4.4.4.</strong> Markov Decision Process (MDPs)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/4/4-4-4-1.html"><strong aria-hidden="true">4.4.4.1.</strong> Busca pela política ótima com MDP</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-5.html"><strong aria-hidden="true">4.4.5.</strong> Monte-Carlo e Temporal-Difference Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-4/4/4-4-5-1.html"><strong aria-hidden="true">4.4.5.1.</strong> Valoração de Monte-Carlo</a></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-5-2.html"><strong aria-hidden="true">4.4.5.2.</strong> TD Learning</a></li></ol></li><li class="chapter-item expanded "><a href="parte-4/4/4-4-6.html"><strong aria-hidden="true">4.4.6.</strong> Q-Learning</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="otimizacoes-no-aprendizado.html"><strong aria-hidden="true">5.</strong> Otimizações no Aprendizado</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-5/1/5-1.html"><strong aria-hidden="true">5.1.</strong> Detecção de anomalias</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-5/1/5-1-1.html"><strong aria-hidden="true">5.1.1.</strong> Motivação</a></li><li class="chapter-item expanded "><a href="parte-5/1/5-1-2.html"><strong aria-hidden="true">5.1.2.</strong> Distribuição Gaussiana</a></li><li class="chapter-item expanded "><a href="parte-5/1/5-1-3.html"><strong aria-hidden="true">5.1.3.</strong> Algoritmo</a></li></ol></li><li class="chapter-item expanded "><a href="parte-5/2/5-2.html"><strong aria-hidden="true">5.2.</strong> Aprendizado em larga escala</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-5/2/5-2-1.html"><strong aria-hidden="true">5.2.1.</strong> Batch Normalization</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-2.html"><strong aria-hidden="true">5.2.2.</strong> Stochastic Gradient Descent (SGD)</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-3.html"><strong aria-hidden="true">5.2.3.</strong> Mini-batch Gradient Descent</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-4.html"><strong aria-hidden="true">5.2.4.</strong> SGD + Momentum</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-5.html"><strong aria-hidden="true">5.2.5.</strong> Nesterov Accelerated Gradient (NAG)</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-6.html"><strong aria-hidden="true">5.2.6.</strong> AdaGrad</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-7.html"><strong aria-hidden="true">5.2.7.</strong> Adam</a></li><li class="chapter-item expanded "><a href="parte-5/2/5-2-8.html"><strong aria-hidden="true">5.2.8.</strong> Mapreduce e Paralelismo de Dados</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="topicos-avancados.html"><strong aria-hidden="true">6.</strong> Tópicos Avançados</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/1/6-1.html"><strong aria-hidden="true">6.1.</strong> Redes neurais convolucionais (Convolutional neural networks)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/1/6-1-1.html"><strong aria-hidden="true">6.1.1.</strong> Visão geral</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-2.html"><strong aria-hidden="true">6.1.2.</strong> Camadas de uma ConvNet</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-3.html"><strong aria-hidden="true">6.1.3.</strong> Técnicas de otimização de treino</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/1/6-1-3-1.html"><strong aria-hidden="true">6.1.3.1.</strong> Data Augmentation</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-3-2.html"><strong aria-hidden="true">6.1.3.2.</strong> Modelos pré-treinados</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-3-3.html"><strong aria-hidden="true">6.1.3.3.</strong> Fine Tuning</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-4.html"><strong aria-hidden="true">6.1.4.</strong> Detecção e Segmentação</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/1/6-1-4-1.html"><strong aria-hidden="true">6.1.4.1.</strong> Segmentação Semântica</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-4-2.html"><strong aria-hidden="true">6.1.4.2.</strong> Localização</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-4-3.html"><strong aria-hidden="true">6.1.4.3.</strong> Detecção de objetos</a></li><li class="chapter-item expanded "><a href="parte-6/1/6-1-4-4.html"><strong aria-hidden="true">6.1.4.4.</strong> Segmentação de instâncias</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="parte-6/2/6-2.html"><strong aria-hidden="true">6.2.</strong> Redes neurais recorrentes (Recurrent neural networks)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/2/6-2-1.html"><strong aria-hidden="true">6.2.1.</strong> Visão geral</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-2.html"><strong aria-hidden="true">6.2.2.</strong> Vetores de palavras (word embeddings)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/2/6-2-2-1.html"><strong aria-hidden="true">6.2.2.1.</strong> One-hot encoding</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-2-2.html"><strong aria-hidden="true">6.2.2.2.</strong> Word2Vec</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-3.html"><strong aria-hidden="true">6.2.3.</strong> Arquitetura de uma RNN</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-4.html"><strong aria-hidden="true">6.2.4.</strong> Aplicações</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/2/6-2-4-1.html"><strong aria-hidden="true">6.2.4.1.</strong> One-to-one</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-4-2.html"><strong aria-hidden="true">6.2.4.2.</strong> One-to-many</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-4-3.html"><strong aria-hidden="true">6.2.4.3.</strong> Many-to-one</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-4-4.html"><strong aria-hidden="true">6.2.4.4.</strong> Many-to-many</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-5.html"><strong aria-hidden="true">6.2.5.</strong> Função Custo (Cost Function)</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-6.html"><strong aria-hidden="true">6.2.6.</strong> Backpropagation</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-7.html"><strong aria-hidden="true">6.2.7.</strong> Função de ativação e propriedades</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-8.html"><strong aria-hidden="true">6.2.8.</strong> Gradiente de desaparecimento e explosão</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-9.html"><strong aria-hidden="true">6.2.9.</strong> Long Short-Term Memory (LSTM)</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-10.html"><strong aria-hidden="true">6.2.10.</strong> Bidirectional &amp; Multi-layer RNNs</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-11.html"><strong aria-hidden="true">6.2.11.</strong> Attention</a></li><li class="chapter-item expanded "><a href="parte-6/2/6-2-12.html"><strong aria-hidden="true">6.2.12.</strong> Redes neurais convolucionais para NLP</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/3/6-3.html"><strong aria-hidden="true">6.3.</strong> Transformers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/3/6-3-1.html"><strong aria-hidden="true">6.3.1.</strong> Positional encoding</a></li><li class="chapter-item expanded "><a href="parte-6/3/6-3-2.html"><strong aria-hidden="true">6.3.2.</strong> Encoder</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/3/6-3-2-1.html"><strong aria-hidden="true">6.3.2.1.</strong> Self-Attention</a></li><li class="chapter-item expanded "><a href="parte-6/3/6-3-2-2.html"><strong aria-hidden="true">6.3.2.2.</strong> Multi-head attention e geração de saída</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/3/6-3-3.html"><strong aria-hidden="true">6.3.3.</strong> Decoder</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/3/6-3-3-1.html"><strong aria-hidden="true">6.3.3.1.</strong> Masked Multi-head attention</a></li><li class="chapter-item expanded "><a href="parte-6/3/6-3-3-2.html"><strong aria-hidden="true">6.3.3.2.</strong> Encoder-Decoder attention</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="parte-6/4/6-4.html"><strong aria-hidden="true">6.4.</strong> Modelos generativos (Generative models)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/4/6-4-1.html"><strong aria-hidden="true">6.4.1.</strong> Visão geral</a></li><li class="chapter-item expanded "><a href="parte-6/4/6-4-2.html"><strong aria-hidden="true">6.4.2.</strong> Aplicações</a></li><li class="chapter-item expanded "><a href="parte-6/4/6-4-3.html"><strong aria-hidden="true">6.4.3.</strong> Auto-Regressive Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/4/6-4-3-1.html"><strong aria-hidden="true">6.4.3.1.</strong> PixelRNN</a></li><li class="chapter-item expanded "><a href="parte-6/4/6-4-3-2.html"><strong aria-hidden="true">6.4.3.2.</strong> PixelCNN</a></li></ol></li><li class="chapter-item expanded "><a href="parte-6/4/6-4-4.html"><strong aria-hidden="true">6.4.4.</strong> Variational Autoencoders (VAE)</a></li><li class="chapter-item expanded "><a href="parte-6/4/6-4-5.html"><strong aria-hidden="true">6.4.5.</strong> Generative Adversarial Networks (GANs)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/4/6-4-5-1.html"><strong aria-hidden="true">6.4.5.1.</strong> Treinamento: Jogo de dois jogadores</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="parte-6/5/6-5.html"><strong aria-hidden="true">6.5.</strong> Deep Q-Learning</a></li><li class="chapter-item expanded "><a href="parte-6/6/6-6.html"><strong aria-hidden="true">6.6.</strong> Busca de Monte Carlo</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parte-6/6/6-6-1.html"><strong aria-hidden="true">6.6.1.</strong> Uninformed seaarch</a></li><li class="chapter-item expanded "><a href="parte-6/6/6-6-2.html"><strong aria-hidden="true">6.6.2.</strong> Monte Carlo Tree Search</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="referencias.html">Referências</a></li><li class="chapter-item expanded affix "><a href="contatos.html">Contatos</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Curso de Machine Learning</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="curso-de-machine-learning"><a class="header" href="#curso-de-machine-learning">CURSO DE MACHINE LEARNING</a></h1>
<h3 id="baseado-em"><a class="header" href="#baseado-em">Baseado em:</a></h3>
<p>Machine Learning (Coursera) | Stanford University <a href="./referencias.html">[14]</a></p>
<p>Deep Learning Lecture Series 2020 | DeepMind &amp; University College London <a href="./referencias.html">[20]</a></p>
<p>Reinforcement Learning Course | DeepMind &amp; University College London <a href="./referencias.html">[11]</a></p>
<p>CS229: Machine Learning | Stanford University <a href="./referencias.html">[13]</a></p>
<p>CS230: Deep Learning | Stanford University <a href="./referencias.html">[2]</a></p>
<p>CS231n: Convolutional Neural Networks for Visual Recognition | Stanford University <a href="./referencias.html">[7]</a></p>
<p>CS224n: Natural Language Processing with Deep Learning | Stanford University <a href="./referencias.html">[6]</a></p>
<p>CS234: Reinforcement Learning | Stanford University <a href="./referencias.html">[5]</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="agradecimentos"><a class="header" href="#agradecimentos">Agradecimentos</a></h1>
<p>Agradeço aos meus colegas do PET Computação UFRGS, em especial Bernardo Beneduzi Borba,
Eduardo Fantini, Victoria Avelar Duarte e Vitor Caruso Rodrigues Ferrer pelo apoio, incentivo, revi-
sões e ideias para a construção deste curso.</p>
<p>Agradeço à tutora do PET Computação UFRGS, Érika Fernandes Cota, pelo apoio, conhecimento
e paciência, tornando possível este projeto. Agradeço, também, aos colegas do Instituto De Infor-
mática, em especial, ao Gabriel Couto Domingues e Garrenlus de Souza pelas críticas, revisão e
incentivo.</p>
<p>Agradeço à Rosália Galiazzi Schneider e ao Leonardo Piletti Chatain pela mentoria, conhecimento,
apoio, críticas, sugestões e incentivos.</p>
<p>Por final, quero agradecer à minha família, à minha namorada Raya Allawy, ao meu amigo Arthur
Carvalho Balejo pelo apoio e incentivo, e a todos que, com o tempo depositado, críticas e incentivo,
tornaram possível a realização deste projeto o qual, com certeza, será muito importante para o
desenvolvimento acadêmico de futuros estudantes com interesse no campo de Inteligência Artificial.</p>
<p align="right">
- <i>Thiago Sotoriva Lermen</i>
</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-i"><a class="header" href="#parte-i">Parte I</a></h2>
<h1 id="introdução"><a class="header" href="#introdução">Introdução</a></h1>
<ul>
<li>
<p><a href="./parte-1/1/1-1.html">Visão Geral</a></p>
<ul>
<li>
<p><a href="./parte-1/1/1-1-1.html">O que é Machine Learning?</a></p>
</li>
<li>
<p><a href="./parte-1/1/1-1-2.html">O que é inteligência?</a></p>
</li>
<li>
<p><a href="./parte-1/1/1-1-3.html">Supervised Learning</a></p>
</li>
<li>
<p><a href="./parte-1/1/1-1-4.html">Unsupervised Learning</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visão-geral"><a class="header" href="#visão-geral">Visão geral</a></h1>
<p>Nesta seção serão introduzidos os conceitos básicos para fazer uma máquina aprender usando dados,
sem, necessariamente, ser explicitamente programada.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="o-que-é-machine-learning"><a class="header" href="#o-que-é-machine-learning">O que é <em>Machine Learning</em>?</a></h1>
<p><em>Machine Learning (M.L.)</em> é um campo de estudo da área de Inteligência Artificial (I.A.) que dá ao
computador a habilidade de aprender sem ser explicitamente programado <a href="parte-1/1/../../referencias.html">[18]</a>.</p>
<p>Em uma definição mais moderna, &quot;Um programa de computador aprende com a experiência E com
relação a alguma classe de tarefas T e medida de desempenho P, se seu desemprenho nas tarefas T,
conforme medido por P, melhora com a experiência E.&quot; <a href="parte-1/1/../../referencias.html">[12]</a>.</p>
<p>Em geral, qualquer problema de <em>M.L.</em> pode ser atribuído a uma dessas duas classificações gerais:</p>
<ul>
<li>
<p>Aprendizado Supervisionado (<em>Supervised Learning</em>);</p>
</li>
<li>
<p>Aprendizado Não-Supervisionado (<em>Unsupervised Learning</em>)</p>
</li>
</ul>
<p>Antes de detalharmos cada uma dessas classificações, é importante definir como medimos a inteligência
de uma máquina, quando tratamos de <em>machine learning</em>. Essa definição será especificada em
detalhes na seção seguinte.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="o-que-inteligência"><a class="header" href="#o-que-inteligência">O que inteligência?</a></h1>
<p>Uma dúvida que pode surgir ao definirmos <em>machine learning</em> é ”O que é inteligência?”. Essa dúvida
se tornou muito comum em diversas áreas de pesquisa relacionadas à IA.</p>
<p>Recentemente, a definição usada é a seguinte: &quot;A inteligência mede a capacidade de um agente de
atingir objetivos em uma ampla variedade de ambientes&quot; <a href="parte-1/1/../../referencias.html">[10]</a>. Essa definição implica na construção
de um modelo matemático para a inteligência de uma máquina. Com essa definição podemos
comparar máquinas inteligentes através de dados concretos - números. A seguir, está definida a
medida de inteligência de uma máquina.</p>
<hr />
<p>\[
\large{} \Upsilon (\pi) := \sum _{\mu \in E} 2 ^{-K(\mu)} V _{\mu} ^{\pi}
\]</p>
<hr />
<p>Onde:</p>
<ul>
<li>
<p>\( \Upsilon (\pi) \) é a medida de inteligência</p>
</li>
<li>
<p>\( \sum _{\mu \in E} \) é a soma sobre os ambientes</p>
</li>
<li>
<p>\( 2 ^{-K(\mu)} \) é a penalidade de complexidade</p>
</li>
<li>
<p>\( V _{\mu} ^{\pi} \) é o valor alcançado</p>
</li>
</ul>
<p>A equação acima é a definição formal da inteligência de uma máquina, também conhecida como
&quot;inteligência universal&quot; de um agente - máquina - \( \pi \). Ela descreve a medida de inteligência,
\( \Upsilon (\pi) \), de um agente \( \pi \) como o somatório dos valores de pontuação de inteligência,
\( V _{\mu} ^{\pi} \), do agente \( \pi \) em cada um dos ambientes \( \mu \). Para cada um desses ambientes
define-se um valor de distribuição de probabilidade universal, \( 2 ^{-K(\mu)} \), onde \( K \) é a função de
complexidade de Kolmogorov <a href="parte-1/1/../../referencias.html">[10]</a>, que penaliza a complexidade de cada um dos ambientes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="supervised-learning"><a class="header" href="#supervised-learning"><em>Supervised Learning</em></a></h1>
<p>No método de aprendizado supervisionado, é dado como entrada um conjunto de dados que já temos
conhecimento do resultado. Então, nosso objetivo nesse tipo de problema é treinar uma máquina, a
fim de fazer com que ela generalize esses resultados para entradas que ainda não foram vistas.</p>
<p>Problemas de aprendizado supervisionado são separados em problemas de classificação e regressão.
No problema de classificação, tentamos prever um resultado em uma saída discreta, ou seja, tentamos
mapear variáveis de entrada - em valores complexos - para categorias mais simples e generalizadas.
Nos problemas de regressão, tentamos prever resultados em uma saída contínua, ou seja, tentamos
mapear as variáveis de entrada para uma função contínua.</p>
<p>Podemos exemplificar um problema de regressão da seguinte forma: dado uma foto de uma pessoa,
tentamos prever a sua idade com base na imagem. De outra maneira, podemos exemplificar um
problema de classificação da seguinte forma: dado um paciente com um tumor, tentamos prever se
o tumor é maligno ou benigno.</p>
<p>Indo um pouco mais fundo, um dos maiores exemplos dessa área é a base de dados MNIST <a href="parte-1/1/../../referencias.html">[9]</a>
e processamento de linguagem natural <em>Natural Language Processing</em> através de <em>Generative Pretrained Transformer 2</em>
(GPT-2) <a href="parte-1/1/../../referencias.html">[15]</a> representados na Figura 1. Essa base de dados foi usada para a
implementação de um algoritmo de reconhecimento de dígitos manuscritos com a utilização de redes
neurais.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unsupervised-learning"><a class="header" href="#unsupervised-learning"><em>Unsupervised Learning</em></a></h1>
<p>O aprendizado não-supervisionado nos possibilita aproximar problemas que temos pouca ou nenhuma
ideia de como seus resultados devem ser. Nós podemos derivar esta estrutura utilizando o método
de <em>clustering</em> nos dados baseados nos relacionamentos com as variáveis nos dados</p>
<p>Podemos exemplificar a técnica de <em>clustering</em> da seguinte forma: ao pegar uma coleção de um milhão
de genes diferentes, encontramos uma forma de agrupá-los, de modo que os genes de cada grupo
compartilhem determinada semelhança, como por exemplo, tempo de vida, localização, funções, etc.</p>
<p>Recentemente, foi desenvolvido pela DeepMind uma inteligência artificial que joga o jogo Go no
mais alto nível de performance. AlphaGo <a href="parte-1/1/../../referencias.html">[19]</a> combina diferentes técnicas de redes neurais profundas
para realizar o treinamento. Uma rede neural escolhe o melhor movimento a ser realizado a partir
de um estado de tabuleiro e a outra realiza uma previsão do vencedor do jogo através de modelos
probabilísticos. Utiliza-se de métodos de <em>unsupervised learning</em> como por exemplo <em>reinforcement learning</em>
e <em>Monte Carlo Search</em> que escolhe a melhor jogada a partir de jogadas prévias. Na Figura 2
está representado um modelo de <em>unsupervised learning</em> utilizado na implementação do AlphaGo.</p>
<p align="center">
  <img src="parte-1/1/./img/1.png">
</p>
<p align="center">
(a) Banco de dados MNIST para reconhecimento de dígitos manuscritos. Muito usado em sistemas de
processamento de imagens. Essa base de dados foi usada para a implementação de um algoritmo de reconhecimento
de dígitos manuscritos com a utilização de redes neurais
</p>
<p align="center">
  <img src="parte-1/1/./img/1-1.png">
</p>
<p align="center">
(b) Representação de uma das funções da inteligência GPT-2 desenvolvida pela OpenAI (2019), onde a partir de
um texto de entrada. É uma inteligência que foi treinada para traduzir textos, responder perguntas, resumir
passagens de textos e gerar um texto de saída que se assemelha com o nível de escrita humano.
</p>
<p align="center">
Figura 1
</p>
<p align="center">
  <img src="parte-1/1/./img/2.png">
</p>
<p align="center">
Figura 2: Representação dos da arquitetura dos métodos de aprendizado implementados para o AlphaGo. Percebe-se
que foram utilizados métodos de <i>supervised learning</i> e <i>unsupervised learning</i> a fim de definir a
melhor possível jogada dentre as milhões possibilidades a partir de um dado estado de jogo.
</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-ii"><a class="header" href="#parte-ii">Parte II</a></h2>
<h1 id="supervised-learning-1"><a class="header" href="#supervised-learning-1">Supervised Learning</a></h1>
<ul>
<li>
<p><a href="./parte-2/1/2-1.html">Representação de modelos</a></p>
</li>
<li>
<p><a href="./parte-2/2/2-2.html">Regressão linear</a></p>
</li>
<li>
<p><a href="./parte-2/3/2-3.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="./parte-2/4/2-4.html">Gradiente Descendente (Gradient Descent)</a></p>
<ul>
<li>
<p><a href="./parte-2/4/2-4-1.html">Algoritmo Gradiente Descendente</a></p>
</li>
<li>
<p><a href="./parte-2/4/2-4-2.html">Gradiente Descendente para Regressão Linear</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-2/5/2-5.html">Álgebra Linear</a></p>
<ul>
<li>
<p><a href="./parte-2/5/2-5-1.html">Matrizes e vetores</a></p>
</li>
<li>
<p><a href="./parte-2/5/2-5-2.html">Operações básicas com matrizes e vetores</a></p>
</li>
<li>
<p><a href="./parte-2/5/2-5-3.html">Multipĺicação entre matrizes e vetores</a></p>
</li>
<li>
<p><a href="./parte-2/5/2-5-4.html">Multiplicação entre duas matrizes</a></p>
</li>
<li>
<p><a href="./parte-2/5/2-5-5.html">Matriz identidade, inversa e transposta</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-2/6/2-6.html">Regressão Linear com múltiplas variáveis</a></p>
<ul>
<li>
<p><a href="./parte-2/6/2-6-1.html">Múltiplos parâmetros de entrada</a></p>
</li>
<li>
<p><a href="./parte-2/6/2-6-2.html">Gradiente Descendente com múltiplas variáveis</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-2/7/2-7.html">Equação Normal (Normal Equation)</a></p>
</li>
<li>
<p><a href="./parte-2/8/2-8.html">Classificação</a></p>
<ul>
<li>
<p><a href="./parte-2/8/2-8-1.html">Problemas de calssificação</a></p>
</li>
<li>
<p><a href="./parte-2/8/2-8-2.html">Representação da hipótese</a></p>
</li>
<li>
<p><a href="./parte-2/8/2-8-3.html">Limites de decisão (Decision Boundary)</a></p>
</li>
<li>
<p><a href="./parte-2/8/2-8-4.html">Classificação Multiclasse</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-2/9/2-9.html">Regressão Logística (Logistic Regression)</a></p>
<ul>
<li>
<p><a href="./parte-2/9/2-9-1.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="./parte-2/9/2-9-2.html">Gradiente Descendente para Regressão Logística</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-2/10/2-10.html">Underfitting e overfitting</a></p>
<ul>
<li>
<p><a href="./parte-2/10/2-10-1.html">Função custo em casos de overfitting</a></p>
</li>
<li>
<p><a href="./parte-2/10/2-10-2.html">Regressão linear regularizada</a></p>
</li>
<li>
<p><a href="./parte-2/10/2-10-3.html">Regressão logística regularizada</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="representação-de-modelos"><a class="header" href="#representação-de-modelos">Representação de modelos</a></h1>
<p>A fim de representação futura, podemos utilizar a notação \( x ^{(i)} \) para denotar as variáveis de entrada
(<em>input</em>) e \( y ^{(i)} \) para denotar as variáveis de saída (<em>output</em>). O par \( (x ^{(i)}, y ^{(i)}) \) é chamado de exemplo
de treino e o conjunto de dados (<em>dataset</em>) que está sendo utilizado para análise é uma lista com \( m \)
exemplos de treino \( (x ^{(i)}, y ^{(i)}); i=1, \dots m \) chamado conjunto de treino (<em>training set</em>).</p>
<p>Podemos usar essa notação para descrever o método de aprendizado supervisionado de uma forma
mais formal, na qual, dado um <em>training set</em>, aprender uma função \( h:X \rightarrow Y \) de forma que
\( h(x) \) seja um &quot;bom&quot; preditor para o valor correspondente de \( y \). Historicamente, a função
\( h \) é conhecida como hipótese (<em>hypothesis</em>). Podemos analisar a seguinte definição através da Figura 3.</p>
<p align="center">
  <img src="parte-2/1/./img/3.png">
</p>
<p align="center">
Figura 3: Processo de treino utilizando Supervised Learning. Dado um conjunto de treino como entrada, o nosso
algoritmo de aprendizado tenta prever um valor \( y \) de saída que tenha relação com o valor \( x \) de entrada.
Essa previsão é calculada através da função hipótese \( h \).
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regressão-linear"><a class="header" href="#regressão-linear">Regressão linear</a></h1>
<p>Regressão é um método que modela um valor de destino com base em preditores independentes.
Este método é usado, principalmente, para prever e descobrir a relação de causa e efeito entre as
variáveis.</p>
<p>Uma regressão linear simples é um tipo de regressão que analisa, a partir de um conjunto de variáveis
independentes de entrada \( x \), a relação entre essas variáveis com os seus respectivos valores esperados
\( y \). Na Figura 4, abaixo, a linha vermelha representa a melhor reta que aproxima melhor cada um
dos pontos representados em azul. Ou seja, baseado em um conjunto de dados, tentamos gerar uma
reta que modela cada um desses dados de forma ótima.</p>
<p align="center">
  <img src="parte-2/2/./img/4.png">
</p>
<p align="center">
Figura 4: Representação de uma reta gerada a partir do método de regressão linear. A reta em vermelho representa a
melhor aproximação de cada um dos pontos representados em azul, que são os dados de entrada.
</p>
<p>A reta gerada pela regressão linear pode ser modelada a partir da equação linear abaixo:</p>
<p>\[
\large{} y = \theta _0 + \theta _1 x
\]</p>
<p>Portanto, o objetivo do algoritmo de regressão linear é encontrar os melhores valores de
\( \theta _0 \) e \( \theta _1 \).</p>
<p>Os métodos utilizados para calcularmos esses parâmetros serão apresentados nas seções seguintes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="função-custo-cost-function"><a class="header" href="#função-custo-cost-function">Função Custo (<em>Cost Function</em>)</a></h1>
<p>A função custo - também chamada de <em>loss function</em> - pode ser utilizada para medir a precisão da
nossa função hipótese \( h:X \rightarrow Y \). A função custo utiliza da diferença média de todos os resultados
da função hipótese com todos <em>inputs</em> de \( x \) e <em>outputs</em> de \( y \), como representado na Figura 5.</p>
<p>\[
\large{} J(\theta _0 , \theta _1) = \frac{1}{2m} \sum _{i=1} ^m (ŷ _i - y _i) ^2 =
\frac{1}{2m} \sum _{i=1} ^m (h _{\theta}(x _i) - y _i) ^2
\]</p>
<p>Através dessa expressão podemos perceber que o objetivo principal da função custo é minimizar a
diferença entre o resultado esperado a função hipótese e o valor de \( y \) através das entradas
\( \theta _0 \) e \( \theta _1 \). Essa função também é conhecida como <em>Squared error function</em> ou <em>Mean squared error</em>.</p>
<p align="center">
  <img src="parte-2/3/./img/5.png">
</p>
<p align="center">
Figura 5: Representação da função custo
</p>
<p>Dessa forma, temos quatro predefinições básicas:</p>
<ul>
<li>
<p>Hipótese: \( h _{\theta}(x) = \theta _0 + \theta _1 x \);</p>
</li>
<li>
<p>Parâmetros: \( \theta _0 , \theta _1 \);</p>
</li>
<li>
<p>Função Custo: \( J(\theta _0 , \theta _1) = \frac{1}{2m} \sum _{i=1} ^m (h _{\theta}(x _i) - y _i) ^2 \);</p>
</li>
<li>
<p>Objetivo: \( \underset{\theta _0 , \theta _1} {min} \ J(\theta _0 , \theta _1) \)</p>
</li>
</ul>
<p>Podemos utilizar linhas de contorno para representar a função de duas variáveis \( J(\theta _0 , \theta _1) \)
em apenas duas dimensões como representado nas Figuras 6 e 7.</p>
<p align="center">
  <img src="parte-2/3/./img/6.png">
</p>
<p align="center">
Figura 6: Representação da função custo através de visualização 3D
</p>
<p align="center">
  <img src="parte-2/3/./img/7.png">
</p>
<p align="center">
Figura 7: Representação da função custo através de linha de contorno
</p>
<p>Os gráficos da Figura 7 minimizam a função custo ao máximo. O resultado de \( \theta _0 \) e \( \theta _1 \)
tende a ficar em torno de 250 e 0.12, respectivamente. Em outras palavras, a melhor aproximação da função custo
está mais no centro das linhas de contorno. Chamamos o método de minimização da função
custo de método do gradiente descendente que será discutido na Seção
<a href="parte-2/3/../4/2-4.html">Gradiente Descendente (<em>Gradient Descent</em>)</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradiente-descendente-gradient-descent"><a class="header" href="#gradiente-descendente-gradient-descent">Gradiente Descendente (<em>Gradient Descent</em>)</a></h1>
<p>O método do gradiente descendente pode ser utilizado para minimizar o valor de uma função.
Utilizaremos esse método a fim de minimizar a função \( J(\theta _0 , \theta _1) \).</p>
<p>Podemos representar esse método a partir de um gráfico em três dimensões, onde \( \theta _0 \) está no eixo
\( x, \theta _1 \) está no eixo \( y \) e o valor da função \( J(\theta _0 , \theta _1) \) está no eixo
\( z \), conforme está representado na Figura 8. Os pontos no gráfico são os resultados da função custo
utilizando a função hipótese \( h(x) \) com as entradas \( \theta _0 \) e \( \theta _1 \).</p>
<p align="center">
  <img src="parte-2/4/./img/8.png">
</p>
<p align="center">
Figura 8: Representação da função gradiente descendente
</p>
<p>O método de minimização utilizando a função gradiente descendente pode ser pensado como um
algoritmo, que para cada ponto, a partir do inicial, dado como entrada, escolhe a descida mais
íngreme do valor de \( J(\theta _0 , \theta _1) \) na função através de derivadas parciais e um
valor \( \alpha \) (<em>learning rate</em>) que determinará a distância entre cada descida. Obtemos sucesso,
quando a função custo estiver em um dos mínimos do gráfico, como representado pelas setas vermelhas na Figura 8.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="algoritmo-gradiente-descendente"><a class="header" href="#algoritmo-gradiente-descendente">Algoritmo Gradiente Descendente</a></h1>
<hr />
<p><strong>Algorithm 1</strong> Algoritmo Gradiente Descendente</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:   
\( \theta _j := \theta _j - \alpha \frac{\partial}{\partial \theta _j} J(\theta _0, \theta _1) \)
  \( \rhd \) (para \( j=0 \dots m \))</p>
<p>4:   <strong>until</strong> \( convergir \)</p>
<p>5: <strong>end procedure</strong></p>
<hr />
<p>Enquanto o método de gradiente descendente não convergir para o mínimo da função, a cada iteração,
atualizamos simultaneamente os valores de \( \theta _1, \theta _2, \dots , \theta _n \) fazendo com que
esses valores se aproximem cada vez mais ao mínimo da função.</p>
<p>Podemos perceber que o valor de \( \alpha \) tem um certo impacto na atualização dos valores de \( \theta \).
Valores de \( \alpha \) pequenos, a convergência do método gradiente descendente é mais lenta. E para valores de
\( \alpha \) muito grandes, a convergência do método gradiente pode ultrapassar o mínimo, o que pode
impossibilitar a convergência da função.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradiente-descendente-para-regressão-linear"><a class="header" href="#gradiente-descendente-para-regressão-linear">Gradiente Descendente para Regressão Linear</a></h1>
<p>Podemos utilizar o método de gradiente descendente para minimizar a função <em>Mean squared error</em>
utilizada no algoritmo de regressão linear, substituído a função \( J(\theta _0, \theta _1) \) por nossa função hipótese.
Dessa forma, o algoritmo de gradiente descendente para a minimização da função \( J \) terá a seguinte estrutura:</p>
<hr />
<p><strong>Algorithm 2</strong> Algoritmo Gradiente Descendente Para Minimização Da Função J</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:   
\( \theta _0 := \theta _0 - \alpha \frac{1}{m} \sum _{i=1} ^m (h _{\theta}(x ^{(i)}) - y ^{(i)}) \)</p>
<p>4:   
\( \theta _1 := \theta _1 - \alpha \frac{1}{m} \sum _{i=1} ^m (h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x ^{(i)} \)</p>
<p>5:   <strong>until</strong> \( convergir \)</p>
<p>6: <strong>end procedure</strong></p>
<hr />
<p>Onde \( m \) é o tamanho do conjunto de treino; \( \theta _0 \) uma constante que será atualizada simultaneamente
com \( \theta _1 \); e \( x ^{(i)}, y ^{(i)} \) são valores dados no conjunto de treino.</p>
<p>Esse algoritmo é aplicado para todos os valores dados no conjunto de treino, chamamos isso de <em>batch gradient descent</em>.
Dessa forma, quando aplicamos o algoritmo, a função \( J \) possui apenas um mínimo
global (sem outros mínimos locais). Portanto, a função de gradiente descendente sempre converge
para regressões lineares, pois \( J \) é uma função quadrática convexa.</p>
<p>Nas próximas seções, veremos alguns métodos de otimização desse algoritmo utilizando álgebra linear.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="Álgebra-linear"><a class="header" href="#Álgebra-linear">Álgebra linear</a></h1>
<p>Nesta seção, serão revisados, brevemente, alguns conceitos básicos da álgebra linear, como por
exemplo operações com matrizes, inversa e transposta de matrizes e suas propriedades.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="matrizes-e-vetores"><a class="header" href="#matrizes-e-vetores">Matrizes e vetores</a></h1>
<p>Matrizes são <em>arrays</em> bidimensionais.</p>
<p>\[
\large{} M _3 =
\begin{bmatrix}
a &amp;&amp; b &amp;&amp; c \\
d &amp;&amp; e &amp;&amp; f \\
g &amp;&amp; h &amp;&amp; i
\end{bmatrix}
\]</p>
<p>A matriz acima é considerada uma matriz 3x3.</p>
<p>Vetores são matrizes com apenas uma coluna e diversas linhas (matriz coluna).</p>
<p>\[
\large{} v =
\begin{bmatrix}
a _1 \\
a _2 \\
a _3
\end{bmatrix}
\]</p>
<p>Como podemos perceber, vetores são um subconjunto de matrizes, e o vetor acima é uma matriz
3x1.</p>
<p>Dessa forma, podemos definir algumas notações referentes a matrizes e vetores.</p>
<ul>
<li>
<p>\( A _{ij} \) se refere ao elemento que se encontra na <em>i-ésima</em> linha e na <em>j-ésima</em> coluna;</p>
</li>
<li>
<p>Um vetor com 'n' linhas é um vetor 'n'-dimensional;</p>
</li>
<li>
<p>\( v _i \) se refere ao <em>i-ésimo</em> elemento do vetor;</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operações-básicas-com-matrizes-e-vetores"><a class="header" href="#operações-básicas-com-matrizes-e-vetores">Operações básicas com matrizes e vetores</a></h1>
<p>A adição e subtração de vetores são operações unitárias referentes a cada linha do vetor.</p>
<p>\[
\large{} v + u =
\begin{bmatrix}
v _1 \\
v _2 \\
v _3
\end{bmatrix}
+
\begin{bmatrix}
u _1 \\
u _2 \\
u _3
\end{bmatrix}
=
\begin{bmatrix}
v _1 + u _1 \\
v _2 + u _2 \\
v _3 + u _3
\end{bmatrix}
\]</p>
<p>Para multiplicarmos ou dividirmos um valor escalar por um vetor é usada a mesma lógica.</p>
<p>\[
\large{} v \cdot x =
\begin{bmatrix}
v _1 \\
v _2 \\
v _3
\end{bmatrix}
\cdot
x
=
\begin{bmatrix}
v _1 \cdot x \\
v _2 \cdot x \\
v _3 \cdot x
\end{bmatrix}
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multiplicação-entre-matrizes-e-vetores"><a class="header" href="#multiplicação-entre-matrizes-e-vetores">Multiplicação entre matrizes e vetores</a></h1>
<p>Para realizar a multiplicação entre uma matriz e um vetor, realizamos a multiplicação no sentido
&quot;linha \( \times \) coluna&quot; conforme a expressão abaixo:</p>
<p>\[
\large{} M _2 \times v =
\begin{bmatrix}
a &amp;&amp; b \\
c &amp;&amp; d
\end{bmatrix}
\times
\begin{bmatrix}
v _1 \\
v _2
\end{bmatrix}
=
\begin{bmatrix}
a \times v _1 + b \times v _2 \\
c \times v _1 + d \times v _2
\end{bmatrix}
\]</p>
<p>Ao multiplicarmos uma matriz \( A _{ij} \) por um vetor \( v \) com \( j \) linhas, teremos como
resultado uma matriz \( B _{i1} \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multiplicação-entre-duas-matrizes"><a class="header" href="#multiplicação-entre-duas-matrizes">Multiplicação entre duas matrizes</a></h1>
<p>Para realizar a multiplicação entre duas matrizes seguimos a mesma lógica apresentada na
Seção <a href="parte-2/5/./2-5-3.html">Multiplicação entre matrizes e vetores</a> multiplicando no sentido &quot;linha \( \times \) coluna&quot;.</p>
<p>\[
A _2 \times B _2 =
\begin{bmatrix}
a _1 &amp;&amp; b _1 \\
c _1 &amp;&amp; d _1
\end{bmatrix}
\times
\begin{bmatrix}
a _2 &amp;&amp; b _2 \\
c _2 &amp;&amp; d _2
\end{bmatrix}
=
\begin{bmatrix}
a _1 \times a _2 + b _1 \times c _2 &amp;&amp; a _1 \times b _2 + b _1 \times d _2 \\
c _1 \times a _2 + d _1 \times c _2 &amp;&amp; c _1 \times b _2 + d _1 \times d _2
\end{bmatrix}
\]</p>
<p>Ao multiplicarmos duas matrizes \( A _{mxn} \) e \( B _{nxo} \) teremos uma matriz \( C _{mxo} \).
Dessa forma, podemos definir algumas propriedades relacionadas às operações com matrizes:</p>
<ul>
<li>
<p>Multiplicação de matrizes não são comutativas, ou seja, \( A \times B \neq B \times A \)</p>
</li>
<li>
<p>Multiplicação de matrizes são associativas, ou seja, \( (A \times B) \times C = A \times (B \times C) \)</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="matriz-identidade-inversa-e-transposta"><a class="header" href="#matriz-identidade-inversa-e-transposta">Matriz identidade, inversa e transposta</a></h1>
<p>A matriz identidade é aquela que, ao ser multiplicada por uma outra matriz de mesma dimensão,
resulta na matriz original. Em outras palavras, é uma matriz onde há apenas '1's' na sua diagonal
principal.</p>
<p>\[
\large{} I _3 =
\begin{bmatrix}
1 &amp;&amp; 0 &amp;&amp; 0 \\
0 &amp;&amp; 1 &amp;&amp; 0 \\
0 &amp;&amp; 0 &amp;&amp; 1
\end{bmatrix}
\]</p>
<p>A inversa de uma matriz, denotada por \( A ^{-} \) é aquela que, ao ser multiplicada por \( A \), resulta na
matriz identidade \( I \) de \( A \). Em outras palavras:</p>
<p>\[
\large{} A \times A ^{-1} = I
\]</p>
<p>A matriz transposta da matriz \( A _{ij} \) é a matriz \( A _{ji} ^T \). Trata-se da matriz que vamos obter quando
reescrevemos a matriz \( A _{ij} \) trocando de posição as linhas e colunas, transformando a primeira linha
de \( A _{ij} \) na primeira coluna de \( A _{ji} ^T \), a segunda linha de \( A _{ij} \) na segunda coluna de
\( A _{ji} ^T \), e assim sucessivamente.</p>
<p>\[
\large{} A _{ij} = A _{ji} ^T
\]</p>
<p>\[
\large{} A =
\begin{bmatrix}
a &amp;&amp; b \\
c &amp;&amp; d \\
e &amp;&amp; f
\end{bmatrix}
, A ^T =
\begin{bmatrix}
a &amp;&amp; c &amp;&amp; e \\
b &amp;&amp; d &amp;&amp; f
\end{bmatrix}
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regressão-linear-com-múltiplas-variáveis"><a class="header" href="#regressão-linear-com-múltiplas-variáveis">Regressão Linear com múltiplas variáveis</a></h1>
<p>Nesta seção iremos introduzir os conceitos básicos para a aplicação do algoritmo de regressão linear
para múltiplas entradas. Em outras palavras, iremos tentar estimar o valor da saída \( y \) a partir de
diversos parâmetros de entrada.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="múltiplos-parâmetros-de-entrada"><a class="header" href="#múltiplos-parâmetros-de-entrada">Múltiplos parâmetros de entrada</a></h1>
<p>Precisamos definir algumas notações que serão utilizadas.</p>
<ul>
<li>
<p>\( x _j ^{(i)} = \) valor do parâmetro \( j \) no <em>i-ésimo</em> exemplo de treino;</p>
</li>
<li>
<p>\( x ^{(i)} = \) valor dos parâmetros no <em>i-ésimo</em> exemplo de treino (vetor);</p>
</li>
<li>
<p>\( m = \) número de exemplos de treino;</p>
</li>
<li>
<p>\( n = \) número de parâmetros;</p>
</li>
</ul>
<p>Com isso, podemos definir a função hipótese para múltiplos parâmetros da seguinte forma:</p>
<hr />
<p>\[
\large{} h _{\theta}(x) = \theta _0 + \theta _1 x _1 + \theta _2 x _2 +
\theta _3 x _3 + \dots + \theta _n x _n
\]</p>
<hr />
<p>Sendo \( \theta _i \) os parâmetros de entrada da função</p>
<p>Podemos utilizar as definições de matrizes anteriormente vistas na Seção <a href="parte-2/6/../5/2-5.html">Álgebra Linear</a>
para definir a função hipótese de múltiplas variáveis da seguinte forma:</p>
<p>\[
\large{} h _{\theta}(x) =
\begin{bmatrix}
\theta _0 &amp;&amp; \theta _1 &amp;&amp; \theta _2 &amp;&amp; \dots &amp;&amp; \theta _n
\end{bmatrix}
\begin{bmatrix}
x _0 \\ x _1 \\ x _2 \\ \dots \\ x _n
\end{bmatrix}
= \theta ^T x
\]</p>
<p>É importante mencionar que o valor de \( x _0 ^{(i)} = 1 \) para \( (i \in 1, \dots ,m) \) por convenção.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradiente-descendente-com-múltiplas-variáveis"><a class="header" href="#gradiente-descendente-com-múltiplas-variáveis">Gradiente Descendente com múltiplas variáveis</a></h1>
<p>Da mesma forma apresentada na Seção <a href="parte-2/6/../3/2-3.html">Função Custo (<em>Cost Function</em>)</a>, teremos as
seguintes predefinições das funções que serão utilizadas a fim de minimizar o custo da função
\( J \) com múltiplas variáveis de entrada.</p>
<ul>
<li>
<p>Hipótese:
\( h _{\theta}(x) = \theta _0 + \theta _1 x _1 + \theta _2 x _2 + \theta _3 x _3 + \dots + \theta _n x _n \);</p>
</li>
<li>
<p>Parâmetros:
\( \theta _0, \theta _1, \dots , \theta _n \) (vetor (n+1)-dimensional);</p>
</li>
<li>
<p>Função Custo:
\( J(\theta _0, \theta _1, \dots , \theta _n)= \frac{1}{2m} \sum _{i=1} ^m (h _{\theta}(x _i) - y _i) ^2 \)</p>
</li>
</ul>
<p>O algoritmo gradiente descendente, em geral, tem a mesma estrutura para os diferentes problemas.
Nós apenas temos que iterar sobre os \( n \) parâmetros de entrada, atualizando-os simultaneamente.</p>
<hr />
<p><strong>Algorithm 3</strong> Algoritmo Gradiente Descendente Para Múltiplas Variáveis</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:    \( \theta _j := \theta _j - \alpha \frac{1}{m} \sum _{i=1} ^m
(h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)} \)
  \( \rhd \) para \( j := 0 \dots n \)</p>
<p>4:   <strong>until</strong> \( convergir \)</p>
<p>5: <strong>end procedure</strong></p>
<hr />
<p>Podemos otimizar o algoritmo gradiente descendente colocando todos os parâmetros no mesmo
intervalo. Esse tipo de operação é muito importante para evitar que o gradiente trabalhe com
valores muito grandes que possam causar &quot;explosão&quot;, ou muito pequenos que possam ser zero, o
que pode se tornar um problema para o treinamento do algoritmo de aprendizado.</p>
<p>Para isso, utilizaremos da técnica de normalização média (<em>mean normalization ou feature scaling</em>).
<em>Feature scaling</em> envolve dividir os dados de entrada por um intervalo (desvio padrão, ou uma subtração
entre o maior e o menor valor das variáveis de entrada) resultando em um valor próximo de 1. <em>Mean normalization</em>
envolve subtrair o valor médio das variáveis de entrada de cada variável de entrada.
Dessa forma, teremos a seguinte fórmula:</p>
<hr />
<p>\[
\large{} x _i = \frac{x _i - \mu _i}{s _i}
\]</p>
<hr />
<p>onde \( \mu _i \) é a média de todos os parâmetros de entrada e \( s _i \) é a variação dos valores de entrada
\( max - min \), ou o desvio padrão.</p>
<p>Com isso, teremos os valores de entrada em um mesmo intervalo, sem valores discrepantes para o
cálculo da função custo e da otimização pelo gradiente descendente. Com os valores normalizados,
evitamos diversos problemas que foram acima mencionados e o custo computacional para o cálculo
das derivadas parciais é diminuído.</p>
<p>Para ter certeza que o método do gradiente descendente está funcionando corretamente, o valor de
\( J(\theta) \) deve diminuir a cada iteração e convergir para um valor próximo de zero. Caso o valor de
\( J(\theta) \) não esteja convergindo, podemos tentar um valor menor de \( \alpha \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="equação-normal-normal-equation"><a class="header" href="#equação-normal-normal-equation">Equação Normal (<em>Normal Equation</em>)</a></h1>
<p>É um outro método de minimização da função \( J \), assim como o método de gradiente descendente.
Essa forma de implementação, muitas vezes pode otimizar o tempo de processamento da função
de minimização através de derivadas da função \( J \) a respeito aos \( \theta ' _j  \)s igualando-os a zero. Isso nos
permite encontrar o valor ótimo para \( \theta \) sem iterações. Fórmula da equação normal é dada abaixo:</p>
<p>\[
\large{} \theta = (X ^T X) ^{-1} X ^T y
\]</p>
<p>onde \( X \) é uma matriz na qual a coluna zero tem todos os elementos iguais a 1.</p>
<p>Dessa forma, podemos comparar as duas formas de implementação que temos: <em>Gradient Descent</em> e
<em>Normal Equation</em>.</p>
<p align="center">
Tabela 1: Comparação entre os métodos Gradient Descent e Normal Equation
</p>
<table><thead><tr><th align="right">Gradient Descent</th><th align="left">Normal Equation</th></tr></thead><tbody>
<tr><td align="right">É necessário definir o valor de \( \alpha \)</td><td align="left">Não é necessário definir o valor de \( \alpha \)</td></tr>
<tr><td align="right">Muitas iterações são necessárias</td><td align="left">Não são necessárias iterações</td></tr>
<tr><td align="right">\( O(kn ^2) \)</td><td align="left">\( O(n ^3) \) pois precisa calcular a inversa de \( X ^T X \)</td></tr>
<tr><td align="right">Funciona bem quando o valor de \( n \) é grande</td><td align="left">Lento quando o valor de \( n \) é grande</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classificação"><a class="header" href="#classificação">Classificação</a></h1>
<ul>
<li>
<p><a href="parte-2/8/./2-8-1.html">Problemas de calssificação</a></p>
</li>
<li>
<p><a href="parte-2/8/./2-8-2.html">Representação da hipótese</a></p>
</li>
<li>
<p><a href="parte-2/8/./2-8-3.html">Limites de decisão (Decision Boundary)</a></p>
</li>
<li>
<p><a href="parte-2/8/./2-8-4.html">Classificação Multiclasse</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="problemas-de-classificação"><a class="header" href="#problemas-de-classificação">Problemas de classificação</a></h1>
<p>O método de classificação tem como objetivo classificar um conjunto de dados entre estados ou tipos
distintos. Um bom exemplo de um problema de classificação é quando queremos prever se um tumor
é maligno ou benigno baseando-se apenas no tamanho do tumor.</p>
<p>Podemos usar o algoritmo de regressão linear para classificar uma base de dados em dois diferentes
grupos, como por exemplo, para valores de \( h(x) &gt; 0.5 \) e para valores \( h(x) \leq 0.5 \). Contudo, essa
forma de implementação não funciona muito bem, pois os problemas de classificação, geralmente,
não cabem em problemas de regressão linear.</p>
<p>De outra forma, gostaríamos de classificar nossa base de dados em saídas discretas. Como base,
iremos nos focar nos problemas de classificação binários (<em>binary classification problems</em>), os quais os
valores de \( y \) podem assumir apenas dois valores: zero ou um, em outras palavras \( y \in 0,1 \).</p>
<p>Podemos representar um problema de classificação através da imagem a seguir na Figura 9</p>
<p align="center">
  <img src="parte-2/8/./img/9.png">
</p>
<p align="center">
Figura 9: Representação de um problema de classificação
</p>
<p>Podemos perceber que na Figura 9 temos dois tipos de classificações: linearmente separáveis e não
linearmente separáveis. Nesta seção iremos discutir os problemas binários linearmente separáveis sem
necessidade de regularização da função.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="representação-da-hipótese"><a class="header" href="#representação-da-hipótese">Representação da hipótese</a></h1>
<p>Podemos utilizar o nosso antigo algoritmo de regressão linear para prever um valor de \( y \) discreto
dado um valor de \( x \). Mas como foi mencionado anteriormente, essa não é uma boa solução para
problemas de classificação e para isso, devemos modificar a função hipótese a fim de satisfazer a
saída discreta dos problemas de classificação, ou seja, \( 0 \geq h _{\theta}(x) \geq 1 \).</p>
<p>Uma boa modificação seria basear a nossa função hipótese na função logística (<em>Logistic Function</em>)
de forma que possamos nos basear na função sigmoide conforme representada na Figura 10. Em
outras palavras, teremos:</p>
<hr />
<p>\[
\large{} h _{\theta}(x) = g(\theta ^T x), \ z = \theta ^T x, \ g(z) = \frac{1}{1 + e ^{-z}}
\]</p>
<hr />
<p align="center">
  <img src="parte-2/8/./img/10.png">
</p>
<p align="center">
Figura 10: Representação da função sigmoide
</p>
<p>A função sigmoide mapeia um valor real em um valor no intervalo \( (0,1) \) fazendo com que seja a
melhor forma de implementação de problemas de classificação.</p>
<p>Dessa forma, podemos chegar a algumas conclusões e interpretações dessa nova forma de implementação
da função hipótese:</p>
<ul>
<li>
<p>\( h _{\theta}(x) \) nos dá a probabilidade da nossa saída ser 1.</p>
</li>
<li>
<p>\( h _{\theta}(x)=P(y=1|x; \theta )=1-P(y=0|x; \theta )P(y=0|x; \theta )+P(y=1|x; \theta )=1 \)</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="limite-de-decisão-decision-boundary"><a class="header" href="#limite-de-decisão-decision-boundary">Limite de decisão (<em>Decision Boundary</em>)</a></h1>
<p>É uma forma de encontrarmos o limite entre os valores discretos que temos na saída da função
hipótese. Em outras palavras, podemos traduzir este pensamento da seguinte forma:</p>
<p>\[
\large{} h _{\theta}(x) \geq 0.5 \rightarrow y = 1
\]</p>
<p>\[
\large{} h _{\theta}(x) &lt; 0.5 \rightarrow y = 0
\]</p>
<p>pois nossa função sigmoide se comporta de tal forma que quando a entrada \( z(x) = \theta ^T x \geq 0 \)
o valor da função \( g(z) \geq 0.5 \).</p>
<p>Dessa forma, o limite de decisão é a linha que separa a área entre os valores classificados, ou
como \( y=0 \), ou como \( y=1 \). Esses valores discretos representam as duas classes do problema de
classificação binário. Em outras palavras, o limite de decisão é uma curva que separa essas duas
classes.</p>
<p>Podemos utilizar outras estruturas de funções para gerarmos diferentes limites de decisão de acordo
com a nossa base de dados. Por exemplo se nossos parâmetros dividem áreas circulares, podemos
utilizar uma função \( z = \theta _0 + \theta _1 x _1 ^2 + \theta _2 x _2 ^2 \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classificação-multiclasse"><a class="header" href="#classificação-multiclasse">Classificação Multiclasse</a></h1>
<p>A classificação multiclasse é uma definição para quando temos além de apenas uma saída binária na
função hipótese. Ou seja, podemos classificar os nossos dados em mais de duas classificações. A
Figura 11 representa um problema de classificação multiclasse utilizando o método <em>one-vs-all</em>.</p>
<p align="center">
  <img src=./img/11.png>
</p>
<p align="center">
Figura 11: Representação de um problema de classificação multiclasse
</p>
<p>No método de classificação <em>one-vs-all</em> treinamos \( N \) classificadores lineares distintos que são projetados
para reconhecer cada classe. Para cada classe distinta aplicamos uma regressão logística binária e utilizamos
o valor retornado da função hipótese para classificar os dados. Em outras palavras,
geramos um limite de decisão aplicando a regressão logística para cada classe de acordo com as
outras. Então, se temos \( N \) classes de entrada, roda-se o algoritmo de regressão logística \( N \) vezes
para cada uma dessas classes, retornando \( N \) valores distintos de \( h \) para cada uma das classes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regressão-logística-logistic-regression"><a class="header" href="#regressão-logística-logistic-regression">Regressão Logística (<em>Logistic Regression</em>)</a></h1>
<ul>
<li>
<p><a href="parte-2/9/./2-9-1.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="parte-2/9/./2-9-2.html">Gradiente Descendente para Regressão Logística</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="função-custo-cost-function-1"><a class="header" href="#função-custo-cost-function-1">Função Custo (<em>Cost Function</em>)</a></h1>
<p>Nas seções anteriores, discutimos a implementação da função custo para a regressão linear. Entretanto,
para a regressão logística, utilizaremos uma função hipótese voltada para os problemas de
classificação. Na Figura 10 está representada a função logística que será utilizada para calcular a
hipótese do problema. Essa função gera valores probabilísticos no intervalo \( [0,1] \) e, com isso,
podemos calcular o valor da função custo de forma que possamos comparar a probabilidade da nossa
hipótese ser igual a 1 com o resultado \( y \) esperado.</p>
<p>Para realizar essa comparação, devemos realizar algumas manipulações algébricas de forma que
possamos definir uma função \( J(\theta) \) que calcule o custo. Abaixo, está descrita a intuição dessa
manipulação.</p>
<p>\[
\large{} J(\theta) = \frac{1}{m} \sum _{i=1} ^m Cost(h _{\theta}(x ^{(i)}), y ^{(i)})
\]</p>
<p>\[
\large{} Cost(h _{\theta}(x ^{(i)}), y) = - \log (h _{\theta}(x)) \ se \ y=1
\]</p>
<p>\[
\large{} Cost(h _{\theta}(x ^{(i)}), y) = - \log (1 - h _{\theta}(x)) \ se \ y=0
\]</p>
<p>Dessa forma, podemos ter duas diferentes funções para a representação da função custo para a
regressão logística. Como podemos ver na Figura 12 temos duas representações, para \( y=1 \) em azul
e para \( y=0 \) em vermelho.</p>
<p align="center">
  <img src="parte-2/9/./img/12.png">
</p>
<p align="center">
Figura 12: Representação da função custo da regressão logística
</p>
<p>\[
\large{} Cost(h _{\theta}(x ^{(i)}), y) = 0 \ \ se \ \ h _{\theta}(x) = y
\]</p>
<p>\[
\large{} Cost(h _{\theta}(x ^{(i)}),y) \rightarrow \infty \ \ se \ \ y=0 \ \ e \ \ h _{\theta}(x) \rightarrow 1
\]</p>
<p>\[
\large{} Cost(h _{\theta}(x ^{(i)}),y) \rightarrow \infty \ \ se \ \ y=1 \ \ e \ \ h _{\theta}(x) \rightarrow 0
\]</p>
<p>Podemos perceber que quando o valor da função custo é zero, então o valor da função hipótese é
igual a \( y \). Além disso, quando a função custo tende ao infinito e o valor de \( y \) é igual a zero, o valor
da função hipótese tende a um e se o valor de \( y \) é igual a um, a função hipótese tende a zero.</p>
<p>A fim de simplificar a função custo, podemos reescrevê-la da seguinte forma sem alterar o valor do
resultado:</p>
<p>\[
\large{} Cost(h _{\theta}(x),y) = -y \cdot \log (h _{\theta}(x)) - (1-y) \cdot \log (1-h _{\theta}(x))
\]</p>
<p>Com isso, podemos generalizar a função custo de acordo com a expressão abaixo:</p>
<p>\[
\large{} J(\theta)=- \frac{1}{m} \sum _{i=1} ^m
\Big[ y ^{(i)} \cdot \log (h _{\theta}(x ^{(i)})) + (1-y ^{(i)}) \cdot \log (1-h _{\theta}(x ^{(i)})) \Big]
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradiente-descendente-para-regressão-logística"><a class="header" href="#gradiente-descendente-para-regressão-logística">Gradiente Descendente para Regressão Logística</a></h1>
<p>Da mesma forma que o método de regressão linear, o algoritmo do gradiente descendente funciona
iterando sobre os valores de \( \theta \) e derivando a função custo \( J \) em relação a \( \theta \).</p>
<p>Assim, podemos descrever o método através do seguinte algoritmo:</p>
<hr />
<p><strong>Algorithm 4</strong> Algoritmo Gradiente Descendente Para Regressão Logística</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:   
\( \theta _j := \theta _j - \frac{\alpha}{m} \sum _{i=1} ^m (h _{\theta}(x ^{(i)})-y ^{(i)}) \cdot x _j ^{(i)} \)
  \( \rhd \) Atualiza simultaneamente todos \( \theta _j\)</p>
<p>4:   <strong>until</strong> \( convergir \)</p>
<p>5: <strong>end procedure</strong></p>
<hr />
<p>ou através da forma vetorizada:</p>
<p>\[
\large{} \theta := \theta - \frac{\alpha}{m} X ^T (g(X \theta)- \overset{\rightarrow}y)
\]</p>
<p>Dessa forma, podemos utilizar dos métodos vistos anteriormente para implementar o algoritmo de
regressão logística. Contudo, ainda podemos utilizar de técnicas de computação numérica para
otimizar os algoritmos. Como por exemplo <em>Conjugate gradient</em>, <em>BFGS</em> e <em>L-BFGS</em> que podemos
utilizar a fim de otimizar o método de gradiente descendente.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="underfitting-e-overfitting"><a class="header" href="#underfitting-e-overfitting"><em>Underfitting</em> e <em>overfitting</em></a></h1>
<p>Quando tentamos prever um valor \( y \) a partir de um conjunto de treino, podem-se ocorrer alguns
problemas relacionados a função de hipótese. Esses problemas são chamados de <em>underfitting</em> e
<em>overfitting</em>. Para explicá-los, podemos nos basear na Figura 13</p>
<p align="center">
  <img src="parte-2/10/./img/13.png">
</p>
<p align="center">
Figura 13: Representação dos problemas de <i>underfitting</i> e <i>overfitting</i>
</p>
<p>Como podemos perceber, na figura mais à esquerda, temos uma função linear do tipo \( y= \theta _0 + \theta _1x \)
que não seja ajusta adequadamente com a nossa base de treino. Na figura do centro temos uma
função quadrática do tipo \( y= \theta _0 + \theta _1x + \theta _2x ^2 \), que aparentemente se adapta muito bem a nossa
base de dados. E na figura mais à direita, temos um polinômio de grau cinco que atinge todos os
pontos da nossa base de dados.</p>
<p>Com isso, podemos dizer que a figura mais à esquerda apresenta o problema de subajuste (<em>underfitting</em>
ou <em>high bias</em>) e a figura mais à direita apresenta o problema de sobreajuste (<em>overfitting</em> ou <em>high variance</em>).</p>
<p>O problema de <em>underfitting</em> ocorre quando a função hipótese \( h \) não consegue mapear com consistência
os valores da saída pois uma função muito simples é utilizada ou foram utilizados poucos parâmetros
de entrada.</p>
<p>De outra forma, o problema de <em>overfitting</em> ocorre quando a função hipótese se adapta perfeitamente
a nossa base de treino, mas não consegue generalizar os resultados das entradas. Isso ocorre, pois
uma função muito complexa foi utilizada ou o número de parâmetros utilizados como entrada da
função é muito alto.</p>
<p>Assim, temos alguns métodos que podemos utilizar para evitar esse tipo de problema. Dois são
principais e estão listados abaixo:</p>
<ol>
<li>
<p>Reduzir o número de parâmetros:</p>
<ul>
<li>
<p>Manualmente selecionar os parâmetros a serem removidos;</p>
</li>
<li>
<p>Usar algoritmo de modelo de seleção <a href="parte-2/10/../../referencias.html">[16]</a>.</p>
</li>
</ul>
</li>
<li>
<p>Regularização (Regularization):</p>
<ul>
<li>Manter todos os parâmetros, mas reduzir a magnitude dos parâmetros de \( \theta _j \);</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="função-custo-em-casos-de-overfitting"><a class="header" href="#função-custo-em-casos-de-overfitting">Função custo em casos de <em>overfitting</em></a></h1>
<p>Em casos de <em>overfitting</em> na função hipótese, podemos reduzir o peso dos termos de maior grau,
aumentando o seu custo.</p>
<p>Para isso, podemos utilizar do método de regularização para aumentar o custo de determinadas
variáveis. Por exemplo, se tivermos uma função hipótese com quatro parâmetros, teremos uma
função de grau quatro do tipo \( \theta _0 + \theta _1x + \theta _2x ^2 + \theta _3x ^3 + \theta _4x ^4 \).
Através da regularização, podemos reduzir o problema diminuindo a influência dos termos de grau três e quatro.</p>
<p>Podemos modificar a função custo a fim de reduzir os valores de \( \theta _3 \) e \( \theta _4 \) e
aumentar os valores de \( \theta _1 \) e \( \theta _2 \) da seguinte forma:</p>
<p>\[
\large{} min _{\theta} \ \frac{1}{2m} \sum _{i=1} ^m
\Big( h _{\theta}(x ^{(i)}) - y ^{(i)} \Big) ^2 + \lambda \sum _{j=1} ^n \theta _j ^2
\]</p>
<p>Chamamos a expressão acima de função custo regularizada. O valor de \( \lambda \) representa o parâmetro de
regularização e determina o quanto os custos dos parâmetros de \( \theta \) serão inflados. Caso selecionarmos
um valor muito alto para \( \lambda \), a função custo resultará em <em>underfitting</em>. Para isso devemos escolher
estrategicamente o valor de \( \lambda \).</p>
<p>Para exemplificar o método de regularização em casos de <em>overfitting</em>, podemos analisar a Figura 13
na seção anterior. Na imagem central, a curva representada em azul descreve a situação ótima do
limite de decisão para os dados. A regularização transforma uma curva complexa em <em>overfitting</em> em
uma curva ótima regularizada.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regressão-linear-regularizada"><a class="header" href="#regressão-linear-regularizada">Regressão linear regularizada</a></h1>
<p>Podemos utilizar o conceito de regularização para evitar problemas de <em>overfitting</em> no método de
regressão linear.</p>
<p>É possível modificar o método gradiente descendente de forma que consigamos regularizar a
atualização do valor de \( \theta \) conforme visto na Seção <a href="parte-2/10/../4/2-4.html">Gradiente Descendente (<em>Gradient Descent</em>)</a>:</p>
<hr />
<p><strong>Algorithm 5</strong> Algoritmo Gradiente Descendente Para Regressão Linear Regularizado</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:   
\(
\large{} \theta _0 := \theta _0 - \frac{\alpha}{m} \sum _{i=1} ^m
\Big( h _{\theta}(x ^{(i)}) - y ^{(i)} \Big) \cdot x _0 ^{(i)}
\)</p>
<p>4:   
\(
\large{} \theta _j := \theta _j - \Big[ \Big( \frac{1}{m} \sum _{i=1} ^m
(h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)} \Big) + \frac{\lambda}{m} \theta _j \Big]
\)
  \( \rhd j \in 1,2, \dots , n \)</p>
<p>5:   <strong>until</strong> \( convergir \)</p>
<p>6: <strong>end procedure</strong></p>
<hr />
<p>Podemos perceber que atualizamos o valor de \( \theta _0 \) separadamente a fim de focarmos apenas nos termos
de maior grau.</p>
<p>Além disso, podemos representar o mesmo algoritmo através da equação normal - descrita na Seção
<a href="parte-2/10/../7/2-7.html">Equação Normal (Normal Equation)</a> - da seguinte forma:</p>
<p>\[
\large{} \theta = (X ^T X + \lambda \cdot L) ^{-1} X ^T y
\]</p>
<p>\[
\large{} onde \ L =
\begin{bmatrix}
0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; \dots &amp;&amp; 0 \\
0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; \dots &amp;&amp; 0 \\
0 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; 0 &amp;&amp; \dots &amp;&amp; 0 \\
0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; 1 &amp;&amp; \dots &amp;&amp; 0 \\
0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; \ddots &amp;&amp; 0 \\
0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; 0 &amp;&amp; \dots &amp;&amp; 1
\end{bmatrix}
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regressão-logística-regularizada"><a class="header" href="#regressão-logística-regularizada">Regressão logística regularizada</a></h1>
<p>Podemos regularizar a regressão logística da mesma forma que regularizamos a função \( J \) para a
regressão linear. Com isso, podemos evitar casos de <em>overfitting</em> no método de regressão logística.</p>
<p>Assim, podemos escrever a função \( J(\theta) \) da seguinte forma:</p>
<hr />
<p>\[
J(\theta) =- \frac{1}{m} \sum _{i=1} ^m \Big[ y ^{(i)} \log (h _{\theta}(x ^{(i)})) +
(1-y ^{(i)}) \log (1-h _{\theta}(x ^{(i)})) \Big] + \frac{\lambda}{2m} \sum _{j=1} ^n \theta _j ^2
\]</p>
<hr />
<p>Com isso, da mesma forma que na regressão linear, podemos escrever o algoritmo gradiente
descendente com a regularização da função \( J \).</p>
<hr />
<p><strong>Algorithm 6</strong> Algoritmo Gradiente Descendente Para Regressão Logística Regularizado</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>repeat</strong></p>
<p>3:   
\( \large{} \theta _0 := \theta _0 - \frac{\alpha}{m} \sum _{i=1} ^m
\Big( h _{\theta}(x ^{(i)}) - y ^{(i)} \Big) \cdot x _0 ^{(i)} \)</p>
<p>4:   
\( \large{} \theta _j := \theta _j - \alpha \Big[ \frac{1}{m} \sum _{i=1} ^m
(h _{\theta}(x ^{(i)})-y ^{(i)}) \cdot x _j ^{(i)} + \frac{\lambda}{m} \theta _j \Big] \)
  \( \rhd j \in 1,2, \dots , n \)</p>
<p>5:   <strong>until</strong> \( convergir \)</p>
<p>6: <strong>end procedure</strong></p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-iii"><a class="header" href="#parte-iii">Parte III</a></h2>
<h1 id="redes-neurais"><a class="header" href="#redes-neurais">Redes Neurais</a></h1>
<ul>
<li>
<p><a href="./parte-3/1/3-1.html">Redes Neurais: Representação</a></p>
<ul>
<li>
<p><a href="./parte-3/1/3-1-1.html">Definição básica</a></p>
</li>
<li>
<p><a href="./parte-3/1/3-1-2.html">Hipótese não-linear</a></p>
</li>
<li>
<p><a href="./parte-3/1/3-1-3.html">Os Neurônios e o Cérebro</a></p>
</li>
<li>
<p><a href="./parte-3/1/3-1-4.html">Representação do modelo</a></p>
</li>
<li>
<p><a href="./parte-3/1/3-1-5.html">Aplicações</a></p>
</li>
<li>
<p><a href="./parte-3/1/3-1-6.html">Classificação Multiclasse</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/2/3-2.html">Redes Neurais: Aprendizado</a></p>
<ul>
<li>
<p><a href="./parte-3/2/3-2-1.html">Estrutura básica</a></p>
<ul>
<li>
<p><a href="./parte-3/2/3-2-1-1.html">Camada linear</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-1-2.html">Camada de ativação: Sigmoid layer</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-1-3.html">Cross entropy loss</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-1-4.html">Camada de ativação: Softmax layer</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-1-5.html">Camada de ativação: Rectified Linear layer (ReLU)</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-1-6.html">Camada de ativação: Hyperbolic tangent</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/2/3-2-2.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-3.html">Backpropagation Algorithm</a></p>
<ul>
<li>
<p><a href="./parte-3/2/3-2-3-1.html">Forward pass</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-3-2.html">Backward pass</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-3-3.html">Algoritmo</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/2/3-2-4.html">Otimizadores</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-5.html">Verificação do gradiente</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-6.html">Inicialização aleatória</a></p>
</li>
<li>
<p><a href="./parte-3/2/3-2-7.html">Organização do conhecimento</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/3/3-3.html">Aplicação de algoritmos de machine learning</a></p>
<ul>
<li>
<p><a href="./parte-3/3/3-3-1.html">Valoração de algoritmos de aprendizagem</a></p>
</li>
<li>
<p><a href="./parte-3/3/3-3-2.html">Curvas de aprendizado</a></p>
<ul>
<li>
<p><a href="./parte-3/3/3-3-2-1.html">High Bias</a></p>
</li>
<li>
<p><a href="./parte-3/3/3-3-2-2.html">High Variance</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/3/3-3-3.html">Decisões a serem tomadas</a></p>
</li>
<li>
<p><a href="./parte-3/3/3-3-4.html">Diagnosticando Redes Neurais</a></p>
</li>
<li>
<p><a href="./parte-3/3/3-3-5.html">Support Vector Machines (SVMs)</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-3/4/3-4.html">Naive Bayes</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redes-neurais-representação"><a class="header" href="#redes-neurais-representação">Redes Neurais: Representação</a></h1>
<ul>
<li>
<p><a href="parte-3/1/./3-1-1.html">Definição básica</a></p>
</li>
<li>
<p><a href="parte-3/1/./3-1-2.html">Hipótese não-linear</a></p>
</li>
<li>
<p><a href="parte-3/1/./3-1-3.html">Os Neurônios e o Cérebro</a></p>
</li>
<li>
<p><a href="parte-3/1/./3-1-4.html">Representação do modelo</a></p>
</li>
<li>
<p><a href="parte-3/1/./3-1-5.html">Aplicações</a></p>
</li>
<li>
<p><a href="parte-3/1/./3-1-6.html">Classificação Multiclasse</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="definição-básica"><a class="header" href="#definição-básica">Definição básica</a></h1>
<p>Redes neurais (do inglês <em>Neural Networks</em> ou <em>NNs</em>) são sistemas de computação com nós 
interconectados que funcionam como os neurônios do cérebro humano. Usando algoritmos, elas podem
reconhecer padrões escondidos e correlações em dados brutos, agrupá-los e classificá-los.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hipótese-não-linear"><a class="header" href="#hipótese-não-linear">Hipótese não-linear</a></h1>
<p>Nesta seção iremos discutir os principais fundamentos da hipótese não linear e as motivações para a
criação de sistemas de redes neurais.</p>
<p>Como vimos nas seções anteriores, para um problema de classificação não linear podemos expandir
a nossa função hipótese para mais termos e, para isso, devemos usar métodos de regularização para
manter o treino consistente e sem problemas de <em>overfitting</em>.</p>
<p>Problemas de redes neurais são utilizados para modelar problemas da vida real. Um bom exemplo
seria quando, a partir de um conjunto de fotos, queremos determinar se uma foto é de um carro
ou não. Um algoritmo utilizando redes neurais analisa cada pixel da foto e compara com os valores
adquiridos no treino. Esse tipo de problema também é um algoritmo de classificação, porém, agora,
utilizando redes neurais devido à sua complexidade.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="os-neurônios-e-o-cérebro"><a class="header" href="#os-neurônios-e-o-cérebro">Os Neurônios e o Cérebro</a></h1>
<p>Problemas utilizando redes neurais são relativamente antigos, tendo sua origem entre os anos 80
e 90. O objetivo era, basicamente, modelar o cérebro humano através de algoritmos. Por exemplo,
uma função hipótese seria modelar o córtex auditivo a fim de reconhecimento de áudio, ou a área
de associação visual responsável pela visão.</p>
<p>Biologicamente, o cérebro humano é composto por estruturas nervosas chamadas de neurônios.
Neurônios são células responsáveis pela transmissão dos impulsos nervosos e constituem cerca de
10% do tecido nervoso. Eles são constituídos basicamente por três estruturas: corpo celular, dendritos
e axônio, como está representado na Figura 14.</p>
<p align="center">
  <img src="parte-3/1/./img/14.png">
</p>
<p align="center">
Figura 14: Representação estrutural de um neurônio humano.
</p>
<p>Baseando-se nessa estrutura, as redes neurais tem como objetivo modelar computacionalmente as
funções especificadas dos neurônios, como por exemplo, entendimento e geração de texto, análise e
classificação de imagens, entre outros.</p>
<p>Na seção seguinte serão apresentadas as modelagens principais das redes neurais artificiais.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="representação-do-modelo"><a class="header" href="#representação-do-modelo">Representação do modelo</a></h1>
<p>A partir de uma análise biológica do cérebro humano, sabemos que dois neurônios se comunicam
através de impulsos nervosos chamados de sinapses. A informação recebida por um neurônio passa,
primeiramente, pelos dendritos e vai em direção ao axônio. Podemos modelar essas informações a
fim de gerar um modelo matemático que represente um neurônio da seguinte forma:</p>
<ul>
<li>
<p>Dendrito: entrada da função \( (x _1, \dots , x _n) \);</p>
</li>
<li>
<p>Axônio: função hipótese \( (h _{\theta}(x)) \);</p>
</li>
</ul>
<p>Nas redes neurais, utilizamos a mesma função utilizada na regressão logística \( \large{} \frac{1}{1+e ^{-z}} \),
a qual chamamos de &quot;ativação&quot; da função e os parâmetros \( \Theta \) são chamados de &quot;pesos&quot;. Podemos perceber
na Figura 15 que o neurônio possui três camadas, as quais chamamos de camada de entrada (<em>input layer</em>),
camada escondida (<em>hidden layer</em>) e camada de saída (<em>output layer</em>). A camada de saída também pode ser
chamada de função hipótese \( h _{\Theta} \).</p>
<p>Com isso, devemos adicionar algumas notações relacionadas às redes neurais.</p>
<ul>
<li>
<p>Chamamos de \( x _0, x _1, \dots , x _n \) os valores da <em>input layer</em>, onde \( x _0 = 1 \) (<em>bias unit</em>);</p>
</li>
<li>
<p>\( a _i ^{(j)} \) é unidade de &quot;ativação&quot; \( i \) na camada \( j \);</p>
</li>
<li>
<p>\( \Theta ^{(j)} \) é a matriz de pesos que controla o mapeamento da função da camada \( j \)
para a camada \( j+1 \).</p>
</li>
</ul>
<p align="center">
  <img src="parte-3/1/./img/15.png">
</p>
<p align="center">
Figura 15: Representação de uma rede neural
</p>
<p>Na Figura 15 podemos perceber que para gerar a saída (função hipótese) passamos por uma <em>input layer</em> e
uma <em>hidden layer</em>. Com isso, podemos descrever uma equação para que possamos determinar
o valor da saída partindo dos parâmetros acimas identificados.</p>
<p>\[
\large{} a _1 ^{(2)} = g( \Theta _{10} ^{(1)} x _0 + \Theta _{11} ^{(1)} x _1 +
\Theta _{12} ^{(1)} x _2 + \Theta _{13} ^{(1)} x _3)
\]</p>
<p>\[
\large{} a _2 ^{(2)} = g( \Theta _{20} ^{(1)} x _0 + \Theta _{21} ^{(1)} x _1 +
\Theta _{22} ^{(1)} x _2 + \Theta _{23} ^{(1)} x _3)
\]</p>
<p>\[
\large{} a _3 ^{(2)} = g( \Theta _{30} ^{(1)} x _0 + \Theta _{31} ^{(1)} x _1 +
\Theta _{32} ^{(1)} x _2 + \Theta _{33} ^{(1)} x _3)
\]</p>
<p>\[
\large{} h _{\Theta}(x) = a _1 ^{(3)} = g( \Theta _{10} ^{(2)} a _0 ^{(2)} + \Theta _{11} ^{(2)} a _1 ^{(2)} +
\Theta _{12} ^{(2)} a _2 ^{(2)} + \Theta _{13} ^{(2)} a _3 ^{(2)})
\]</p>
<p>Percebe-se que a matriz \( \Theta \) é uma matriz de tamanho \( 3 \times 4 \) (3 = número de parâmetros na camada
dois e 4 = número de parâmetros de entrada). Quando aplicamos a função sigmoide para cada uma
das camadas, nós obtemos o nodo de ativação da próxima camada. Assim, a função hipótese é a
função logística aplicada na soma dos valores dos nodos de ativação os quais são multiplicados pelo
parâmetro da matriz \( \Theta ^{(2)} \) que contém os valores dos pesos da segunda camada de nodos.</p>
<p>Com isso, temos a seguinte definição:</p>
<p align="center">
<b>Se uma rede neural tem</b> \( s _j \) <b>unidades na camada</b> \( j \) <b>e</b> \( s _{j+1} \) <b>unidades na camada</b>
\( j+1 \)<b>, então</b> \( \Theta ^{(j)} \) <b>terá dimensões</b> \( s _{j+1} \times (s _j + 1) \)<b>.</b>
</p>
<p>Com as definições vistas acima, podemos introduzir uma implementação vetorizada das funções vistas. Para isso, iremos definir
uma variável \( z _k ^{(j)} \) que engloba os parâmetros dentro da função \( g \). Assim, teríamos:</p>
<p>\[
\large{} a _1 ^{(2)} = g(z _1 ^{(2)})
\]</p>
<p>\[
\large{} a _2 ^{(2)} = g(z _2 ^{(2)})
\]</p>
<p>\[
\large{} a _3 ^{(2)} = g(z _3 ^{(2)})
\]</p>
<p>Em outras palavras, para a camada \( j=2 \), a função \( z \) seria:</p>
<p>\[
\large{} z _k ^{(2)} = \Theta _{k,0} ^{(1)} x _0 + \Theta _{k,1} ^{(1)} x _1 +
\Theta _{k,2} ^{(1)} x _2 + \dots + \Theta _{k,n} ^{(1)} x _n
\]</p>
<p>E a função vetorizada teria a forma:</p>
<p>\[
\large{} x =
\begin{bmatrix}
x _0 \\ x _2 \\ \dots \\ x _n
\end{bmatrix}
, z ^{(j)} =
\begin{bmatrix}
z _1 ^{(j)} \\ z _2 ^{(j)} \\ \dots \\ z _n ^{(j)}
\end{bmatrix}
\]</p>
<p>Com \( x = a ^{(1)} \) teremos:</p>
<p>\[
\large{} z ^{(j-1)} = \Theta ^{(j-1)} a ^{(j-1)}
\]</p>
<p>Com essas manipulações temos que a função hipótese pode ser definida da seguinte forma:</p>
<hr />
<p>\[
\large{} h _{\Theta}(x) = a ^{(j+1)} = g(z ^{(j+1)})
\]</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="aplicações"><a class="header" href="#aplicações">Aplicações</a></h1>
<p>Mas afinal, para que uma rede neural pode ser útil? Ao longo das últimas décadas, a área de
pesquisa que envolve redes neurais está se desenvolvendo de maneira significativa. Apesar desse
desenvolvimento ainda estar no começo, já foram descobertas diversas aplicações de redes neurais,
como podem ser vistas a seguir:</p>
<ul>
<li>
<p><em>Computer Vision</em>: reconhecimento de objetos, emoções, etc;</p>
</li>
<li>
<p><em>Natural Language Processing</em> (NLP): reconhecimento de discursos e tradução de idiomas;</p>
</li>
<li>
<p><em>Deep Q-Network</em> (DQN) e <em>Asynchronous Actor-Critic Agents</em> (A3C) para <em>reinforcement learning</em>,
como por exemplo Atari;</p>
</li>
<li>
<p>AlphaGo <a href="parte-3/1/../../referencias.html">[19]</a>;</p>
</li>
<li>
<p><em>Aesthetic quality assessment</em></p>
</li>
</ul>
<p>Para o melhor entendimento do tipo de implementação que uma rede neural linear com apenas duas
camadas - <em>input</em> e <em>output</em> - podemos criar um vetor para \( \Theta ^{(1)} \) para computar a
função \( x _1 AND x _2 \) da seguinte forma:</p>
<p>\[
\large{} \Theta ^{(1)} =
\begin{bmatrix}
-30 &amp;&amp; 20 &amp;&amp; 20
\end{bmatrix}
\]</p>
<p>Percebemos que no exemplo não temos <em>hidden layers</em>. Temos a <em>input layer</em> seguida pelo
processamento da função hipótese.</p>
<p>Como sabemos que, pela função sigmoide, com uma entrada de duas variáveis \( x _1 \) e \( x _2 \) teremos
valores binários (0 ou 1) na saída da função hipótese \( h _{\Theta}(x)=g(-30+20x _1 + 20x _2) \). Essa ideia
pode ser representada na tabela a seguir:</p>
<p align="center">
Tabela 2: Implementação da rede neural usando portas 'and' \( x _1 AND x _2 \)
</p>
<table><thead><tr><th align="center">\( x _1 \)</th><th align="center">\( x _2 \)</th><th align="center">\( h _{\Theta}(x) \)</th></tr></thead><tbody>
<tr><td align="center">0</td><td align="center">0</td><td align="center">\( g(-30) \approx 0 \)</td></tr>
<tr><td align="center">0</td><td align="center">1</td><td align="center">\( g(-10) \approx 0 \)</td></tr>
<tr><td align="center">1</td><td align="center">0</td><td align="center">\( g(-10) \approx 0 \)</td></tr>
<tr><td align="center">1</td><td align="center">1</td><td align="center">\( g(10) \approx 1 \)</td></tr>
</tbody></table>
<p>Como vimos até agora, representamos redes neurais simples com duas camadas - <em>input</em> e <em>output</em> -
chamadas de <em>Perceptrons</em>. Contudo, esse tipo de implementação possui diversas limitações. A
primeira delas foi observada ao tentar implementar a função \( XOR \). Essa limitação potencializou a
criação de novas arquiteturas de redes neurais, pois a função \( XOR \) não é linearmente separável, ou
seja, não se pode, de maneira linear - com apenas duas camadas - representá-la em uma rede neural.
A Figura 16, abaixo, representa o surgimento desse problema.</p>
<p align="center">
  <img src="parte-3/1/./img/16.png">
</p>
<p align="center">
Figura 16: No gráfico da esquerda, percebemos a aplicação da função \( OR \), cujos pontos verdes representam a saída 1
e os vermelhos a saída zero. No gráfico da esquerda, percebemos a aplicação da função \( XOR \), cujos pontos possuem
a mesma representatividade. Na função \( OR \), podemos traçar uma reta que divida as classes 0 e 1 do problema,
enquanto na função \( XOR \) isso não é possível devido ao fato das classes não sejam linearmente separáveis.
</p>
<p>Uma solução para este problema foi a criação de novas camadas internas, chamadas de <em>hidden layers</em>,
que foram discutidas na Seção <a href="parte-3/1/./3-1-4.html">Representação do modelo</a>, dando origem a uma nova arquitetura de rede neural chamada
<em>Multilayer perceptron</em> (MLP). Na Figura 17 abaixo, está representada a arquitetura de rede neural
para a resolução do problema \( XOR\).</p>
<p align="center">
  <img src="parte-3/1/./img/17.png">
</p>
<p align="center">
Figura 17: Representação da arquitetura que soluciona o problema \( XOR \). Neste caso, temos três camadas:
<i>input layer, hidden layer</i> e <i>output layer</i>
</p>
<p>A seguir, iremos discutir as diferentes arquiteturas de redes neurais para diferentes tipos de classifi-
cação.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classificação-multiclasse-1"><a class="header" href="#classificação-multiclasse-1">Classificação Multiclasse</a></h1>
<p>Em problemas que são utilizados redes neurais, comumente são utilizados métodos de resolução
desses problemas de forma multiclasse. Em outras palavras, a rede neural não terá apenas uma
função hipótese como saída, \( h _{\Theta}(x) \) nesses casos será um vetor que pertence a
\( \mathbb{R} ^n \).</p>
<p>Na Figura 18 abaixo podemos perceber que a rede neural foi construída com o objetivo de resolver
um problema de classificação multiclasse, pois percebemos que a camada verde, abaixo (&quot;<em>layer 4</em>&quot;),
que representa a função hipótese, é um vetor que pertence a \( \mathbb{R} ^4 \)</p>
<p align="center">
  <img src="parte-3/1/./img/18.png">
</p>
<p align="center">
Figura 18: Representação de uma rede neural com classificação multiclasse
</p>
<p>É possível escrever um programa de visão computacional para que possamos diferenciar pedestres,
carros, motos e caminhões. Assim, teremos quatro nodos na output layer que representam cada um
dos tipos de automóveis descritos.</p>
<p>\[
\large{} h _{\Theta}(x) ^1 \approx
\begin{bmatrix}
1 \\ 0 \\ 0 \\ 0
\end{bmatrix}
, \
h _{\Theta}(x) ^2 \approx
\begin{bmatrix}
0 \\ 1 \\ 0 \\ 0
\end{bmatrix}
, \
h _{\Theta}(x) ^3 \approx
\begin{bmatrix}
0 \\ 0 \\ 1 \\ 0
\end{bmatrix}
, \
h _{\Theta}(x) ^4 \approx
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 1
\end{bmatrix}
\]</p>
<p>Onde \( h _{\Theta}(x) ^1 \) representa um pedestre, \( h _{\Theta}(x) ^2 \) representa um carro,
\( h _{\Theta}(x) ^3 \) representa uma moto e \( h _{\Theta}(x) ^4 \) representa um caminhão.
Dessa forma, a rede neural poderia ser modelada, em forma vetorial, da seguinte forma:</p>
<p>\[
\large{}
\begin{bmatrix}
x _0 \\ x _1 \\ x _2 \\ \dots \\ x _n
\end{bmatrix}
\rightarrow
\begin{bmatrix}
a _0 ^{(2)} \\ a _1 ^{(2)} \\ a _2 ^{(2)} \\ a _3 ^{(2)} \\ \dots
\end{bmatrix}
\rightarrow
\begin{bmatrix}
a _0 ^{(3)} \\ a _1 ^{(3)} \\ a _2 ^{(3)} \\ a _3 ^{(3)} \\ \dots
\end{bmatrix}
\rightarrow
\dots
\rightarrow
\begin{bmatrix}
h _{\Theta}(x) _1 \\ h _{\Theta}(x) _2 \\ h _{\Theta}(x) _3 \\ h _{\Theta}(x) _4
\end{bmatrix}
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redes-neurais-aprendizado"><a class="header" href="#redes-neurais-aprendizado">Redes Neurais: Aprendizado</a></h1>
<ul>
<li>
<p><a href="parte-3/2/./3-2-1.html">Estrutura básica</a></p>
<ul>
<li>
<p><a href="parte-3/2/./3-2-1-1.html">Camada linear</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-1-2.html">Camada de ativação: Sigmoid layer</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-1-3.html">Cross entropy loss</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-1-4.html">Camada de ativação: Softmax layer</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-1-5.html">Camada de ativação: Rectified Linear layer (ReLU)</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-1-6.html">Camada de ativação: Hyperbolic tangent</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-3/2/./3-2-2.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-3.html">Backpropagation Algorithm</a></p>
<ul>
<li>
<p><a href="parte-3/2/./3-2-3-1.html">Forward pass</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-3-2.html">Backward pass</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-3-3.html">Algoritmo</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-3/2/./3-2-4.html">Otimizadores</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-5.html">Verificação do gradiente</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-6.html">Inicialização aleatória</a></p>
</li>
<li>
<p><a href="parte-3/2/./3-2-7.html">Organização do conhecimento</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="estrutura-básica"><a class="header" href="#estrutura-básica">Estrutura básica</a></h1>
<p>Uma rede neural consiste em uma sequência de camadas nas quais os dados são aplicados a transformações
lineares e não lineares. Cada uma dessas camadas é composta por neurônios (<em>neurons</em> ou <em>units</em>) e cada
um desses neurônios estão conectados com os próximos, presentes na camada seguinte.
Essas conexões são chamadas de pesos (<em>weights</em>) - um valor numérico. E, além disso, cada camada
possui um valor fixo numérico, chamado de <em>bias</em>. Assim, os dados passamos como entrada começam
na <em>input layer</em>, passando por transformações lineares e não lineares nas camadas internas até atingir
a <em>output layer</em>.</p>
<p>Com isso, o aprendizado de uma rede neural tem como objetivo otimizar a função custo (<em>loss function</em>) de
acordo com os parâmetros, geralmente através da regra da cadeia ou através do método
do gradiente descendente.</p>
<p>Nas próximas subseções iremos discutir os conceitos fundamentais para a estruturação básica de uma
rede neural, apresentando os principais modelos e arquiteturas utilizados.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camada-linear"><a class="header" href="#camada-linear">Camada linear</a></h1>
<p>Como foi apresentado nas seções anteriores, uma rede neural possui diversas camadas e cada uma
delas executa uma função sobre um dado. A primeira camada que iremos discutir é a camada mais
básica, a qual executa transformações lineares sobre os dados. A camada linear - do inglês <em>linear layer</em> -
realiza o treino das camadas intermediárias da rede neural baseado no método de regressão
linear (Seção <a href="parte-3/2/../../parte-2/2/2-2.html">Regressão linear</a>).</p>
<p>A camada linear realiza uma soma ponderada dos dados de entrada, ou seja, realiza uma soma dos
resultados de funções afins da seguinte forma:</p>
<hr />
<p>\[
\large{} y = \sum _i w _i x _i + b
\]</p>
<hr />
<p>Onde \( w \) são as conexões de cada neurônio da camada com os pesos (<em>weight</em>), \( x \) são os valores
dos neurônios conectados, \( b \) é o valor numérico <em>bias</em> de cada camada (constante), \( i \) é o número de
conexões e \( y \) é o valor de saída do neurônio atual.</p>
<p>Os valores de \( y \) retornados irão percorrer toda a rede neural. Para isso, temos camadas de ativação as
quais irão adicionar complexidade e dimensionalidade para a rede neural. Essas camadas tem como
objetivo realizar transformações não-lineares dentro da rede. Com essas transformações, podemos
classificar dados mais complexos e que não se comportam de maneira linear. Essas camadas de ativação
podem ter diferentes formas e a mais comum delas é a chamada <em>dense layer</em>, cujos neurônios das
camadas subsequentes estão, de alguma forma, conectados com os neurônios da camada precedente.</p>
<p>A seguir, iremos discutir diferentes implementações da camada de ativação usando diferentes modelos
matemáticos</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camada-de-ativação-sigmoid-layer"><a class="header" href="#camada-de-ativação-sigmoid-layer">Camada de ativação: <em>Sigmoid layer</em></a></h1>
<p>A camada de ativação - do inglês <em>activation layer</em> ou <em>sigmoid layer</em> - realiza o cálculo da função
sigmoide através de operações ponto a ponto não lineares. A função sigmoide, assim, tem a seguinte estrutura:</p>
<hr />
<p>\[
\large{} g(x)=sigmoid(x)= \frac{1}{1+e ^{-x}}
\]</p>
<hr />
<p>Geralmente, aplicamos a função sigmoide a após realizarmos o cálculo da camada linear, ou seja:</p>
<hr />
<p>\[
\large{} y=g \Big( \sum _i w _i x _i + b \Big)
\]</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="cross-entropy-loss"><a class="header" href="#cross-entropy-loss"><em>Cross entropy loss</em></a></h1>
<p><em>Cross entropy loss</em> tem como objetivo calcular o quão discrepante do valor esperado está o valor
produzido como saída na rede neural. Como foi visto na Seção <a href="parte-3/2/../../parte-2/9/2-9-1.html">Função Custo (Cost Function)</a>.
Essa função tem a seguinte estrutura:</p>
<hr />
<p>\[
\large{}
\mathcal{L} _{CE}(y,g(z))= - \frac{1}{m} \Big[ \sum _{j=1} ^m y \log (g(z)) +
(1-y) \log (1-g(z)) \Big]
\]</p>
<hr />
<p>Essa função tem o mesmo objetivo da função custo anteriormente vista e que será detalhada na
Seção <a href="parte-3/2/./3-2-2.html">Função Custo (Cost Function)</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camada-de-ativação-softmax-layer"><a class="header" href="#camada-de-ativação-softmax-layer">Camada de ativação: <em>Softmax layer</em></a></h1>
<p>A função <em>softmax</em> é uma generalização da função logística para múltiplas dimensões. Essa função é
muito utilizada em classificações multiclasse. A função softmax tem a seguinte estrutura:</p>
<hr />
<p>\[
\large{} y=f _{SM}(x) = softmax(x), \ \ onde \ \ y _i =
\frac{e ^{x _i}}{\sum _{j=1} ^K e ^{x _j}}
\]</p>
<hr />
<p>Ela recebe como entrada um vetor \( x \) e para cada uma das classes ela gera um valor entre -1 e 1,
como se pode perceber na Figura 19.</p>
<p align="center">
  <img src="parte-3/2/./img/19.png">
</p>
<p align="center">
Figura 19: Representação da função softmax
</p>
<p>Essa função representa a distribuição de probabilidade sobre os índices \( K \) de \( y \).</p>
<p>Da mesma forma que a camada de ativação, podemos aplicar a saída dessa função na função <em>cross entropy loss</em>.</p>
<p>\[
\large{} \mathcal{L} _{CE}(y,g(z))=- \sum _{j=1} ^k y \log (f _{SM}(x _j)) =-
\sum _{j=1} ^k y \Big[ x _j - \log \sum _{l=1} ^k e ^{x _l} \Big]
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camada-de-ativação-rectified-linear-layer-relu"><a class="header" href="#camada-de-ativação-rectified-linear-layer-relu">Camada de ativação: <em>Rectified Linear layer</em> (ReLU)</a></h1>
<p>A <em>Rectified Linear Layer</em> é uma função de ativação, assim como a função sigmoide que nos retorna
<em>Rectified Linear Units</em> (ReLUs). É uma das funções de ativação mais utilizadas, atualmente, nas
implementações de redes neurais devido ao fato de ser mais simples e mais barata que a função
sigmoide.</p>
<p>Podemos implementar a ativação ReLU da seguinte forma:</p>
<hr />
<p>\[
\large{} y = f _{relu}(x)=relu(x), \ \ onde \ \ y _i = max(0, x _i)
\]</p>
<hr />
<p>Para melhor compreensão, a Figura 20 representa o comportamento da função \( relu(x) \) de acordo
com os parâmetros \( x \) de entrada.</p>
<p align="center">
  <img src="parte-3/2/./img/20.png">
</p>
<p align="center">
Figura 20: Representação da função ReLU usada para a camada de ativação
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camada-de-ativação-hyperbolic-tangent"><a class="header" href="#camada-de-ativação-hyperbolic-tangent">Camada de ativação: <em>Hyperbolic tangent</em></a></h1>
<p>A função <em>hyperbolic tangent</em>, ou \( tanh \) é muito similiar a função de ativação <em>softmax</em>. Essa função é
muito utilizada redes neurais recorrentes (Seção
<a href="parte-3/2/../../parte-6/2/6-2.html">Redes neurais recorrentes (<em>Recurrent neural networks</em>)</a>), especialmente nas arquiteturas GRU e LSTM.
A função \( tanh \) tem a seguinte estrutura:</p>
<hr />
<p>\[
\large{} y=tanh(z)= \frac{e ^z - e ^{-z}}{e ^z + e ^{-z}}
\]</p>
<hr />
<p>Assim como a <em>softmax</em>, a função \( tanh \) também gera valores entre -1 e 1, como percebemos na
Figura 21, porém não gera a distribuição para \( K \) valores de classe.</p>
<p align="center">
  <img src="parte-3/2/./img/21.png">
</p>
<p align="center">
Figura 21: Representação da função \( tanh \) usada para a camada de ativação
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="função-custo-cost-function-2"><a class="header" href="#função-custo-cost-function-2">Função Custo (<em>Cost Function</em>)</a></h1>
<p>Primeiramente, precisamos definir as variáveis que serão usadas para definir a função custo.</p>
<ul>
<li>
<p>\( L \) = número total de camadas (<em>layers</em>) na rede neural;</p>
</li>
<li>
<p>\( s _l \) = número de unidades na camada \( l \) (sem contar a unidade bias);</p>
</li>
<li>
<p>\( K \) = número de unidades/classes de saída (<em>output layer</em>).</p>
</li>
</ul>
<p>Com isso podemos definir a função custo que será utilizada para calcular o custo de uma rede
neural. Utilizaremos como base a função custo da regressão logística vista na Seção
<a href="parte-3/2/../../parte-2/9/2-9-1.html">Função Custo (Cost Function)</a> a fim de
determinarmos o valor da saída da função \( J(\Theta) \).</p>
<p>A função custo para redes neurais tem o seguinte formato:</p>
<hr />
<p>\[
J(\Theta)=- \frac{1}{m} \sum _{i=1} ^m \sum _{k=1} ^K \Big[ y _k ^{(i)} \log ((h _{\Theta}(x ^{(i)})) _k) +
(1-y _k ^{(i)}) \log (1-(h _{\Theta}(x ^{(i)})) _k) \Big] + \frac{\lambda}{2m} \sum _{l=1} ^{L-1}
\sum _{i=1} ^{s _l} \sum _{j=1} ^{s _{l+1}} (\Theta _{j,i} ^{(l)}) ^2
\]</p>
<hr />
<p>Podemos comparar esta equação com a equação da função custo da regressão logística. Assim,
percebe-se que adicionamos alguns somatórios. Nos dois primeiros somatórios, de \( k=1 \) até \( K \) e de
\( i=1 \) até \( m \), somamos os valores do retorno da função custo da regressão logística para cada nodo
da <em>output layer</em>. Nos três últimos somatórios, de \( j=1 \) até \( s _{l+1} \), de \( i=1 \) até
\( s _l \) e de \( l=1 \) até \( L=1 \), somamos os valores dos quadrados de todos os valores de
\( \Theta \) individuais em toda a rede neural.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backpropagation-algorithm"><a class="header" href="#backpropagation-algorithm"><em>Backpropagation Algorithm</em></a></h1>
<p>O algoritmo de <em>Backpropagation</em> é, sem dúvida, o algoritmo mais importante para as redes neurais.
É com esse algoritmo que as redes neurais aprendem, essencialmente.</p>
<p>Como foram vistos nas seções anteriores, existem algoritmos que são usados com o objetivo de
minimizar a função custo. Para redes neurais, usa-se um algoritmo chamado <em>Backpropagation</em> que
tem o mesmo objetivo: minimizar a função custo, ou seja \( \underset{\Theta}{min} J(\Theta) \).
A minimização ocorre após realizarmos o processo de <em>forward propagation</em> usando o método de gradiente
descendente - visto na Seção <a href="parte-3/2/../../parte-2/4/2-4.html">Gradiente Descendente (Gradient Descent)</a> -,
atualizando os valores das <em>hidden layers</em> de acordo com os valores retornados da função custo.</p>
<p>Em geral, o algoritmo de <em>backpropagation</em> acontece em duas fases principais que serão discutidas em
detalhes a seguir. Essas fases são:</p>
<ol>
<li>
<p><strong><em>Forward pass</em></strong>: nossas entradas são passadas através da rede e as previsões de saída são
obtidas. Nessa fase, calculamos a função custo e computamos as funções de ativação para
cada transição de camada.</p>
</li>
<li>
<p><strong><em>Backward pass</em></strong>: calculamos o gradiente da função custo na camada final (<em>output layer</em>) e
usamos esse gradiente para aplicar recursivamente a regra da cadeia para atualizar os pesos da
rede neural.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="forward-pass"><a class="header" href="#forward-pass"><em>Forward pass</em></a></h1>
<p>O principal objetivo dessa fase é calcular os valores de cada neurônio da nossa rede neural aplicando
uma série de <em>dot products</em> (multiplicações entre vetores) e funções de ativação a fim de atingirmos
a camada final da rede.</p>
<p>Por exemplo, se temos os seguintes valores de entrada \( x _i \) e a saída esperada \( y \), de acordo com a
tabela a seguir:</p>
<p align="center">
Tabela 3: Valores hipotéticos de entrada para a rede neural seguindo uma tupla do tipo \( (x, y) \)
</p>
<table><thead><tr><th align="center">\( x _0 \)</th><th align="center">\( x _1 \)</th><th align="center">\( x _2 \) <strong>(<em>bias</em>)</strong></th><th align="center">\( y \)</th></tr></thead><tbody>
<tr><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr>
<tr><td align="center">0</td><td align="center">1</td><td align="center">1</td><td align="center">1</td></tr>
<tr><td align="center">1</td><td align="center">0</td><td align="center">1</td><td align="center">1</td></tr>
<tr><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">0</td></tr>
</tbody></table>
<p>Cada um desses valores de \( x _i \) estarão presentes na <em>input layer</em> da rede neural. E para cada um desses
valores, serão realizados <em>dot products</em> entre as entradas de cada camada seguido pelo cálculo da
função de ativação. Esta ideia está exemplificada na Figura 22 a seguir.</p>
<p align="center">
  <img src="parte-3/2/./img/22.png">
</p>
<p align="center">
Figura 22: Exemplificação do algoritmo <i>backpropagation</i> a partir dos inputs definidos na Tabela 3. Na <i>input layer</i>
estão os valores de \( x \) definidos. Os valores das arestas são inicializados aleatoriamente e representam os pesos. Na
<i>hidden layer</i> estão os valores calculados a partir dos <i>dot products</i> e das aplicações das funções de ativação (neste
caso, sigmoide) e na <i>output layer</i> o valor calculado pela função custo.
</p>
<p>A partir dos valores de entrada baseados na Tabela 3 e os pesos que foram inicializados aleatoriamente,
para cada um desses valores executamos as seguintes operações de <em>dot products</em> e ativações utilizando
a função \( g(z) \) sigmoide:</p>
<ol>
<li>
<p>\( g((0 \times 0.351) + (1 \times 1.076) + (1 \times 1.116)) = 0.899 \)</p>
</li>
<li>
<p>\( g((0 \times 0.097) + (1 \times 0.165) + (1 \times 0.542)) = 0.593 \)</p>
</li>
<li>
<p>\( g((0 \times 0.457) + (1 \times 0.165) + (1 \times 0.331)) = 0.378 \)</p>
</li>
</ol>
<p>Os valores dos neurônios das <em>hidden layers</em> são atualizados de acordo com essas operações e, com
eles podemos aplicar mais uma vez as mesmas operações para cada um dos valores atualizados para
gerarmos o valor da <em>output layer</em>.</p>
<p>\[
\large{}
g((0.899 \times 0.383) + (0.593 \times -0.327) + (0.378 \times -0.329)) = 0.506
\]</p>
<p>A saída é, portanto, \( 0.506 \), o que representa um valor de probabilidade de 50,6%. Contudo,
percebe-se que a rede neural não tem muita confiança a respeito do valor gerado, então, para que a rede
neural realmente aprenda precisamos realizar a minimização da função custo através da etapa de
<em>Backward pass</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backward-pass"><a class="header" href="#backward-pass"><em>Backward pass</em></a></h1>
<p>Para otimizar a função custo, desejamos selecionar pesos que fornecem uma estimativa ótima de
uma função que modela nossos dados de treinamento. Ou seja, desejamos encontrar um conjunto
de pesos \( \Theta \) que minimize a saída \( J(\Theta) \).</p>
<p>Para aplicar o algoritmo de <em>backpropagation</em> a nossa função de ativação deve ser diferenciável, de
modo que possamos calcular a derivada parcial do erro em relação a um dado peso \( \Theta ^{(L)} \), o custo
\( \mathcal{L} \), saída do nó \( a ^{(L)} \) da <em>hidden layer</em> e saída da rede \( z ^{(L)} \).</p>
<hr />
<p>\[
\large{} \frac{\partial \mathcal{L}}{\partial \Theta ^{(L)}} =
\frac{\partial z ^{(L)}}{\partial \Theta ^{(L)}} \frac{\partial a ^{(L)}}{\partial z ^{(L)}}
\frac{\partial \mathcal{L}}{\partial a ^{(L)}}
\]</p>
<hr />
<p>A saída dessa equação é uma função composta dos pesos, entrada e funções de ativação.</p>
<p>Podemos analisar cada um desses termos separadamente.</p>
<p>\[
\large{} \frac{\partial \mathcal{L}}{\partial a ^{(L)}} = 2(a ^{(L)}-y), \ \
\frac{\partial a ^{(L)}}{\partial z ^{(L)}} = g'(z ^{(L)}), \ \
\frac{\partial z ^{(L)}}{\partial \Theta ^{(L)}} = a ^{(L-1)}
\]</p>
<p>Então, seguindo o exemplo da Figura 22, a partir do valor de saída, iremos calcular a derivada da
respectiva camada baseando-se em relação aos pesos que nela estão conectados. Essa derivada é
uma composição de multiplicações de derivadas do custo calculado em relação a camada anterior, a
camada anterior em relação a saída da rede neural e a saída da rede neural em relação aos pesos.</p>
<p>Com isso, podemos minimizar o erro dos pesos seguindo a seguinte ideia:</p>
<p>\[
\large{} Novo \ Peso = Peso \ Antigo - Derivada \times Taxa \ de \ aprendizado
\]</p>
<p>Para o caso de uma camada de uma rede neural, nós temos uma função com \( n \) entradas (número
de neurônios da <em>input layer</em>) e \( m \) saídas (número de neurônios da <em>output layer</em>). Então, este é o
caso que teremos uma matriz de derivadas parciais que se refere ao Jacobiano. Um Jacobiano é uma
matriz de derivadas parciais \( mxn \), em outras palavras</p>
<p>\[
\large{}
\frac{\partial \mathbf{f}}{\partial \mathbf{x}} =
\begin{bmatrix}
\frac{\partial f _1}{\partial x _1} &amp;&amp; \cdots &amp;&amp; \frac{\partial f _1}{\partial x _n} \\
\vdots &amp;&amp; \ddots &amp;&amp; \vdots \\
\frac{\partial f _m}{\partial x _1} &amp;&amp; \cdots &amp;&amp; \frac{\partial f _m}{\partial x _n}
\end{bmatrix}
\rightarrow
\Big( \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \Big) _{ij} = \frac{\partial f _i}{\partial x _j}
\]</p>
<p>Podemos exemplificar o cálculo das derivadas parciais pensando em um exemplo simples com operações
matemáticas. A Figura 23 a seguir representa uma rede de quatro camadas: camada de
entrada, com os valores \( x, y \) e \( z \), camada interna com operações soma (\( + \)) e \( max \),
camada interna com a operação multiplicação (\( * \)) e a camada de saída. Percebe-se que executamos a operação
\( f=(x+y)*max(y,z) \). Supondo que os valores das arestas são os valores retornados na saída de
cada camada, temos que o valor da função \( f=6 \).</p>
<p align="center">
  <img src="parte-3/2/./img/23.png">
</p>
<p align="center">
Figura 23
</p>
<p>Com isso podemos gerar três valores internos distintos que representam a função:</p>
<p>\[
\large{} a = x + y
\]</p>
<p>\[
\large{} b = max(y,z)
\]</p>
<p>\[
\large{} f = ab
\]</p>
<p>onde \( x=1, y=2 \) e \( z=0 \).</p>
<p>Podemos realizar a <em>backpropagation</em> nessa rede calculando as derivadas parciais de cada um dos
níveis a respeito ao nível anterior, como vimos anteriormente. Para isso, para facilitar a compreensão,
calculamos as derivadas parciais locais, da seguinte forma:</p>
<p>\[
\large{} \frac{\partial a}{\partial x} = 1, \ \frac{\partial a}{\partial y} = 1
\]</p>
<p>\[
\large{} \frac{\partial b}{\partial y} = 1(y &gt; z) = 1, \
\frac{\partial b}{\partial z} = 1(z &gt; y) = 0
\]</p>
<p>\[
\large{} \frac{\partial f}{\partial a} = b = 2, \ \frac{\partial f}{\partial b} = a = 3
\]</p>
<p>\[
\large{} \frac{\partial f}{\partial f} = 1
\]</p>
<p>Com esses valores definidos, podemos calcular a aplicação da regra da cadeia para atualizar os
valores internos dos nodos. Os valores representados em azul na Figura 24 são as multiplicações
das derivadas resultantes da função em relação ao estado atual com a derivada da função gerada no
estado imediatamente anterior.</p>
<p align="center">
  <img src="parte-3/2/./img/24.png">
</p>
<p align="center">
Figura 24
</p>
<p>Assim, geramos a seguinte atualização das derivadas da função em relação a cada uma das variáveis
de entrada:</p>
<p>\[
\large{} \frac{\partial f}{\partial x} = 2
\]</p>
<p>\[
\large{} \frac{\partial f}{\partial y} = 3+2 = 5
\]</p>
<p>\[
\large{} \frac{\partial f}{\partial z} = 0
\]</p>
<p>Portanto, na prática, <em>backpropagation</em> é apenas uma aplicação recursiva da regra da cadeia por toda
a rede neural baseando-se nos valores gerados das camadas mais finais até as camadas mais iniciais.
Em suma, <em>backpropagation</em> das redes neurais é equivalente ao algoritmo de gradiente descendente
dos problemas de regressão.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="algoritmo"><a class="header" href="#algoritmo">Algoritmo</a></h1>
<p>Agora, podemos verificar como funciona esse algoritmo na prática.</p>
<hr />
<p><strong>Algorithm 7</strong> <em>Backpropagation Algorithm</em></p>
<hr />
<p>1: <strong>procedure</strong> \( BACKPROPAGATION \) (Training set
\( [(x ^{(1)},y ^{(1)}), \dots , (x ^{(m)}, y^{(m)})] \))</p>
<p>2:   Inicializar \( \Delta _{ij} ^{(l)} = 0 \)</p>
<p>3:   <strong>for</strong> \( i=1 \) <strong>to</strong> \( m \) <strong>do</strong></p>
<p>4:    Inicializar \( a ^{(1)} = x ^{(i)} \)</p>
<p>5:    Realizar <em>forward propagation</em> para computar \( a ^{(l)} \)
  \( \rhd \ para \ l=2,3, \dots , L \)</p>
<p>6:    Usando \( y ^{(i)} \), computar \( \delta ^{(L)} = a ^{(L)} - y ^{(i)} \)</p>
<p>7:    Computar \( \delta ^{(L-1)}, \delta ^{(L-2)}, \dots , \delta ^{(2)} \)</p>
<p>8:   
\( \Delta _{ij} ^{(l)} := \Delta _{ij} ^{(l)} + a _j ^{(l)} \delta _i ^{(l+1)} \)</p>
<p>9:   <strong>end for</strong></p>
<p>10:  <strong>if</strong> \( j \neq 0 \) <strong>then</strong></p>
<p>11:  
\( D _{(ij)} ^{(l)} := \frac{1}{m} \Delta _{ij} ^{(l)} + \lambda \Theta _{ij} ^{(l)} \)
  \( \rhd \frac{\partial}{\partial \Theta _{ij} ^{(l)}} J(\Theta) = D _{(ij)} ^{(l)} \)</p>
<p>12:  <strong>else</strong></p>
<p>13:   \( D _{(ij)} ^{(l)} := \frac{1}{m} \Delta _{ij} ^{(l)} \)</p>
<p>14:  <strong>end if</strong></p>
<p>15:<strong>end procedure</strong></p>
<hr />
<p>Realizando uma análise do algoritmo, temos que na linha 2 inicializamos \( \Delta _{ij} ^{(l)} \)
com zeros, gerando uma matriz de zeros. No <em>loop for</em> inicializamos as variáveis \( a ^{(1)} \) com os
valores da <em>input layer</em> e, depois realizamos <em>forward propagation</em> para computar os valores de \( a ^{(l)} \).
Para cada valor de \( a ^{(l)} \) definido, computamos os valores de \( \delta ^{(l)} \) através
de <em>backpropagation</em>, na linha 6 e 7 sabendo que</p>
<hr />
<p>\[
\large{} \delta ^{(l)} = ((\Theta ^{(l)}) ^T \delta (l-1)) * a ^{(l)} * (1-a ^{(l)})
\]</p>
<hr />
<p>Computamos os valores de \( \delta ^{(l)} \) começando na camada \( L \) da rede neural até a camada 2. Utilizamos
os valores de delta para armazenar o erro presente em cada um dos vetores dos nodos de ativação
\( a ^{(l)} \).</p>
<p>Por fim, na linha 8, calculamos os valores de \( \Theta _{ij} ^{(l)} \) e determinamos o valor da derivada de
\( J(\Theta) \), armazenando em \( D _{(ij)} ^{(l)} \) nas linhas 11 e 13.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="otimizadores"><a class="header" href="#otimizadores">Otimizadores</a></h1>
<p>Em algumas implementações do gradiente descendente, podemos encontrar diferentes formas de
otimizações. A seguir estão listadas algumas formas de implementações.</p>
<ul>
<li>
<p>Gradient Descent;</p>
</li>
<li>
<p>Stochastic Gradient Descent;</p>
</li>
<li>
<p>Mini-Batch Gradient Descent;</p>
</li>
<li>
<p>Momentum;</p>
</li>
<li>
<p>Nesterov Accelerated Gradient;</p>
</li>
<li>
<p>AdaGrad;</p>
</li>
<li>
<p>Adam;</p>
</li>
</ul>
<p>Esses sistemas de otimização serão abordados e detalhados em seções seguintes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="verificação-do-gradiente"><a class="header" href="#verificação-do-gradiente">Verificação do gradiente</a></h1>
<p>Usamos o método de verificação do gradiente para assegurar que o algoritmo de <em>backpropagation</em>
está funcionando corretamente.</p>
<hr />
<p>\[
\large{}
\frac{\partial}{\partial \Theta} \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2 \epsilon}
\]</p>
<hr />
<p>Usamos a expressão acima para computar todos os valores de \( \Theta _j \) e para isso usamos valores pequenos
para epsilon, como por exemplo, \( \epsilon = 10 ^{-4} \).</p>
<p>Verificamos se os valores armazenados de \( \Theta _j \) retornados pela expressão se aproxima dos valores de
\( D \) retornados pelo algoritmo de <em>backpropagation</em>. Contudo, uma vez confirmado que o algoritmo
funciona corretamente, não precisamos verificar novamente, pois o algoritmo de verificação é muito
lento.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="inicialização-aleatória"><a class="header" href="#inicialização-aleatória">Inicialização aleatória</a></h1>
<p>Para o melhor funcionamento do algoritmo de <em>backpropagation</em> devemos inicializar os vetores
\( \Theta _{ij} ^{(l)} \) aleatoriamente de forma que \( \Theta _{ij} ^{(l)} \in [- \epsilon , \epsilon] \).</p>
<p>Nota-se que o valor de \( \epsilon \) utilizado é o mesmo valor que foi utilizado no método de verificação do
gradiente.</p>
<p>Com essa inicialização, garantimos que os valores de teta são simétricos e adaptados para o melhor
funcionamento do algoritmo de <em>backpropagation</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="organização-do-conhecimento"><a class="header" href="#organização-do-conhecimento">Organização do conhecimento</a></h1>
<p>Como foi visto até agora, nós temos três tipos de estruturas básicas em uma arquitetura de uma
rede neural:</p>
<ul>
<li>
<p>Número de unidades de entrada (<em>input units</em>): dimensão do vetor de entradas \( x \);</p>
</li>
<li>
<p>Número de unidades de saída (<em>output units</em>): dimensão do vetor de saída (classes);</p>
</li>
<li>
<p>Número de unidades intermediárias (<em>hidden units</em>) por camada: são definidas a partir da
complexidade do problema e, geralmente, quando temos mais de uma camada, cada uma delas
devem ter o mesmo número de unidades.</p>
</li>
</ul>
<p>A partir dessas definições mencionadas, podemos, novamente, estrutural a lógica que deve ser seguida
ao realizarmos o treino de uma rede neural. Abaixo, estão divididos em passos o algoritmo de treino
utilizando <em>backpropagation</em>.</p>
<ol>
<li>
<p>Aleatoriamente inicializar os pesos \( \Theta _{ij} ^{(l)} \) (Seção <a href="parte-3/2/./3-2-6.html">Inicialização aleatória</a>);</p>
</li>
<li>
<p>Implementar o método de <em>forward propagation</em> para computar os valores da função hipótese
\( h _{\Theta}(x ^{(i)}) \) para todos os valores de \( x ^{(i)} \) (Seção <a href="parte-3/2/./3-2.html">Redes Neurais: Aprendizado</a>);</p>
</li>
<li>
<p>Implementar a função custo (Seção <a href="parte-3/2/./3-2-2.html">Função Custo (Cost Function)</a>);</p>
</li>
<li>
<p>Implementar o método de <em>backpropagation</em> para computar os valores das derivadas
parciais \( \frac{\partial}{\partial \Theta} \) (Seção <a href="parte-3/2/./3-2-3.html">Backpropagation Algorithm</a>);</p>
</li>
<li>
<p>Utilizar o método de verificação do gradiente para confirmar que o método de <em>backpropagation</em>
está funcionando corretamente. Após executar uma vez, desabilitamos a verificação
(Seção <a href="parte-3/2/./3-2-5.html">Verificação do gradiente</a>);</p>
</li>
<li>
<p>Utilizar o algoritmo de gradiente descendente ou algum outro método de minimização mais
otimizado para minimizarmos a função custo utilizando os valores de teta (Seção <a href="parte-3/2/./3-2-4.html">Otimizadores</a>).</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aplicação-de-algoritmos-de-machine-learning"><a class="header" href="#aplicação-de-algoritmos-de-machine-learning">Aplicação de algoritmos de <em>machine learning</em></a></h1>
<ul>
<li>
<p><a href="parte-3/3/./3-3-1.html">Valoração de algoritmos de aprendizagem</a></p>
</li>
<li>
<p><a href="parte-3/3/./3-3-2.html">Curvas de aprendizado</a></p>
<ul>
<li>
<p><a href="parte-3/3/./3-3-2-1.html">High Bias</a></p>
</li>
<li>
<p><a href="parte-3/3/./3-3-2-2.html">High Variance</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-3/3/./3-3-3.html">Decisões a serem tomadas</a></p>
</li>
<li>
<p><a href="parte-3/3/./3-3-4.html">Diagnosticando Redes Neurais</a></p>
</li>
<li>
<p><a href="parte-3/3/./3-3-5.html">Support Vector Machines (SVMs)</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="valoração-de-algoritmos-de-aprendizagem"><a class="header" href="#valoração-de-algoritmos-de-aprendizagem">Valoração de algoritmos de aprendizagem</a></h1>
<p>Muitas vezes, podemos ter alguns problemas durante o teste das funções de treino. Para isso, é
possível realizar alguns levantamentos a respeito do problema realizando as seguintes atividades:</p>
<ul>
<li>
<p>Aumentar o número de exemplos de treino;</p>
</li>
<li>
<p>Diminuir o número de parâmetros;</p>
</li>
<li>
<p>Adicionar parâmetros;</p>
</li>
<li>
<p>Tentar parâmetros polinomiais;</p>
</li>
<li>
<p>Aumentar ou diminuir o valor de \( \lambda \).</p>
</li>
</ul>
<p>Existem outros métodos que podem ser utilizados para valorar a função hipótese, ou seja, verificar a
acurácia dos conjuntos de treino e teste. Assim, para cada um dos conjuntos teremos duas funções
custo; uma para o conjunto de treino: \( J _{treino}(\theta) \); e outra para o conjunto de teste
\( J _{test}(\theta) \).</p>
<p>Com isso, para regressão linear e para algoritmos de classificação, temos as seguintes equações para
computar o erro de cada um dos dois métodos.</p>
<ol>
<li>Para regressão linear:</li>
</ol>
<p>\[
\large{} J _{test}(\theta) = \frac{1}{2m _{test}} \sum _{i=1} ^{m _{test}}
(h _{\Theta}(x _{test} ^{(i)}) - y _{test} ^{(i)}) ^2
\]</p>
<ol start="2">
<li>Para classificação:</li>
</ol>
<p>\[
\large{} err(h _{\theta}(x), y) = \Bigg\{
_{0 \ \ caso \ contrário}
^{1 \ \ se \ h _{\theta}(x) \geq 0.5 \ e \ y=1 \ ou \ h _{\theta}(x) &lt; 0.5 \ e \ y=1}
\]</p>
<p>Calculamos a média do erro do conjunto de teste:</p>
<p>\[
\large{}
TesteErro = \frac{1}{m _{test}} \sum _{i=1} ^{m _{test}}
err(h _{\theta}(x _{test} ^{(i)}) - y _{test} ^{(i)})
\]</p>
<p>Cada uma dessas formas nos retorna a proporção que os nossos dados foram erroneamente
classificados.</p>
<p>Usualmente, dividimos o <em>dataset</em> em três diferentes conjuntos:</p>
<ul>
<li>
<p>Conjunto de treino (60%);</p>
</li>
<li>
<p>Conjunto de <em>cross-validation</em> (20%);</p>
</li>
<li>
<p>Conjunto de teste (20%).</p>
</li>
</ul>
<p>O conjunto de treino é utilizado efetivamente para treinar o nosso modelo de rede neural. O conjunto
de <em>cross-validation</em> é utilizado para testar o nosso modelo enquanto realizamos o treino, para que
possamos ajustar os hiperparâmetros e controlar os casos de <em>overfitting</em> e <em>underfitting</em> que podem,
eventualmente vir a ocorrer. O conjunto de teste, serve, essencialmente para testarmos o modelo e
verificarmos o quão bem ele está generalizando dados que não foram previamente vistos.</p>
<p>Podemos calcular os os erros para cada um desses três conjuntos, resultando em três diferentes
valores: \( J _{train}(\theta), J _{cv}(\theta), J _{test}(\theta) \). Com isso, usamos os valores
dos erros de \( J _{cv}(\theta) \) para ajustarmos o grau do polinômio a fim de que possamos atingir
o menor erro possível para \( J _{test}(\theta) \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="curvas-de-aprendizado"><a class="header" href="#curvas-de-aprendizado">Curvas de aprendizado</a></h1>
<p>Nesta seção iremos interpretar gráficos que representam curvas de aprendizado analisando os valores
de \( J _{train}(\theta), J _{cv}(\theta) \) e \( J _{test}(\theta) \) para concluir se o treino
está bem ajustado, com <em>overfitting</em> ou <em>underfitting</em>.</p>
<p>Em primeira análise, sabemos que para uma quantidade pequena de dados de entrada o valor do erro
esperado é próximo de zero, pois o modelo aprende cerca de 100% do conjunto de treino. Porém,
quando aumentamos o tamanho do conjunto de treino este erro tende a aumentar, muitas vezes,
tanto para o conjunto de treino, quanto para o conjunto de teste. Para isso, podemos analisar dois
tipos de situações esperadas: <em>high bias</em> e <em>high variance</em>.</p>
<p>Na representação a seguir que representa o erro da função \( J \) para casos de treino e teste de acordo
com o grau do polinômio \( d \). Percebe-se que quanto maior o grau do polinômio, menor é a taxa de
erro no conjunto de treino e maior é a taxa de erro no conjunto de teste, e quanto menor o grau do
polinômio maior é a taxa de erro do conjunto de treino e maior é a taxa de erro do conjunto de teste.
Assim, devemos ajustar o grau do polinômio de forma que ele seja grande suficiente para evitar os
casos de <em>underfitting</em> e <em>overfitting</em>.</p>
<p align="center">
  <img src="parte-3/3/./img/25.png">
</p>
<p align="center">
Figura 25: Representação das curvas \( J _{cv}(\theta) \) e \( J _{train}(\theta) \) relacionando o grau
do polinômio \( d \) com a taxa de erro de cada uma dessas funções.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="high-bias"><a class="header" href="#high-bias"><em>High Bias</em></a></h1>
<p>Em casos de <em>underfitting</em> teremos a seguinte situação apresentada na Figura 26</p>
<p align="center">
  <img src="parte-3/3/./img/26.png">
</p>
<p align="center">
Figura 26: Representação da curva de aprendizado para o caso de <i>high bias</i>
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="high-variance"><a class="header" href="#high-variance"><em>High Variance</em></a></h1>
<p>Em casos de <em>overfitting</em> teremos a seguinte situação apresentada na Figura 27</p>
<p align="center">
  <img src="parte-3/3/./img/27.png">
</p>
<p align="center">
Figura 27: Representação da curva de aprendizado para o caso de <i>high variance</i>
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decisões-a-serem-tomadas"><a class="header" href="#decisões-a-serem-tomadas">Decisões a serem tomadas</a></h1>
<p>Caso desejamos ajustar o nosso algoritmo de treino da melhor forma possível, podemos dividir a
tomada de decisão a respeito de que tipo de ajuste devemos realizar da seguinte forma:</p>
<ul>
<li>
<p>Aumentar o número de exemplos de treino: ajusta em casos de <em>high variance</em>;</p>
</li>
<li>
<p>Diminuir o número de parâmetros: ajusta em casos de <em>high variance</em>;</p>
</li>
<li>
<p>Adicionar parâmetros: ajusta em casos de <em>high bias</em>;</p>
</li>
<li>
<p>Tentar parâmetros polinomiais: ajusta em casos de <em>high bias</em>;</p>
</li>
<li>
<p>Diminuir o valor de \( \lambda \): ajusta em casos de <em>high bias</em>;</p>
</li>
<li>
<p>Aumentar o valor de \( \lambda \): ajusta em casos de <em>high variance</em>;</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diagnosticando-redes-neurais"><a class="header" href="#diagnosticando-redes-neurais">Diagnosticando Redes Neurais</a></h1>
<p>Como foi visto nas seções anteriores, os problemas de <em>underfitting</em> e <em>overfitting</em> podem acontecer
por diversos motivos e discutimos maneiras de podermos solucioná-los.</p>
<p>Em uma rede neural, esses problemas também podem ocorrer. O primeiro caso está relacionado à
possibilidade de <em>underfitting</em>, e geralmente isso ocorre devido ao fato da baixa quantidade de
parâmetros (computacionalmente mais barato). O segundo caso está relacionado com a possibilidade de
<em>overfitting</em>, e geralmente isso ocorre devido ao fato da grande quantidade de parâmetros (compu-
tacionalmente mais caro), porém, neste caso, o problema pode ser facilmente corrigido através de
regularização.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="support-vector-machines-svms"><a class="header" href="#support-vector-machines-svms"><em>Support Vector Machines</em> (SVMs)</a></h1>
<p><em>Support Vector Machines</em>, ou SVMs, podem ser usadas para tanto para problemas de regressão
quanto problemas de classificação. O objetivo principal de uma SVM é encontrar um hiperplano em
um espaço \( N \)-dimensional, sendo \( N \) o número de parâmetros, que classifica de forma distinta cada
um dos dados.</p>
<p>Na Figura 28 a seguir, podemos perceber a funcionalidade de uma SVM na busca de um hiperplano
ótimo (imagem à direita).</p>
<p align="center">
  <img src="parte-3/3/./img/28.png">
</p>
<p align="center">
Figura 28: Exemplificação da utilização de uma SVM para encontrar o hiperplano ótimo que divide duas classes
distintas de dados. Na imagem à esquerda temos todos os hiperplanos possíveis que distinguem as duas classes de
dados e na imagem à direita temos o hiperplano ótimo que as distinguem.
</p>
<p>Nosso objetivo é encontrar um plano que possui a margem máxima, ou seja, que possui a máxima
distância entre os pontos de ambas as classes.</p>
<p>Hiperplanos são limites de decisão que ajudam a classificar as classes de dados. Além disso, a
dimensão de um hiperplano depende do número de parâmetros.</p>
<p>Com isso, temos a definição de <em>support vectors</em>. <em>Support vectors</em> são pontos que estão mais próximos 
ao hiperplano e influenciam a posição e a orientação desse hiperplano. Usando-os, podemos
maximizar a margem do classificador.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="naive-bayes"><a class="header" href="#naive-bayes"><em>Naive</em> Bayes</a></h1>
<p><em>Naive</em> Bayes é um algoritmo de classificação que utiliza aprendizado supervisionado. O algoritmo
é chamado de &quot;naive&quot; (do inglês, ingênuo) porque ele realiza uma suposição ingênua de que cada
dado é independente dos outros, o que não é verdade na vida real. Portando, o algoritmo através
do teorema de Bayes, realiza decisões que classificam os dados a partir de valores de probabilidade
gerados a partir da suposição ingênua de independência dos dados.</p>
<p>O teorema de Bayes nos ajuda a encontrar o valor de probabilidade da hipótese dado um conhecimento
prévio que pode ter interferência. Em outras palavras:</p>
<hr />
<p>\[
\large{} P(H|e) = \frac{P(e|H)P(H)}{P(e)}
\]</p>
<hr />
<p>onde:</p>
<ul>
<li>
<p>\( P(H|e) \) (<em>posterior</em>): o quão provável é a nossa hipótese dada a evidência observada.</p>
</li>
<li>
<p>\( P(e|H) \) (<em>likelihood</em>): o quão provável é a evidência dada que a hipótese é verdadeira.</p>
</li>
<li>
<p>\( P(H) \) (<em>prior</em>): o quão provável era a hipótese antes de observar a evidência.</p>
</li>
<li>
<p>\( P(e) \) (<em>marginal</em>): o quão provável é a nova evidência sob todas as possíveis hipóteses.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-iv"><a class="header" href="#parte-iv">Parte IV</a></h2>
<h1 id="unsupervised-learning-1"><a class="header" href="#unsupervised-learning-1"><em>Unsupervised Learning</em></a></h1>
<ul>
<li>
<p><a href="./parte-4/1/4-1.html">Clustering</a></p>
<ul>
<li>
<p><a href="./parte-4/1/4-1-1.html">K-Means Algorithm</a></p>
</li>
<li>
<p><a href="./parte-4/1/4-1-2.html">Otimização</a></p>
</li>
<li>
<p><a href="./parte-4/1/4-1-3.html">Inicialização</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-4/2/4-2.html">K-Nearest Neighbors</a></p>
<ul>
<li>
<p><a href="./parte-4/2/4-2-1.html">Algoritmo</a></p>
</li>
<li>
<p><a href="./parte-4/2/4-2-2.html">Escolhendo o valor correto para K</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-4/3/4-3.html">Redução de dimensionalidade</a></p>
<ul>
<li>
<p><a href="./parte-4/3/4-3-1.html">Compressão de dados</a></p>
</li>
<li>
<p><a href="./parte-4/3/4-3-2.html">Visualização</a></p>
</li>
<li>
<p><a href="./parte-4/3/4-3-3.html">Ánalise do componente principal (PCA)</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-4/4/4-4.html">Aprendizado por reforço (Reinforcement learning)</a></p>
<ul>
<li>
<p><a href="./parte-4/4/4-4-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="./parte-4/4/4-4-2.html">Exploration e Exploitation</a></p>
</li>
<li>
<p><a href="./parte-4/4/4-4-3.html">Markov Process</a></p>
<ul>
<li>
<p><a href="./parte-4/4/4-4-3-1.html">Propriedade de Markov</a></p>
</li>
<li>
<p><a href="./parte-4/4/4-4-3-2.html">Cadeia de Markov</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-4/4/4-4-4.html">Markov Decision Process (MDPs)</a></p>
<ul>
<li><a href="./parte-4/4/4-4-4-1.html">Busca pela política ótima com MDP</a></li>
</ul>
</li>
<li>
<p><a href="./parte-4/4/4-4-5.html">Monte-Carlo e Temporal-Difference Learning</a></p>
<ul>
<li>
<p><a href="./parte-4/4/4-4-5-1.html">Valoração de Monte-Carlo</a></p>
</li>
<li>
<p><a href="./parte-4/4/4-4-5-2.html">TD Learning</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-4/4/4-4-6.html">Q-Learning</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="clustering"><a class="header" href="#clustering"><em>Clustering</em></a></h1>
<p>Clustering é um método de aprendizado não supervisionado. A principal diferença entre <em>supervised learning</em> e
<em>unsupervised learning</em> é que no método não supervisionado não passamos dados previamente classificado,
em outras palavras, uma entrada para um algoritmo sem supervisão seria apenas
o conjunto de treino \( x _1, x _2, \dots , x _n \).</p>
<p>As principais aplicações de <em>clustering</em> são:</p>
<ul>
<li>
<p>Segmentação de mercado;</p>
</li>
<li>
<p>Análise de redes sociais;</p>
</li>
<li>
<p>Organização de <em>clusters</em> de computadores (<em>datacenters</em>);</p>
</li>
<li>
<p>Análise de dados astronômicos.</p>
</li>
</ul>
<p>O primeiro algoritmo de aprendizado não supervisionado que iremos discutir é chamado de
<em>K-Means Algorithm</em> que será apresentado na seção seguinte.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="k-means-algorithm"><a class="header" href="#k-means-algorithm"><em>K-Means Algorithm</em></a></h1>
<p><em>K-Means Algorithm</em> é um dos algoritmos mais populares e são amplamente utilizados a fim de,
automaticamente, agrupar dados em subgrupos denominados <em>clusters</em>.</p>
<p align="center">
  <img src="parte-4/1/./img/29.png">
</p>
<p align="center">
Figura 29: Representação de <i>clustering</i> através do algoritmo de <i>K-Means</i>. Percebe-se que, com a aplicação do
algoritmos os dados passam a formar conjuntos diferentes, na figura, representados pelas cores azul, verde e preto.
</p>
<p>Antes de discutir o algoritmo, podemos discutir o funcionamento desse método de <em>machine learning</em>
para os três clusters da imagem acima.</p>
<ol>
<li>
<p>Randomicamente inicializar três pontos no conjunto de dados. Esses pontos serão chamados
de <em>cluster centroids</em>;</p>
</li>
<li>
<p>Atribuição do <em>cluster</em>: atribua todos os exemplos em um dos três grupos baseado em qual
<em>centroid</em> os exemplos estão mais próximos;</p>
</li>
<li>
<p>Mover os <em>centroids</em>: computar as médias de todos os pontos dentro de cada um dos três
<em>centroids</em>, e mover os <em>centroids</em> para os pontos que representam as médias;</p>
</li>
<li>
<p>Executar os passos (2) e (3) até convergir.</p>
</li>
</ol>
<p>Para o algoritmo, teremos as seguintes variáveis principais:</p>
<ul>
<li>
<p>\( K \): número de <em>clusters</em>;</p>
</li>
<li>
<p>\( x ^{(1)}, x ^{(2)}, \dots , x ^{(n)} \): conjunto de treino, onde \( x ^{(i)} \in \mathbb{R} ^n \)</p>
</li>
</ul>
<hr />
<p><strong>Algorithm 8</strong> Algoritmo <em>K-Means</em></p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   Randomicamente inicializar \( K \) <em>cluster centroids</em> \( \mu _1, \mu _2, \dots , \mu _K \in \mathbb{R} ^n \)</p>
<p>3:   <strong>repeat</strong></p>
<p>4:    <strong>for</strong> \( i=1 \) <strong>to</strong> \( m \) <strong>do</strong></p>
<p>5:    
\( c ^{(i)} := \) índice (de 1 até \( K \)) do <em>cluster centroid</em> mais perto de \( x ^{(i)} \)</p>
<p>6:    <strong>end for</strong></p>
<p>7:    <strong>for</strong> \( k=1 \) <strong>to</strong> \( K \) <strong>do</strong></p>
<p>8:    
\( \mu _k := \) média dos pontos atribuídos ao cluster \( k \)</p>
<p>9:    <strong>end for</strong></p>
<p>10: <strong>until</strong> \( convergir \)</p>
<p>11: <strong>end procedure</strong></p>
<hr />
<p>No algoritmo, percebemos que existem dois <em>loops</em>. O primeiro, realiza a etapa (2) descrita acima da
seguinte forma:</p>
<p>\[
\large{} c ^{(i)} = argmin _k || x ^{(i)} - \mu _k || ^2 == || (x _1 ^i - \mu _{i(k)}) ^2 +
(x _2 ^i - \mu _{2(k)}) ^2 + \dots ||
\]</p>
<p>No segundo <em>loop</em> realiza a etapa (3) e pode ser descrita da seguinte forma:</p>
<p>\[
\large{} \mu _k = \frac{1}{n} [x ^{(k _1)} + x ^{(k _2)} + \dots + x ^{(k _n)}]
\]</p>
<p>Onde cada valor de \( x ^{(k _1)},x ^{(k _2)}, \dots ,x ^{(k _n)} \) são os exemplos de treino atribuidos
por \( \mu _k \)</p>
<p>Depois de um número de iterações, o algoritmo irá convergir e as posições dos <em>centroids</em> não serão
mais alteradas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="otimização"><a class="header" href="#otimização">Otimização</a></h1>
<p>Através dos parâmetros apresentados na seção anterior, podemos definir a função custo.</p>
<p>\[
\large{} J(c ^{(i)}, \dots , c ^{(m)}, \mu _1, \dots , \mu _k) = \frac{1}{m}
\sum _{i=1} ^m || x ^{(i)} - \mu _{c ^{(i)}} || ^2
\]</p>
<p>Nosso objetivo de otimização é minimizar todos os parâmetros da função custo descrita acima, em
outras palavras, desejamos</p>
<p>\[
\large{} min _{c, \mu} J(c, \mu)
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="inicialização"><a class="header" href="#inicialização">Inicialização</a></h1>
<p>Um método recomendado para o algoritmo \( K-Means \) é inicializar aleatoriamente os <em>cluster centroids</em>.
Para isso, devemos considerar os seguintes requisitos:</p>
<ul>
<li>
<p>\( K &lt; m \): O número de <em>clusters</em> deve ser menor que o número de exemplos de treino;</p>
</li>
<li>
<p>Aleatoriamente escolher \( K \) exemplos de treinos;</p>
</li>
<li>
<p>Definir \( \mu _1, \dots , \mu _K \) serem iguais aos \( K \) exemplos.</p>
</li>
</ul>
<p>Para escolher o número de <em>clusters</em>, usa-se um método chamado <em>elbow method</em>, o qual se analisa a
curva da função custo \( J \) e o número de <em>clusters</em> \( K \), representado na Figura 30. A
função custo deve decrescer de acordo com o aumento do número de <em>clusters</em> até tender a zero. Escolhemos
um valor para \( K \) no ponto em que a função custo começa a se estabilizar.</p>
<p align="center">
  <img src="parte-4/1/./img/30.png">
</p>
<p align="center">
Figura 30: Representação do método de escolha do valor ótimo de \( K \) para o algoritmo KNN.
</p>
<p>Uma outra forma de escolher o número de <em>clusters</em> é de acordo com o objetivo que desejamos atingir
com o uso deles.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="k-nearest-neighbors"><a class="header" href="#k-nearest-neighbors"><em>K-Nearest Neighbors</em></a></h1>
<p><em>K-Nearest Neighbors</em> (KNN) é um classificador que se baseia na similaridade dos dados próximos.
Em outras palavras, classifica os dados em grupos de acordo com a similaridade entre eles e o quão
próximo estão entre eles.</p>
<p>Na Figura 31 abaixo, podemos perceber como é realizada essa classificação. Cada uma das cores da
imagem representa um conjunto de dados que possui similaridade entre si.</p>
<p align="center">
  <img src="parte-4/2/./img/31.png">
</p>
<p align="center">
Figura 31: Representação das similaridades entre os conjuntos
</p>
<p>A ideia de similaridade menciona é baseada na distância entre os pontos no gráfico.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="algoritmo-1"><a class="header" href="#algoritmo-1">Algoritmo</a></h1>
<p>A seguir, será apresentado o algoritmo básico para a classificação de dados utilizando o método
KNN.</p>
<ol>
<li>
<p>Carregar os dados</p>
</li>
<li>
<p>Inicializar \( K \) como o número de &quot;vizinhos&quot; escolhidos.</p>
</li>
<li>
<p>Para cada um dos pontos: calcular a distância entre o atual ponto em relação aos outros e
adicionar a distância e o índice do ponto a uma coleção ordenada</p>
</li>
<li>
<p>Ordenar a coleção ordenada de distâncias e índices de forma crescente das distâncias</p>
</li>
<li>
<p>Retornar os rótulos dos \( K \) primeiros valores da sequência</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="escolhendo-o-valor-correto-para--k-"><a class="header" href="#escolhendo-o-valor-correto-para--k-">Escolhendo o valor correto para \( K \)</a></h1>
<p>Para escolher o valor de \( K \) rodamos o algoritmo diversas vezes de forma a encontrar o valor de
\( K \) que reduza o número de erros nos exemplos de teste.</p>
<p>É importante mencionar, que quanto menor o valor de \( K \), há maior possibilidade de <em>overfitting</em> e
quanto maior esse valor, maior a possibilidade de <em>underfitting</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redução-de-dimensionalidade"><a class="header" href="#redução-de-dimensionalidade">Redução de dimensionalidade</a></h1>
<ul>
<li>
<p><a href="parte-4/3/./4-3-1.html">Compressão de dados</a></p>
</li>
<li>
<p><a href="parte-4/3/./4-3-2.html">Visualização</a></p>
</li>
<li>
<p><a href="parte-4/3/./4-3-3.html">Ánalise do componente principal (PCA)</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compressão-de-dados"><a class="header" href="#compressão-de-dados">Compressão de dados</a></h1>
<p>Em um conjunto de dados, muitas vezes podemos ter diversos dados redundantes e, para isso,
devemos reduzir a quantidade desses dados a fim de facilitar a compreensão do conjunto o qual
estamos trabalhando.</p>
<p>Assim, nós selecionamos dados que estão correlacionados e os colocamos em uma única linha que
possa descrever o comportamento de ambos. Com redução de dimensionalidade, podemos reduzir o total
de dados guardados aumentando a memória disponível e, muitas vezes, acelerando o processamento do
algoritmo de aprendizagem.</p>
<p align="center">
  <img src="parte-4/3/./img/32.png">
</p>
<p align="center">
Figura 32: Representação de uma compressão de dados. Na figura, percebe-se que houve a redução na
dimensionalidade dos dados. Os dados previamente em três dimensões foram convertidos para duas dimensões. Essa
conversão se dá através de um algoritmo que será apresentado nas seções seguintes chamado PCA.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visualização"><a class="header" href="#visualização">Visualização</a></h1>
<p>Quando realizamos redução de dimensionalidade dos dados, desejamos visualizá-los em duas ou,
no máximo, três dimensões. Para isso, precisamos encontrar novos valores \( z _1, z _2 \) (ou \( z _3 \))
para que possamos resumir esses dados efetivamente em menos dimensões.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="análise-do-componente-principal-pca"><a class="header" href="#análise-do-componente-principal-pca">Análise do componente principal (PCA)</a></h1>
<p>O algoritmo de análise do componente principal (do inglês, <em>Principal Component Analysis Algorithm</em>
(PCA)) é um algoritmo de redução de dimensionalidade de dados. Como está representado na
Figura 32, o algoritmo busca comprimir os dados relacionando cada um dos dados em uma só
semelhança, em outras palavras, busca reduzir a média de todas as distâncias de cada um dos
valores em relação a linha projetada. Essa redução é chamada de erro de projeção.</p>
<p align="center">
  <img src="parte-4/3/./img/33.png">
</p>
<p align="center">
Figura 33: Representação da aplicação do algoritmo de PCA. Percebe-se que o algoritmo busca encontrar uma
relação entre os pontos e fim de projetar os dados sobre essa relação. Na figura, os pontos em azuis são relacionados
e projetados sobre a reta em verde, gerando os pontos em laranja sobre a reta.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aprendizado-por-reforço-reinforcement-learning"><a class="header" href="#aprendizado-por-reforço-reinforcement-learning">Aprendizado por reforço (<em>Reinforcement learning</em>)</a></h1>
<ul>
<li>
<p><a href="parte-4/4/./4-4-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="parte-4/4/./4-4-2.html">Exploration e Exploitation</a></p>
</li>
<li>
<p><a href="parte-4/4/./4-4-3.html">Markov Process</a></p>
<ul>
<li>
<p><a href="parte-4/4/./4-4-3-1.html">Propriedade de Markov</a></p>
</li>
<li>
<p><a href="parte-4/4/./4-4-3-2.html">Cadeia de Markov</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-4/4/./4-4-4.html">Markov Decision Process (MDPs)</a></p>
<ul>
<li><a href="parte-4/4/./4-4-4-1.html">Busca pela política ótima com MDP</a></li>
</ul>
</li>
<li>
<p><a href="parte-4/4/./4-4-5.html">Monte-Carlo e Temporal-Difference Learning</a></p>
<ul>
<li>
<p><a href="parte-4/4/./4-4-5-1.html">Valoração de Monte-Carlo</a></p>
</li>
<li>
<p><a href="parte-4/4/./4-4-5-2.html">TD Learning</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-4/4/./4-4-6.html">Q-Learning</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visão-geral-1"><a class="header" href="#visão-geral-1">Visão geral</a></h1>
<p>Nesta seção será apresentado mais um método de aprendizado de máquina chamado aprendizado por
reforço (do inglês, <em>Reinforcement Learning</em> ou RL). RL ensina um agente a como escolher uma ação
que faça sentido de acordo com o ambiente que ele esteja inserido (e.g. escolher uma movimentação
de peça adequada em um jogo de tabuleiro) a fim de maximizar a recompensa que esse agente recebe
ao longo do tempo.</p>
<p>Para isso, precisamos definir alguns elementos essenciais para a implementação de um algoritmo de
RL (representado na Figura 34).</p>
<ul>
<li>
<p>Agente: o que o programa está exatamente treinando a fim de realizar alguma tarefa específica;</p>
</li>
<li>
<p>Ambiente: o mundo, real ou virtual, no qual o agente realiza as suas ações;</p>
</li>
<li>
<p>Ação: um movimento realizado pelo agente. Essa movimentação muda o estado do ambiente;</p>
</li>
<li>
<p>Recompensa: a valoração de uma ação realizada pelo agente. Essa valoração pode ser positiva
ou negativa.</p>
</li>
</ul>
<p align="center">
  <img src="parte-4/4/./img/34.png">
</p>
<p align="center">
Figura 34: Representação de um esquema básico de um algoritmo de RL. Percebe-se que o agente através de ações
realizadas sobre um ambiente determinado atualiza o estado do ambiente e recebe recompensas (positivas ou
negativas) de acordo com a ação realizada.
</p>
<p>Com essas definições, percebe-se que o processo de aprendizado do agente se dá por meio de tomada
de decisões baseadas no ambiente e nas recompensas. Na próxima seção será apresentada as ideias
fundamentais de exploração do ambiente em RL.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exploration-e-exploitation"><a class="header" href="#exploration-e-exploitation"><em>Exploration</em> e <em>Exploitation</em></a></h1>
<p>Quando o agente realiza explorações no ambiente a fim de conhecer o território no qual ele está
inserido, ele realiza dois tipos de ações que são determinantes para o seu aprendizado: <em>exploration</em> e
<em>exploitation</em>.</p>
<p>Apesar de parecerem redundantes, os dois termos possuem uma grande diferença. Quando o agente
toma decisões baseadas em <em>exploration</em>, o agente toma decisões puramente aleatórias, com o intuito
de encontrar melhores ações para o estado no qual ele se encontra. Isso é importante para possibilitar
ao agente descoberta de novas estratégias, podendo, assim, expandir o seu leque de opções de ações.
Em <em>exploitation</em> o agente toma decisões baseadas em valorações que foram previamente estabelecidas
de estados previamente alcançados. Isso é importante para maximizar a tomada de decisão do agente,
pois ele saberá se determinado estado é ”bom” ou ”ruim” ao longo da compreensão do ambiente.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="markov-process"><a class="header" href="#markov-process"><em>Markov Process</em></a></h1>
<p>Nesta seção iremos discutir o princípio de formação dos métodos que envolvem
<em>Reinforcement Learning: Markov process</em>. Contudo, antes disso, é necessário introduzir alguns
conceitos chaves desse processo: a Propriedade de Markov e a Cadeia de Markov.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="propriedade-de-markov"><a class="header" href="#propriedade-de-markov">Propriedade de Markov</a></h1>
<p>Na teoria das probabilidades e estatística, o termo &quot;propriedade de Markov&quot; se refere à propriedade
sem memória de um processo estocástico, em outras palavras, em um processo aleatoriamente determinado.</p>
<p>Para o melhor entendimento da Propriedade de Markov, podemos descrevê-la da seguinte forma:</p>
<p>\[
\large{} P(X(t+1)=j|X(0)=i _0,X(1)=i _1, \dots ,X(t)=i) = P(X(t+1)=j|X(t)=i)
\]</p>
<p>A equação acima representa uma situação de um estado X no tempo \( t+1 \) que depende apenas do
estado precedente, o estado X no tempo \( t \). Em outras palavras, a propriedade de Markov nos diz que
o próximo estado de um processo de Markov independe dos estados precedentes do anterior, porque
o estado imediatamente anterior irá conter todas as informações dos estados precedentes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cadeia-de-markov"><a class="header" href="#cadeia-de-markov">Cadeia de Markov</a></h1>
<p>A Cadeia de Markov é um modelo estocástico que descreve uma sequência de possíveis eventos os
quais são dependentes de uma possibilidade de ocorrência. E essa possibilidade só depende do evento
prévio a esta ação.</p>
<p>Quando usamos a Propriedade de Markov em um processo aleatório, chamamos esse uso de Cadeia
de Markov, podendo ser definida da seguinte forma:</p>
<p>A Cadeia de Markov é uma tupla \( (S,P) \) onde:</p>
<ul>
<li>
<p>\( S \) é um conjunto de estados;</p>
</li>
<li>
<p>\( P(s,s') \) é uma transição de estado cujo peso é a probabilidade. A probabilidade de transição
do estado \( s \) no tempo \( t \) para o estado \( s' \) no tempo \( t+1 \).</p>
</li>
</ul>
<p align="center">
  <img src="parte-4/4/./img/35.png">
</p>
<p align="center">
Figura 35: Representação de uma Cadeia de Markov com os estados \(e, a, t\) onde cada aresta representa uma
transação de estado e seus pesos representam a probabilidade de transição.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="markov-decision-processes-mdps"><a class="header" href="#markov-decision-processes-mdps"><em>Markov Decision Processes</em> (MDPs)</a></h1>
<p>MDP é um dos tópicos mais importantes para o entendimento de RL. MDP é um método que resolve
a maioria dos problemas de RL com tomadas de decisões discretas. Com esse método, um agente
atinge uma política ótima para receber o maior número de recompensas ao longo do processo de
exploração do ambiente.</p>
<p>Como foi apresentado na seção anterior na qual discutimos as definições de Propriedade e Cadeia
de Markov, uma Cadeia de Markov trabalha com as transições entre os estados e as probabilidades
de transição relacionadas aos mesmos. MDP difere da Cadeia de Markov pois, agora, os estados
não dependem apenas do estado imediatamente posterior, mas sim, de todas as ações que foram
executadas a partir do estado atual.</p>
<p>Com isso, o objetivo principal do MDP é treinar um agente a fim de encontrar a melhor política
possível acumulando o maior número de recompensas a partir de tomadas de decisões em um ou
mais estados. Podemos definir, formalmente MDP da seguinte forma:</p>
<p>MDP é uma 5-upla \( (S,A,P,R, \gamma) \), onde:</p>
<ul>
<li>
<p>\( S: \) conjunto de estados;</p>
</li>
<li>
<p>\( A: \) conjunto de ações;</p>
</li>
<li>
<p>\( P(s, a, s'): \) probabilidade de uma ação \( a \) no estado \( s \) levar ao estado
\( s' \) no tempo \( t+1 \);</p>
</li>
<li>
<p>\( R(s, a, s'): \) recompensa recebida imediatamente após a transição do estado \( s \) para o
\( s' \) a partir de uma ação \( a \);</p>
</li>
<li>
<p>\( \gamma \): fator de desconto o qual é usado para gerar uma recompensa relativa. Esse valor está entre
0 e 1 e quantifica a diferença de importância de recompensas imediatas e recompensas futuras.</p>
</li>
</ul>
<p>Em outras palavras, MDP pode ser escrito como</p>
<p>\[
\large{} \sum _{t=0} ^{t= \infty} \gamma ^t r(x(t), a(t))
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="busca-pela-política-ótima-com-mdp"><a class="header" href="#busca-pela-política-ótima-com-mdp">Busca pela política ótima com MDP</a></h1>
<p>Com MDP podemos fazer com que o nosso agente selecione a decisão ótima para determinado estado
de ambiente. Iremos maximizar a recompensa do agente ao longo do tempo a fim de fazer com que
ele atinja a política ótima, i.e. determinaremos qual é a melhor ação a ser tomada em cada estado.</p>
<p>Para determinar a melhor ação a ser tomada, iremos usar a equação de Bellman
(<em>Bellman Optimality Equation</em> <a href="parte-4/4/../../referencias.html">[4]</a>) que nos possibilita estimar o valor ótimo
para cada estado. A equação estima o valor de um estado computando as recompensas esperadas que cada estado pode gerar.</p>
<p>Abaixo está definida a equação de Bellman recursiva</p>
<p>\[
\large{} V ^*(s) = max _a \sum _{s'} P(s,a,s')[R(s,a,s') + \gamma V(s')]
\]</p>
<p>Onde:</p>
<ul>
<li>
<p>\( P(s,a,s') \) é a probabilidade da transição do estado \( s \) para o \( s' \) a partir da escolha
da ação \( a \);</p>
</li>
<li>
<p>\( R(s,a,s') \) é a recompensa imediata do estado \( s \) para o \( s' \) quando o agente escolhe a
ação \( a \);</p>
</li>
<li>
<p>\( \gamma \) é o fator de desconto (recursivo).</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monte-carlo-e-temporal-difference-learning"><a class="header" href="#monte-carlo-e-temporal-difference-learning">Monte-Carlo e <em>Temporal-Difference Learning</em></a></h1>
<p>Nesta seção serão apresentados dois métodos chaves de aprendizado de máquina através de RL:
aprendizado de <em>Monte-Carlo</em> (MC) e <em>Temporal-Difference Learning</em> (TD).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="valoração-de-monte-carlo"><a class="header" href="#valoração-de-monte-carlo">Valoração de Monte-Carlo</a></h1>
<p>O método de Monte-Carlo faz com que o agente aprenda interagindo com o ambiente e coletando
diversas amostras. Essa coleção de amostras estão relacionadas as distribuições de probabilidades
mencionadas \( R(s,a,s') \) e \( R(s,a,s') \).</p>
<p>Entretanto, Valoração de MC é uma forma de aprendizado baseado em testes, em outras palavras,
um MDP sem a tupla \( P   \) pode aprender por tentativa e erro, por meio de muitas repetições.</p>
<p>Nesse cenário, cada &quot;tentativa&quot; é chamado de episódio, e ele termina quando o estado final do MDP
é atingido. No final, todos os valores de recompensa são atualizados na recompensa \( G _t \), final de cada
estado. Abaixo está representada a equação de Monte-Carlo que atualiza o valor de cada estado.</p>
<p>\[
\large{} V(s _t) \leftarrow V(S _t) + \alpha [G _t - V(S _t)]
\]</p>
<p>Onde:</p>
<ul>
<li>
<p>\( V(S _t) \) é o valor do estado que desejamos estimar. Esse valor pode ser inicializado
aleatoriamente ou baseado em alguma estratégia;</p>
</li>
<li>
<p>\( G _t \) pode ser calculado da seguinte forma, onde T é o tempo de terminação:</p>
</li>
</ul>
<p>\[
\large{} G _t = R _{t+1} + \gamma R _{t+2} + \gamma ^2 R _{t+3} + \dots + \gamma ^{T-1} R _T
\]</p>
<ul>
<li>\( \alpha \) é um parâmetro que influencia a convergência.</li>
</ul>
<p>Sendo assim, temos diversas formas de atualizarmos os valores dos estados do ambiente. Essas
atualizações são significantes para determinar a política do agente e otimizar a escolha da ação.
Essas formas estão relacionadas ao momento de visita de cada estado, ou seja, como devemos
atualizar o valor de um estado na primeira visita e em cada vez que atingimos o mesmo estado.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="td-learning"><a class="header" href="#td-learning">TD <em>Learning</em></a></h1>
<p>Como vimos na seção anterior, o método de Monte-Carlo exige que nós esperamos até o fim do
episódio para determinar o valor do estado \( V(S _t) \). No método de <em>Temporal-Difference</em>, esperamos
até o próximo passo a ser tomado. Em outras palavras, no tempo \( t+1 \), o método TD utilizado da
recompensa observada \( R _{t+1} \) e, imediatamente, gera um valor, chamado <em>TD target</em>
\( R(t+1)+V(S _{t+1}) \), atualizando o valor do estado \( V(S _t) \) através do erro,
chamado <em>TD error</em> \( R(t+1)+V(S _{t+1})-V(S _t) \).</p>
<p>Com isso, podemos definir a equação \( TD(\lambda) \) na qual substituímos o valor de <em>TD target</em> por
\( G _t \) da seguinte forma:</p>
<p>\[
\large{} V(S _t) \leftarrow V(S _t) + \alpha [G _t ^{(\lambda)} - V(S _t)]
\]</p>
<p>onde:</p>
<p>\[
\large{} G _t ^{(\lambda)} = (1- \lambda) \sum _{n=1} ^{\infty} \lambda ^{n-1} G _t ^{n}
\]</p>
<p>Assim, com essas duas equações, Monte-Carlo(MC) e <em>Temporal-Difference</em>(TD) podemos definir um
método de aprendizado por reforço que é uma combinação das duas, chamado de \( Q-Learning \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="q-learning"><a class="header" href="#q-learning"><em>Q-Learning</em></a></h1>
<p><em>Q-Learning</em> é uma combinação entre os métodos de MC e TD vistos na seção anteriores. Nesse
método, a cada passo, tomamos uma decisão gananciosa que tem como objetivo maximizar o valor
da variável \( Q(S _{t+1},a) \), como está representado abaixo:</p>
<hr />
<p>\[
\large{} Q(S _t,a) \leftarrow Q(S _t,a) + \alpha [R _{t+1} + \gamma max _a Q(S _{t+1},a) - Q(S _t,a)]
\]</p>
<hr />
<p>Para a implementação desse método, precisamos de dois fatores principais: a atualização do valor
de \( Q \) - chamado de <em>Q-value</em> e um lugar para salvar esses valores, chamado de <em>Q-table</em>.</p>
<p>Em <em>Q-learning</em>, a cada passo tomamos a melhor decisão possível para o agente, ou seja, a decisão
que terá o melhor impacto na atualização de <em>Q-value</em>. Esse tipo de tomada de decisão se chama
\( \epsilon \)<em>-greedy-policy</em>, onde \( 0 &lt; \epsilon &lt; 1 \) é o grau de ganância do nosso agente,
em outras palavras, quanto mais alto o seu valor, maior a quantidade de ações realizadas aleatoriamente.</p>
<p>No início da exploração, o agente toma decisões puramente aleatórias a fim de conhecer o ambiente
no qual ele está inserido. E a cada tomada de decisão, atualiza-se o valor de cada estado <em>Q-value</em>
na tabela <em>Q-table</em>. Portanto, é comum começar a exploração com um valor alto de \( \epsilon \), como, por
exemplo, igual a 1, a fim de fazer com que o agente tome decisões 100% aleatórias.</p>
<p>Ao longo da exploração, o agente começa a conhecer melhor o ambiente, então podemos diminuir o
valor de \( \epsilon \) gradativamente ao longo do treino.</p>
<p>Contudo, a utilização desse tipo de implementação não é escalável, ou seja, para modelos extremamente
complexos, é extremamente ineficiente a valoração dos estados e a busca pela política ótima.
Para resolver esse tipo de problema, métodos derivados foram desenvolvidos utilizando redes neurais
e o principal deles se chama <em>Deep Q-Learning</em> que será discutido na Seção <a href="parte-4/4/../../parte-6/5/6-5.html"><em>Deep Q-Learning</em></a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-v"><a class="header" href="#parte-v">Parte V</a></h2>
<h1 id="otimizações-no-aprendizado"><a class="header" href="#otimizações-no-aprendizado">Otimizações no Aprendizado</a></h1>
<ul>
<li>
<p><a href="./parte-5/1/5-1.html">Detecção de anomalias</a></p>
<ul>
<li>
<p><a href="./parte-5/1/5-1-1.html">Motivação</a></p>
</li>
<li>
<p><a href="./parte-5/1/5-1-2.html">Distribuição Gaussiana</a></p>
</li>
<li>
<p><a href="./parte-5/1/5-1-3.html">Algoritmo</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-5/2/5-2.html">Aprendizado em larga escala</a></p>
<ul>
<li>
<p><a href="./parte-5/2/5-2-1.html">Batch Normalization</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-2.html">Stochastic Gradient Descent (SGD)</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-3.html">Mini-batch Gradient Descent</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-4.html">SGD + Momentum</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-5.html">Nesterov Accelerated Gradient (NAG)</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-6.html">AdaGrad</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-7.html">Adam</a></p>
</li>
<li>
<p><a href="./parte-5/2/5-2-8.html">Mapreduce e Paralelismo de Dados</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="detecção-de-anomalias"><a class="header" href="#detecção-de-anomalias">Detecção de anomalias</a></h1>
<ul>
<li>
<p><a href="parte-5/1/./5-1.html">Detecção de anomalias</a></p>
<ul>
<li>
<p><a href="parte-5/1/./5-1-1.html">Motivação</a></p>
</li>
<li>
<p><a href="parte-5/1/./5-1-2.html">Distribuição Gaussiana</a></p>
</li>
<li>
<p><a href="parte-5/1/./5-1-3.html">Algoritmo</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="motivação"><a class="header" href="#motivação">Motivação</a></h1>
<p>Suponha que tenhamos um conjunto de dados de treino \( x ^{(1)}, x ^{(2)}, \dots , x ^{(m)} \). E dado um novo exemplo
de treino \( x _{test} \) queremos saber se esse dado pode ser considerado anormal ou anômalo.</p>
<p>Para isso, nós definimos uma espécie de &quot;modelo&quot; \( p(x) \) que nos diz a probabilidade do exemplo não
ser anômalo. Usamos, também uma <em>flag</em> \( \epsilon \) (epsilon) que serve como uma divisão no nosso conjunto
de dados que diferencia os dados anômalos e não anômalos.</p>
<p>Uma aplicação comum de detecção de anomalias é a detecção de fraudes:</p>
<ul>
<li>
<p>\( x ^{(i)} \) = dados das atividades do usuário \( i \)</p>
</li>
<li>
<p>Modelo \( p(x) \) dos dados</p>
</li>
<li>
<p>Identifica usuários não usuais checando se \( p(x) &lt; \epsilon \)</p>
</li>
</ul>
<p>Assim, caso o exemplo \( x ^{(i)} \) seja considerado anômalo, o valor de \( p(x) &lt; \epsilon \), caso contrário
\( p(x \geq \epsilon) \).</p>
<p>Caso o detector de anomalias esteja detectando muitos exemplos anômalos, devemos diminuir o valor
da <em>flag</em> \( \epsilon \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="distribuição-gaussiana"><a class="header" href="#distribuição-gaussiana">Distribuição Gaussiana</a></h1>
<p>A Distribuição Gaussiana, ou Distribuição Normal, é uma forma de distribuição uniforme em formato
de sino que pode ser descrita através de uma função \( N(\mu , \sigma ^2) \). Onde, dado um valor
\( x, x \sim N(\mu , \sigma ^2) \), representa a probabilidade na distribuição de \( x \),
com média \( \mu \) e variância \( \sigma ^2 \).</p>
<p>A Distribuição Gaussiana, então, é parametrizada pela média e pela variância do conjunto de dados
(Figura 36).</p>
<p>Além disso, \( \mu \) representa o centro da curva (média) e a largura da distribuição é descrita
por \( \sigma \) (desvio padrão).</p>
<p>A função da Distribuição Gaussiana é definida da seguinte forma:</p>
<hr />
<p>\[
\Large{} p(x; \mu , \sigma ^2) = \frac{1}{\sigma \sqrt{(2 \pi)}}
e ^{- \frac{1}{2} (\frac{x - \mu}{\sigma}) ^2}
\]</p>
<hr />
<p>Podemos estimar o parâmetro \( \mu \) de um dado conjunto de dados apenas calculando a média do todos
os exemplos:</p>
<hr />
<p>\[
\large{} \mu = \frac{1}{m} \sum _{i=1} ^m x ^i
\]</p>
<hr />
<p align="center">
  <img src="parte-5/1/./img/36.png">
</p>
<p align="center">
(a) Representação da Distribuição Gaussiana. O eixo \( x \) representa os valores dos exemplos de acordo com a média
e com a variância e o eixo \( y \) representa a densidade de probabilidade o exemplo.
</p>
<p align="center">
  <img src="parte-5/1/./img/36-2.png">
<p>
<p align="center">
(b) Representação de diferentes estruturas distribuições gaussianas de acordoc om os valores dos parâmetros \( µ \) e
\( \sigma ^2 \) . Podemos perceber o efeito da alteração desses parâmetros nas estruturas das curvas.
</p>
<p align="center">
Figura 36
</p>
<p>Da mesma forma, podemos estimar o parâmetro \( \sigma ^2 \) através da fórmula de erro quadrático a qual já
estamos familiarizados.</p>
<hr />
<p>\[
\large{} \sigma ^2 = \frac{1}{m} \sum _{i=1} ^m (x ^i - \mu) ^2
\]</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="algoritmo-2"><a class="header" href="#algoritmo-2">Algoritmo</a></h1>
<p>Dado um conjunto de exemplos de treino \( { x ^{(1)}, x ^{(2)}, \dots , x ^{(m)} } \)
onde cada exemplo é um vetor \( x \in \mathbb{R} ^n \).</p>
<p>\[
\large{} p(x) = p(x _1; \mu _1 , \sigma _1 ^2)
p(x _2; \mu _2 , \sigma _2 ^2) \dots
p(x _n; \mu _n , \sigma _n ^2)
\]</p>
<p>Nesse caso, os valores das probabilidades acima são multiplicados pois os valores de \( x _i \), de treino,
são independentes entre si.</p>
<p>De outra forma, podemos escrever a expressão acima em forma de produtório, como segue:</p>
<p>\[
\large{} p(x) = \prod _{j=1} ^n p(x _j; \mu _j , \sigma _j ^2)
\]</p>
<p>Com isso, podemos definir o algoritmo de detecção de anomalias através desses valores de
probabilidade.</p>
<ol>
<li>
<p>Escolher os valores \( x _i \) que possam ser indicativos de exemplos de anomalia</p>
</li>
<li>
<p>Ajustar os parâmetros \( \mu _1 , \mu _2 , \dots , \mu _n , \sigma _1 ^2 , \sigma _2 ^2 , \dots , \sigma _n ^2 \)</p>
</li>
<li>
<p>Calcular \( \mu _j = \frac{1}{m} \sum _{i=1} ^m x _j ^{(i)} \)</p>
</li>
<li>
<p>Calcular \( \sigma ^2 = \frac{1}{m} \sum _{i=1} ^m (x _j ^i - \mu _j) ^2 \)</p>
</li>
<li>
<p>Dado um novo exemplo \( x \), calcular
\[
\large{} p(x)= \prod _{j=1} ^n p(x _j; \mu _j , \sigma _j ^2) =
\prod _{j=1} ^n \frac{1}{\sigma _j \sqrt{(2 \pi)}} e ^{- \frac{1}{2} (\frac{x _j - \mu _j}{\sigma _j}) ^2}
\]</p>
</li>
<li>
<p>Caso o valor de \( p(x) \) seja menor que \( \epsilon \), então o exemplo \( x \) é uma anomalia.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aprendizado-em-larga-escala"><a class="header" href="#aprendizado-em-larga-escala">Aprendizado em larga escala</a></h1>
<p>Na maioria das vezes, trabalhar com um grande conjunto de dados é muito melhor para qualquer
tipo de problema que envolva aprendizado de máquina. Por isso, muitas vezes, iremos trabalhar com
conjuntos de dados de tamanhos como \( m = 100.000.000 \) e, neste caso, o algoritmo de gradiente
descendente terá que realizar operações com todos os 100 milhões de dados - o que torna esse
método extremamente ineficiente e complexo. Queremos evitar tentar evitar isso criando algoritmos
mais otimizados que serão descritos e detalhados nas seções seguintes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="batch-normalization"><a class="header" href="#batch-normalization"><em>Batch Normalization</em></a></h1>
<p>Quando falarmos em normalização, estamos nos referindo à compactação de uma gama diversa
de números em uma gama fixa, como foi visto, por exemplo, na Seção <a href="parte-5/2/../1/5-1-2.html">Distribuição Gaussiana</a>
quando falamos de Distribuição Normal.</p>
<p>Por exemplo, quando temos entradas com uma variância muito alta, desejamos normalizar esses
valores a fim de gerarmos uma distribuição mais simples de trabalhar, colocando os valores em um
mesmo alcance. Isso se chama, <em>input normalization</em>.</p>
<p>Redes neurais profundas, geralmente possuem uma sequência de camadas que, muitas vezes, somam
milhares de neurônios. De camada a camada, esses valores podem se distinguir através das sequencias
operações, dificultando que as camadas da rede neural consigam fazer o trabalho de manter esses
valores bem distribuídos e estáveis.</p>
<p>Outro problema é quando a escala de diferentes parâmetros da rede neural está vinculada às atualizações
do gradiente, o que pode levar ao problema de explosão e desaparecimento dos gradientes.
Esses problemas estão relacionados, respectivamente ao crescimento descontrolado e infinito dos
valores do, ou à queda descontrolada tendendo à zero dos valores do gradiente.</p>
<p>Geralmente, para evitar esse tipo de problema, desejamos desacoplar a escala dos parâmetros da
rede neural das atualizações do gradiente e, para isso, podemos usar funções de ativação mais
robustas e estáveis, como por exemplo, ReLU (Seção
<a href="parte-5/2/../../parte-3/2/3-2-1-5.html">Camada de ativação: Rectified Linear layer (ReLU)</a>), escolhendo taxas de aprendizado
ótimas e inicializando os parâmetros das redes neurais cuidadosamente. Todavia, quanto mais tarefas
adicionamos, aumentamos a complexidade da rede neural, o que, muitas vezes, pode ser um problema.</p>
<p>Para solucionar esses problemas, utilizamos um método chamado <em>Batch Normalization</em>. Esse método
tem como objetivo evitar gradientes instáveis, reduzir os efeitos da inicialização da rede neural na
convergência do algoritmo de otimização e possibilitar o uso de taxas de aprendizado mais rápidas
para acelerar a convergência do algoritmo de otimização do gradiente.</p>
<p>Para isso, devemos normalizar cada camada intermediária da rede neural de acordo com as entradas
de cada camada, calculando a média e o desvio padrão de cada uma delas.</p>
<p>Assim, podemos finalmente descrever o algoritmo de normalização <em>Batch Normalization</em>.</p>
<hr />
<p><strong>Algorithm 9</strong> Algoritmo <em>Batch Normalization</em> (BN)</p>
<hr />
<p>1: <strong>procedure</strong> (Valores de \( x \) sobre um <em>mini-batch</em>: \( \mathcal{B} = { x _1 , \dots , m } \)),
Parâmetros a serem aprendidos: \( \gamma , \beta \)</p>
<p>2:   \( \large{} \mu _{\mathcal{B}} \leftarrow \frac{1}{m} \sum _{i=1} ^m x ^i \)
   \( \rhd \) Média <em>mini-batch</em></p>
<p>3:   \( \large{} \sigma _{\mathcal{B}} ^2 \leftarrow \frac{1}{m} \sum _{i=1} ^m (x _i - \mu _{\mathcal{B}}) ^2 \)
   \( \rhd \) Variância <em>mini-batch</em></p>
<p>4:   \( \large{} x̂ _i \leftarrow \frac{x _i - \mu _{\mathcal{B}}}{\sqrt{\sigma _{\mathcal{B}} ^2 + \epsilon}} \)
   \( \rhd \) Normalização</p>
<p>5:   \( \large{} y _i = \gamma x̂ _i + \beta \equiv BN _{\gamma , \beta} (x _i) \)
   \( \rhd \) Escala e <em>shift</em></p>
<p>6: <strong>end procedure</strong></p>
<hr />
<p>No algoritmo acima, as primeiras três equações calculam a média e o desvio padrão e depois normalizam
as entradas, respectivamente. O valor de \( \epsilon \) é um número que ajuda na estabilidade numérica,
evitando que seja efetuada uma divisão por zero. A principal característica desse algoritmo é que a
normalização acontece para cada um dos inputs no <em>batch</em> separadamente.</p>
<p>Na última equação, temos dois novos parâmetros, \( \gamma \) e \( \beta \), que representam respectivamente o
<em>scaling</em> e o <em>shift</em> dos valores. Isso limita o alcance dos valores, facilitando com que a próxima camada trabalhe
com eles de forma mais simples. Esses valores são aprendidos pela rede neural e, assim, podemos
ajustar esses valores de acordo com a distribuição.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stochastic-gradient-descent-sgd"><a class="header" href="#stochastic-gradient-descent-sgd"><em>Stochastic Gradient Descent</em> (SGD)</a></h1>
<p>SGD é uma alternativa mais eficiente e escalável ao método clássico de otimização gradiente des-
cendente.</p>
<p>SGD pode ser escrito de maneira muito similar ao método de gradiente descendente clássico, visto.</p>
<p>\[
\large{} cost(\theta , (x ^{(i)}, y ^{(i)})) = \frac{1}{2}(h _{\theta}(x ^{(i)}) - y ^{(i)}) ^2
\]</p>
<p>A principal diferença entre o método clássico e o SGD é que o SGD considera cada exemplo de treino
individualmente para calcular o custo antes de realizar a média dos custos, enquanto o método clássico
calcula o custo de acordo com a média dos valores.</p>
<p>Com isso, podemos definir a função custo da seguinte forma:</p>
<p>\[
\large{} J _{train}(\theta) = \frac{1}{m} \sum  _{i=1} ^m cost(\theta , (x ^{(i)}, y ^{(i)}))
\]</p>
<p>\( J _{train} \), agora, é apenas a média dos custos aplicado a todos os exemplos de treino.</p>
<p>Com esse tipo de metodologia, podemos otimizar o cálculo das derivadas parciais a fim de reduzir a
função custo. Devido ao fato desse método ser estocástico, adicionamos aleatoriedade nas escolhas
dos custos dos exemplos de treino. Para cada uma dessas escolhas, otimizamos os valores dos custos
o que reduz de forma muito mais eficiente a complexidade da computação. Portanto, com o SGD,
computamos a atualização do parâmetro \( \theta \) para cada um dos exemplos de treino \( x _i \) em relação ao
seu rótulo \( y _i \), como podemos verificar abaixo:</p>
<p>\[
\large{} \theta = \theta - \alpha \cdot \nabla _{\theta} J(\theta ; x ^{(i)}; y ^{(i)})
\]</p>
<p>onde \( \alpha \) é a taxa de aprendizado e \( \nabla \) é o gradiente do custo \( J \).</p>
<p>Podemos visualizar na Figura 37 as diferenças entre as etapas de aprendizado entre o método clássico
do gradiente descendente e o método estocástico. Percebemos, que SGD é mais &quot;barulhento&quot; em
relação ao método clássico, porém, como não precisamos calcular as derivadas para cada um dos \( m \) m
exemplos de treino por iteração, o SGD acaba sendo mais eficiente.</p>
<p align="center">
  <img src="parte-5/2/./img/37.png">
</p>
<p align="center">
Figura 37
</p>
<p>O algoritmo de SGD é muito similar ao algoritmo clássico, iremos atualizar os parâmetros \( \Theta _j \) ao
longo do aprendizado de acordo com a função custo, como segue:</p>
<hr />
<p><strong>Algorithm 10</strong> Algoritmo Stochastic Gradient Descent (SGD)</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   Aleatoriamente,&quot;misturar&quot; o conjunto de dados</p>
<p>3:   <strong>for</strong> \( i = 1 \) <strong>to</strong> \( m \) <strong>do</strong></p>
<p>4:   
\( \theta _j := \theta _j - \alpha (h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)} \)</p>
<p>5:   <strong>end for</strong></p>
<p>6: <strong>end procedure</strong></p>
<hr />
<p>Esse algoritmo, diferentemente do método clássico, ajustará um exemplo de treino por vez. Com
isso, podemos prosseguir com o algoritmo de gradiente descendente, sem, necessariamente examinar
todos os \( m \) exemplos de treino antes.</p>
<p>Um dos problemas desse algoritmo é que, muitas vezes, ele não convergirá para o mínimo global do
problema e, ao invés disso, irá vagar aleatoriamente ao redor desse mínimo, porém, na maioria das
vezes, produz um retorno muito próximo ao esperado. Normalmente, o SGD varre o conjunto de
dados entre uma ou dez vezes antes de chegar próximo ao mínimo global. Como podemos perceber
na Figura 38, o SGD pode acabar travado em um mínimo local e isso resulta em dificuldade em
otimizar ao máximo a função custo, pois o algoritmo não irá convergir para o mínimo global.</p>
<p align="center">
  <img src="parte-5/2/./img/38.png">
</p>
<p align="center">
Figura 38: Representação da dificuldade de convergência do algoritmo SGD. Percebe-se que a minimização da função
custo atinge um mínimo local devido às "voltas" que o algoritmo dá ao redor do mínimo global.
</p>
<p>Podemos evitar esse tipo de problema incentivando a convergência do algoritmo. Para isso, podemos
escolher valores de α diferentes a cada iteração, diminuindo o seu valor.</p>
<p>Com uma taxa de aprendizado menor, o algoritmo irá oscilar de maneira mais suave, sem grandes
pulos em torno do mínimo global. Dessa forma, uma estratégia seria diminuir o valor de \( \alpha \) a cada
iteração, como segue:</p>
<p>\[
\large{} \alpha = \frac{const1}{iterationNumber+const2}
\]</p>
<p>Todavia, esse método não é muito utilizado devido a quantidade extra de parâmetros que devem ser
ajustados.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mini-batch-gradient-descent"><a class="header" href="#mini-batch-gradient-descent"><em>Mini-batch Gradient Descent</em></a></h1>
<p><em>Mini-batch Gradient Descent</em> muitas vezes, pode ser mais eficiente que o SGD. Ao invés de usarmos
todos os \( m \) exemplos de treino, como no método clássico, ou apenas um exemplo no SGD, usaremos
algo entre essas duas opções. Selecionaremos \( b \) exemplos de treino. O valor de \( b \), geralmente está
entre 2-100.</p>
<p>Na figura a seguir, podemos verificar as diferenças entre o SGD e o <em>Mini-batch Gradient Descent</em>.
Na imagem, percebe-se que o método <em>Mini-batch</em> converge mais rapidamente, principalmente devido
ao fato de processar os dados em batches de forma vetorizada.</p>
<p align="center">
  <img src="parte-5/2/./img/39.png">
</p>
<p align="center">
Figura 39
</p>
<p>Por exemplo, se \( b = 10 \) e \( m = 1000 \), teremos o seguinte algoritmo:</p>
<hr />
<p><strong>Algorithm 11</strong> Algoritmo Mini-batch Gradient Descent</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   Aleatoriamente,&quot;misturar&quot; o conjunto de dados</p>
<p>3:   <strong>for</strong> \( i = 1,11,21,31, \dots , 991 \) <strong>do</strong></p>
<p>4:   
\( \theta _j := \theta _j - \alpha \frac{1}{10} \sum _{k=i} ^{i+9} (h _{\theta}(x ^{(k)}) - y ^{(k)}) \cdot x _j ^{(k)} \)</p>
<p>5:   <strong>end for</strong></p>
<p>6: <strong>end procedure</strong></p>
<hr />
<p>Nesse algoritmo, estamos simplesmente somando dez exemplos por vez. A principal vantagem em
computar mais de um exemplo por vez (em <em>batches</em>) é que podemos usar implementações vetorizadas
sobre os exemplos em \( b \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sgd--momentum"><a class="header" href="#sgd--momentum">SGD + <em>Momentum</em></a></h1>
<p>Como foi explicado, um dos problemas do SGD é que muitas vezes, o algoritmo pode ficar &quot;preso&quot;
em um mínimo local próximo do global e não convergir de forma adequada para o nosso problema.
Para isso, uma forma de otimização foi utilizar <em>momentum</em> a fim de incentivar a convergência.</p>
<p>Antes de definir esse método de otimização, podemos intuitivamente compreender como funciona o
<em>momentum</em>. Para isso, podemos pensar em uma bola que rola em uma colina do ponto mais alto ao
mais baixo, seguindo a inclinação. Desejamos que essa bola tenha a noção de onde ela está indo.
Essa intuição pode ser relacionada aos valores dos parâmetros que desejamos atualizar, ou seja, esses
parâmetros se comportariam como se fossem a tal bola.</p>
<p>Uma boa definição para o <em>momentum</em> é que é uma média móvel dos nossos movimentos, ou seja,
como determinado corpo se comporta em determinado terreno de acordo com a velocidade e a
aceleração proporcionada pelo mesmo. Dessa forma, a cada etapa do processo de otimização da
função custo, teremos vetores que representam o <em>momentum</em> do ponto em determinada parte do
&quot;terreno&quot; da função custo, e esses vetores proporcionarão uma velocidade maior de convergência na
direção a qual ele está apontando.</p>
<p>Na Figura 40 está representada a diferenciação no comportamento do SGD sem e com a utilização
de <em>momentum</em>.</p>
<p align="center">
  <img src="parte-5/2/./img/40.png">
</p>
<p align="center">
Figura 40: À esquerda representação no comportamento dos parâmetros sendo minimizados através do método SGD
e à direita o mesmo método com a aplicação de <i>momentum</i>.
</p>
<p>Podemos diferenciar os dois modelos de otimização da forma que segue:</p>
<table><thead><tr><th>SGD</th><th>SGD + Momentum</th></tr></thead><tbody>
<tr><td>\( x _{t+1} = x _t - \alpha \nabla f(x _t) \)</td><td>\( v _{t+1} = \rho v _t + \nabla f(x _t) \)</td></tr>
<tr><td></td><td>\( x _{t+1} = x _t - \alpha v _{t+1} \)</td></tr>
</tbody></table>
<p>Onde \( x _i \) representa os parâmetros a serem atualizados, \( v _i \) representa o momentum do ponto em determinada parte,
\( \nabla f(x _i) \) representa o gradiente da função custo e \( \rho \) representa o atrito do ponto
em determinada parte (geralmente esse parâmetro é definido como 0.9 ou 0.99).</p>
<p>Contudo, um dos problemas que esse método de otimização proporciona é que, muitas vezes, quando
os parâmetros se aproximam do mínimo, devido ao <em>momentum</em>, demoram para convergir pois ultrapassam
esse mínimo. Para resolver esse problema, devemos desacelerar a atualização desses
parâmetros de acordo que eles se aproximam do mínimo, o que será visto na seção seguinte.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nesterov-accelerated-gradient-nag"><a class="header" href="#nesterov-accelerated-gradient-nag">Nesterov <em>Accelerated Gradient</em> (NAG)</a></h1>
<p>O método NAG utiliza da mesma ideia vista na seção anterior, porém, como foi dito, desaceleramos
a atualização dos parâmetros de forma a não ultrapassar de maneira muito significativa o valor de
mínimo da função.</p>
<p>Na Figura 41, abaixo está representada a diferença no comportamento da atualização dos valores
dos parâmetros.</p>
<p align="center">
  <img src="parte-5/2/./img/41.png">
</p>
<p align="center">
Figura 41: Exemplificação do método de Nesterov (à direita) para gradiente descendente utilizando <i>momentum</i>
(à esquerda). Percebe-se que a cada passo, os valores da função custo diminuem tendendo ao mínimo. E quando esse
valor se aproxima do mínimo, desacelera para facilitar a convergência.
</p>
<p>Nesse método de otimização da função custo, primeiro olhamos para o onde o vetor atual <em>momentum</em>
está apontando para, com ele, calcularmos o gradiente partindo desse ponto, como podemos verificar
na Figura 42.</p>
<p align="center">
  <img src="parte-5/2/./img/42.png">
</p>
<p align="center">
Figura 42: Representação da atualização dos vetores <i>momentum</i> utilizando o método de Nesterov. Na imagem à
esquerda, verificamos a atualização do vetor <i>momentum</i> a partir de um determinado ponto. Na figura à direita
verificamos a atualização do vetor <i>momentum</i> através do método de Nesterov, a partir do ponto onde o vetor está
apontando.
</p>
<p>O método de Nesterov utiliza o termo de <em>momentum</em> \( \rho v _{t-1} \) para atualizar os parâmetros \( \theta \).
Assim, se computamos \( \theta - \rho v _{t-1} \) temos a aproximação da próxima posição dos parâmetros \( \theta \).
Para calcularmos a atualização dos parâmetros, primeiro devemos definir a atualização de \( v _t \) utilizando o método de
Nesterov, como descrito abaixo.</p>
<p>\[
\large{} v _{t} = \rho v _{t-1} + \alpha \nabla _{\theta} J(\theta - \rho v _{t-1})
\]</p>
<p>\[
\large{} \theta = \theta - v _t
\]</p>
<p>onde, \( \rho \) é o termo definido como <em>momentum</em> (atrito, geralmente um valor em torno de 0.9), \( \alpha \) é a
taxa de aprendizado e \( \nabla _{\theta} \) é o gradiente de \( J \) em relação aos parâmetros \( \theta \).</p>
<p>Assim, com a aplicação do método de Nesterov <em>Momentum</em>, o algoritmo de otimização da função
converge muito mais rapidamente, evitando o problema de aproximação ruim da convergência no
SGD.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adagrad"><a class="header" href="#adagrad">AdaGrad</a></h1>
<p>Outro método de otimização é chamado de <em>Adaptive Gradient Algorithm</em> (Adagrad). Esse método
adapta a taxa de aprendizado aos parâmetros, realizando atualizações menores nos parâmetros,
diminuindo o &quot;barulho&quot; das atualizações.</p>
<p>Como não precisamos definir uma taxa de aprendizado \( \alpha \), a convergência é mais rápida e confiável. O
método adapta as atualizações e a taxa de aprendizado automaticamente de acordo com os níveis da
superfície, impossibilitando que as atualizações sejam muito oscilantes em superfícies mais instáveis.</p>
<p>Essa atualização, ocorre da seguinte forma:</p>
<p>\[
\large{} \theta _{t+1,i} = \theta _{t,i} - \frac{\eta}{\sqrt{G _{t,ii}} + \epsilon} \cdot g _{t,i}
\]</p>
<p>Atualizamos os parâmetros \( \theta \), multiplicando a taxa de aprendizado \( \eta \) pelo gradiente do ponto, e
dividimos esse valor pela raiz quadrada dos somatórios dos quadrados dos gradientes para cada
iteração \( t \). Além disso, \( \epsilon \cong 10 ^{-7} \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adam"><a class="header" href="#adam">Adam</a></h1>
<p><em>Adaptative Moment Estimation</em> (Adam) é outro método de otimização que computa taxas de aprendizado
adaptativas para cada parâmetro, assim como AdaGrad e outros métodos de otimização.</p>
<p>Enquanto nos métodos que utilizam <em>momentum</em>, nós podemos pensar em uma bola descendo uma
curva em direção ao ponto mínimo, Adam se comporta como se fosse uma bolsa pesada com atrito
descendo essa superfície. Essa forma de minimização faz com que o erro de minimização seja o
menor possível, ao mesmo tempo que o tempo de convergência seja pequeno.</p>
<p>Assim como os otimizadores que utilizam <em>momentum</em>, vistos nas seções anteriores, Adam utiliza
uma média exponencial dos gradientes calculados anteriormente para definir a taxa de aprendizado.
Com isso, a atualização dos parâmetros se dá de forma muito similar aos otimizadores que utilizam
<em>momentum</em>.</p>
<p>A intuição da implementação do Adam é dividida em três etapas: <em>momentum</em>, correção de <em>bias</em> e
AdaGrad.</p>
<p>Na primeira etapa - <em>momentum</em> - computamos as médias vetores momentum passados \( m _t \) e, na
terceira etapa, os quadrados dos gradientes passados \( v _t \), respectivamente, como segue:</p>
<p>\[
\large{} m _t = \beta _1 m _{t-1} + (1 - \beta _1) \nabla _{\theta} J(\theta)
\]</p>
<p>\[
\large{} v _t = \beta _2 v _{t-1} + (1 - \beta _2) \nabla _{\theta} J(\theta) ^2
\]</p>
<p>Onde \( m _t \) e \( v _t \) são, respectivamente, o <em>momentum</em> acumulado e o gradiente acumulado até um
período \( t \).</p>
<p>Como, \( m _t \) e \( v _t \) são previamente, inicializados com 0's, foi observado que essas atualizações,
principalmente nas etapas iniciais, tendem à zero e, especialmente quando a taxa de decaimento é baixa
(ou seja, quando \( \beta _1 \) e \( \beta _2 \) estão próximos de 1).</p>
<p>Para isso, na segunda etapa neutralizamos essas tendências não previstas através de <em>bias-correction</em>.
Para isso, calculamos o valor de \( m̂ _t \) para atualizar a direção de cada um dos parâmetros e, através
dos valores dos gradientes de \( v̂ _t \), atualizamos efetivamente a otimização dos parâmetros. Assim,
teremos:</p>
<p>\[
\large{} m̂ _t = \frac{m _t}{1 - \beta _1 ^t}
\]</p>
<p>\[
\large{} v̂ _t = \frac{v _t}{1 - \beta _2 ^t}
\]</p>
<p>Agora, só basta calcular as atualizações dos parâmetros \( \theta \) utilizando, por exemplo AdaGrad.</p>
<p>\[
\large{} \theta _{t+1} = \theta _t - \frac{\eta}{\sqrt{v̂ _t} + \epsilon} m̂ _t
\]</p>
<p>Com isso, a otimização funcionaria corretamente com os parâmetros propostos para \( \beta _1 = 0.9 \) e
\( \beta _2 = 0.999 \) e taxa de aprendizado \( \eta = 10 ^{-3} \) ou \( 5 \times 10 ^{-4} \)</p>
<p>Na figura abaixo, podemos perceber as diferenças entre os modelos de otimizações discutidos. Adam
combina cada um dos modelos vistos anteriores a fim de otimizar a função custo.</p>
<p align="center">
  <img src="parte-5/2/./img/43.png">
</p>
<p align="center">
Figura 43: Comparação entre os modelos de otimização vistos. Percebe-se que Adam combinas os diferentes métodos.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce-e-paralelismo-de-dados"><a class="header" href="#mapreduce-e-paralelismo-de-dados"><em>Mapreduce</em> e Paralelismo de Dados</a></h1>
<p>Muitas vezes, iremos trabalhar com grandes quantidades de dados e para isso, iremos dividir os
o conjunto de dados processado no algoritmo de otimização em subconjuntos para que possamos
processar cada um deles em diferentes máquinas e, assim, treinar o nosso algoritmo em paralelo.</p>
<p>Podemos dividir o nosso conjunto de treino em \( z \) subconjuntos, onde \( z \) corresponde ao número de
máquinas que podemos usar em paralelo. Para cada uma dessas máquinas, calculamos</p>
<p>\[
\large{} \sum _{i=p} ^q   (h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)}
\]</p>
<p>Com isso, <em>Mapreduce</em> irá utilizar desses dados mapeados a cada máquina, e &quot;reduzir&quot; eles calculando:</p>
<hr />
<p><strong>Algorithm 12</strong> Algoritmo <em>Mapreduce</em></p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>for</strong> \( i=0 \) <strong>to</strong> \( n \) <strong>do</strong></p>
<p>3:   
\( \theta := \theta _j - \alpha \frac{1}{z}(temp _j ^{(1)} + temp _j ^{(2)} + \dots + temp _j ^{(z)}) \)</p>
<p>4:   <strong>end for</strong></p>
<p>5: <strong>end procedure</strong></p>
<hr />
<p>Onde o valor de \( temp _j \) representa a função custo calculada em cada uma das máquinas. <em>Mapreduce</em>
calcula a média, multiplicando pela taxa de aprendizado \( \alpha \) e atualizando os valores de \( \theta \).</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parte-vi"><a class="header" href="#parte-vi">Parte VI</a></h2>
<h1 id="tópicos-avançados"><a class="header" href="#tópicos-avançados">Tópicos avançados</a></h1>
<ul>
<li>
<p><a href="./parte-6/1/6-1.html">Redes neurais convolucionais (Convolutional neural networks)</a></p>
<ul>
<li>
<p><a href="./parte-6/1/6-1-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-2.html">Camadas de uma ConvNet</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-3.html">Técnicas de otimização de treino</a></p>
<ul>
<li>
<p><a href="./parte-6/1/6-1-3-1.html">Data Augmentation</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-3-2.html">Modelos pré-treinados</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-3-3.html">Fine Tuning</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/1/6-1-4.html">Detecção e Segmentação</a></p>
<ul>
<li>
<p><a href="./parte-6/1/6-1-4-1.html">Segmentação Semântica</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-4-2.html">Localização</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-4-3.html">Detecção de objetos</a></p>
</li>
<li>
<p><a href="./parte-6/1/6-1-4-4.html">Segmentação de instâncias</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/2/6-2.html">Redes neurais recorrentes (Recurrent neural networks)</a></p>
<ul>
<li>
<p><a href="./parte-6/2/6-2-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-2.html">Vetores de palavras (word embeddings)</a></p>
<ul>
<li>
<p><a href="./parte-6/2/6-2-2-1.html">One-hot encoding</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-2-2.html">Word2Vec</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/2/6-2-3.html">Arquitetura de uma RNN</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-4.html">Aplicações</a></p>
<ul>
<li>
<p><a href="./parte-6/2/6-2-4-1.html">One-to-one</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-4-2.html">One-to-many</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-4-3.html">Many-to-one</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-4-4.html">Many-to-many</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/2/6-2-5.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-6.html">Backpropagation</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-7.html">Função de ativação e propriedades</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-8.html">Gradiente de desaparecimento e explosão</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-9.html">Long Short-Term Memory (LSTM)</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-10.html">Bidirectional &amp; Multi-layer RNNs</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-11.html">Attention</a></p>
</li>
<li>
<p><a href="./parte-6/2/6-2-12.html">Redes neurais convolucionais para NLP</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/3/6-3.html">Transformers</a></p>
<ul>
<li>
<p><a href="./parte-6/3/6-3-1.html">Positional encoding</a></p>
</li>
<li>
<p><a href="./parte-6/3/6-3-2.html">Encoder</a></p>
<ul>
<li>
<p><a href="./parte-6/3/6-3-2-1.html">Self-Attention</a></p>
</li>
<li>
<p><a href="./parte-6/3/6-3-2-2.html">Multi-head attention e geração de saída</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/3/6-3-3.html">Decoder</a></p>
<ul>
<li>
<p><a href="./parte-6/3/6-3-3-1.html">Masked Multi-head attention</a></p>
</li>
<li>
<p><a href="./parte-6/3/6-3-3-2.html">Encoder-Decoder attention</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/4/6-4.html">Modelos generativos (Generative models)</a></p>
<ul>
<li>
<p><a href="./parte-6/4/6-4-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="./parte-6/4/6-4-2.html">Aplicações</a></p>
</li>
<li>
<p><a href="./parte-6/4/6-4-3.html">Auto-Regressive Models</a></p>
<ul>
<li>
<p><a href="./parte-6/4/6-4-3-1.html">PixelRNN</a></p>
</li>
<li>
<p><a href="./parte-6/4/6-4-3-2.html">PixelCNN</a></p>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/4/6-4-4.html">Variational Autoencoders (VAE)</a></p>
</li>
<li>
<p><a href="./parte-6/4/6-4-5.html">Generative Adversarial Networks (GANs)</a></p>
<ul>
<li><a href="./parte-6/4/6-4-5-1.html">Treinamento: Jogo de dois jogadores</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="./parte-6/5/6-5.html">Deep Q-Learning</a></p>
</li>
<li>
<p><a href="./parte-6/6/6-6.html">Busca de Monte Carlo</a></p>
<ul>
<li>
<p><a href="./parte-6/6/6-6-1.html">Uninformed seaarch</a></p>
</li>
<li>
<p><a href="./parte-6/6/6-6-2.html">Monte Carlo Tree Search</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redes-neurais-convolucionais-convolutional-neural-networks"><a class="header" href="#redes-neurais-convolucionais-convolutional-neural-networks">Redes neurais convolucionais (<em>Convolutional neural networks</em>)</a></h1>
<ul>
<li>
<p><a href="parte-6/1/./6-1-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-2.html">Camadas de uma ConvNet</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-3.html">Técnicas de otimização de treino</a></p>
<ul>
<li>
<p><a href="parte-6/1/./6-1-3-1.html">Data Augmentation</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-3-2.html">Modelos pré-treinados</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-3-3.html">Fine Tuning</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-6/1/./6-1-4.html">Detecção e Segmentação</a></p>
<ul>
<li>
<p><a href="parte-6/1/./6-1-4-1.html">Segmentação Semântica</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-4-2.html">Localização</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-4-3.html">Detecção de objetos</a></p>
</li>
<li>
<p><a href="parte-6/1/./6-1-4-4.html">Segmentação de instâncias</a></p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visão-geral-2"><a class="header" href="#visão-geral-2">Visão geral</a></h1>
<p>Redes neurais convolucionais (CNNs ou ConvNets) são muito similares as redes neurais vistas nas
Seções <a href="parte-6/1/../../parte-3/1/3-1.html">Redes Neurais: Representação</a> e
<a href="parte-6/1/../../parte-3/2/3-2.html">Redes Neurais: Aprendizado</a>. Cada neurônio recebe <em>inputs</em> e executam funções
lineares e não lineares de uma extremidade a outra da rede neural através da função custo. CNN é uma
classe de rede neural artificial do tipo feed-forward, que vem sendo aplicada no processamento e
análise de imagens digitais.</p>
<p>A principal diferença entre NNs e CNNs é que, basicamente, uma CNN pode ser pensada como uma
rede neural que possui várias cópias do mesmo neurônio. Esse tipo de implementação nos permite
trabalhar com modelos computacionalmente grandes enquanto mantém o número de parâmetros
atuais.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camadas-de-uma-convnet"><a class="header" href="#camadas-de-uma-convnet">Camadas de uma ConvNet</a></h1>
<p>Como foi descrito acima, uma ConvNet é uma sequência de camadas que transforma um volume
de ativações em outro através de uma função de diferenciação. Para isso, utilizamos camadas
específicas para realizar o treinamento de uma ConvNet, dentre elas: <em>Convolutional layer</em>,
<em>Pooling layer</em> e <em>Fully-connected layer</em>. Esta última, exatamente como uma rede neural regular.</p>
<p>A principal diferença entre uma <em>dense layer</em> e uma <em>convolutional layer</em> é que as camadas densas
detectam padrões globais, enquanto camadas convolucionais detectam padrões locais.</p>
<p>Para a melhor compreensão será utilizado um exemplo CIFAR-10 <a href="parte-6/1/../../referencias.html">[1]</a> o qual consiste em uma base
de dados com 60 mil imagens de tamanho 32x32 separadas em 10 classes. O conjunto de dados é
separado em 50 mil para usado para treino e 10 mil para teste.</p>
<p>Uma ConvNet para classificar as imagens de CIFAR-10 tem a seguinte estrutura, detalhada abaixo:</p>
<ul>
<li><em>Input</em> [32x32x3]: recebe um vetor dos valores dos <em>pixels</em> da imagem. Neste caso a imagem
tem largura 32, altura 32 com três canais de cores (RGB);</li>
</ul>
<p align="center">
  <img src="parte-6/1/./img/44.png">
</p>
<p align="center">
Figura 44: Representação de uma camada de <i>input</i> de uma rede neural convolucional. Podemos perceber, que na
imagem representada temos uma imagem de tamanho 4x4 com três canais de cores RGB e cada valor de cada pixel
corresponde a um valor em cada uma dessas escalas de cores.
</p>
<ul>
<li><em>Conv Layer</em>: computa a saída dos neurônios que estão conectados a regiões de entrada. Cada
um computa um produto escalar entre os pesos e a uma região as quais eles estão conectados
no volume de entrada. Isso pode resultar em um volume como [32x32x12] se desejarmos
utilizar 12 filtros <em>kernel</em>. Este filtro possui um tamanho que varre a imagem através de
<em>strides</em> - distância entre duas varreduras do filtro - gerando uma nova imagem de mesmo tamanho,
porém com esse filtro aplicado;</li>
</ul>
<p align="center">
  <img src="parte-6/1/./img/45.png">
</p>
<p align="center">
Figura 45: Representação da utilização de um filtro (<i>kernel</i>) para mapear os valores da imagem dada como entrada.
Percebe-se que o filtro utilizado mantém as bordas da imagem de entrada.
</p>
<p align="center">
  <img src="parte-6/1/./img/46.png">
</p>
<p align="center">
Figura 46: Representação das operações realizadas por um filtro a partir de uma imagem. O filtro (3x3) multiplica os
valores da imagem (5x5) pelos seus respectivos valores até gerar uma saída correspondente de tamanho 3x3. Esses
valores retornados são somados e adicionados ao respectivo pixel da imagem (5x5) camada seguinte com o filtro
aplicado.
</p>
<ul>
<li>
<p><em>ReLU Layer</em>: computa a função de ativação;</p>
</li>
<li>
<p><em>Pool Layer</em>: computa uma operação de redução da resolução ao longo das dimensões,
resultando em um volume como [16x16x12];</p>
</li>
</ul>
<p align="center">
  <img src="parte-6/1/./img/47.png">
</p>
<p align="center">
Figura 47: Representação de uma operação de <i>pooling</i>. Na figura, utiliza-se o método de <i>max-pooling</i> com um filtro
de tamanho 2x2 o qual escolhe o maior valor do pixel que se encontra dentro do filtro.
</p>
<ul>
<li><em>FC Layer</em>: computa as pontuações das classes, resultando em um volume de tamanho [1x1x10],
onde cada um dos 10 números representa uma categoria da base de dados CIFAR-10</li>
</ul>
<p>Abaixo, na Figura 48 está representado, esquematicamente uma ConvNet voltada para a base de
dados CIFAR-10. Nela, percebemos as operações que foram realizar ao longo do processamento de
convolução e de aprendizado da classificação.</p>
<p align="center">
  <img src="parte-6/1/./img/48.png">
</p>
<p align="center">
Figura 48: Representação de uma ConvNet para a base de dados CIFAR-10
</p>
<p>Além disso, uma camada de convolução de uma ConvNet recebe alguns outros parâmetros que serão
descritos a seguir:</p>
<ul>
<li>
<p>Aceita um volume de tamanho: \( W _1 \times H _1 \times D _1 \);</p>
</li>
<li>
<p>Requer quatro hiperparâmetros:</p>
<ol>
<li>
<p>Número de filtros (<em>kernels</em>): \( K \)</p>
</li>
<li>
<p>Tamanho do filtro: \( F \)</p>
</li>
<li>
<p><em>Stride</em> (distância entre duas posições consecutivas do filtro): \( S \)</p>
</li>
<li>
<p>Tamanho da borda (<em>zero-padding</em>): \( P \)</p>
</li>
</ol>
</li>
<li>
<p>Produz um volume de: \( W _2 \times H _2 \times D _2 \) onde:</p>
<ul>
<li>
<p>\( W _2 = \frac{(W _1 - F + 2P)}{S + 1} \)</p>
</li>
<li>
<p>\( H _2 = \frac{(H _1 - F + 2P)}{S + 1} \)</p>
</li>
<li>
<p>\( D _2 = K \)</p>
</li>
</ul>
</li>
</ul>
<p>Geralmente, uma inicialização comum para esses parâmetros é \( F=3 , \ S=1 \) e \( P=1 \), porém esses
parâmetros podem ser modificados de acordo com a intenção de treino da ConvNet.</p>
<p>Da mesma forma, uma camada de <em>pooling</em> de uma ConvNet recebe alguns parâmetros a seguir
descritos:</p>
<ul>
<li>
<p>Aceita um volume de tamanho: \( W _1 \times H _1 \times D _1 \);</p>
</li>
<li>
<p>Requer dois hiperparâmetros:</p>
<ol>
<li>
<p>Tamanho do filtro: \( F \)</p>
</li>
<li>
<p><em>Stride</em>: \( S \)</p>
</li>
</ol>
</li>
<li>
<p>Produz um volume de: \( W _2 \times H _2 \times D _2 \) onde:</p>
<ul>
<li>
<p>\( W _2 = \frac{(W _1 - F)}{S + 1} \)</p>
</li>
<li>
<p>\( H _2 = \frac{(H _1 - F)}{S + 1} \)</p>
</li>
<li>
<p>\( D _2 = D _1 \)</p>
</li>
</ul>
</li>
</ul>
<p>Assim, percebemos que uma ConvNet é simplesmente uma lista de camadas que transformam um
volume de entrada (neste caso uma imagem) e um outro volume de saída (neste caso os valores das
classes).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="técnicas-de-otimização-de-treino"><a class="header" href="#técnicas-de-otimização-de-treino">Técnicas de otimização de treino</a></h1>
<p>Em muitos casos, o conjunto de dados que desejamos utilizar para treinar o nosso modelo não é
grande o suficiente para potencializar a acurácia do modelo para casos de teste. Para isso, nesta seção
serão apresentadas algumas técnicas de manipulação de dados que possam ser úteis para otimização
do treino utilizando milhares de imagens.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-augmentation"><a class="header" href="#data-augmentation"><em>Data Augmentation</em></a></h1>
<p>Essa técnica é utilizada para aumentar o tamanho do conjunto de dados e evitar casos de <em>overfitting</em>.
Essa técnica realiza transformações nas imagens que serão usadas para serem treinadas a fim de potencializar
a generalização do modelo. Essas transformações podem ser compressões, rotações,
alongamentos e algumas mudanças de cor.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="modelos-pré-treinados"><a class="header" href="#modelos-pré-treinados">Modelos pré-treinados</a></h1>
<p>Nesta seção, iremos discutir sobre modelos de ConvNets previamente treinados a fim de aumentarmos
a acurácia do nosso modelo. Em outras palavras, iremos utilizar um modelo que foi treinado com
milhões de imagens que estão relacionadas com o nosso modelo. Isso nos garante em ter camadas
de convolução extremamente eficazes.</p>
<p>Dessa forma, podemos treinar o nosso modelo com um conjunto de dados pequeno, pois a camada de
convolução já foi treinada anteriormente com um conjunto de dados muito maior, podendo, portando
aumentar a confiabilidade dos resultados de teste do nosso modelo.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fine-tuning"><a class="header" href="#fine-tuning"><em>Fine tuning</em></a></h1>
<p>Após utilizar um modelo pré-treinado, devemos ajustar as camadas finais para se adaptarem com
o nosso modelo. Fazemos isso, pois as camadas inicias conseguem classificar muito bem dados de
baixo nível, como bordas, linhas e contornos em uma imagem. Assim, se ajustarmos as camadas
finais, podemos procurar apenas recursos relevantes para o nosso problema que está relacionado ao
nosso conjunto de treino.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="detecção-e-segmentação"><a class="header" href="#detecção-e-segmentação">Detecção e Segmentação</a></h1>
<p>Detecção de objetivos é um problema multitarefa que usa algoritmos de classificação e localização a
fim de saber qual objeto é e qual a sua localização. Na Figura 49 temos uma representação de uma
imagem que os objetos presentes foram classificados e localizados.</p>
<p align="center">
  <img src="parte-6/1/./img/49.png">
</p>
<p align="center">
Figura 49: Representação de uma imagem que seus objetos foram devidamente classificados e localizados. Em (a)
está representada a classificação da imagem, em (b) a localização dos objetos, em (c) a segmentação semântica da
imagem e em (d) a segmentação classificada dos objetos e suas determinadas localizações.
</p>
<p>Este tipo de estudo é muito recorrente na área de visão computacional, principalmente usado para a
detecção de rostos através de imagens e em carros autônomos através de imagens e vídeos.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="segmentação-semântica"><a class="header" href="#segmentação-semântica">Segmentação Semântica</a></h1>
<p>Dada uma imagem, desejamos classificar cada pixel dessa imagem em diferentes categorias. Podemos
verificar na Figura 50 que dadas imagens de entradas, classificamos cada pixel em cinco diferentes
categorias: céu, árvores, grama, gato ou vaca.</p>
<p align="center">
  <img src="parte-6/1/./img/50.png">
</p>
<p align="center">
Figura 50: Representação de segmentação semântica. Na coluna da esquerda, percebemos que classificamos a
imagem de um gato em quatro categorias representadas pelas cores azul, rosa, verde e amarelo. Na coluna da direita
classificamos a imagem em quatro categorias representadas pelas cores azul, rosa, verde e marrom.
</p>
<p>Para a implementação desse método, utilizamos de ConvNets que trabalham de forma muito específica:
diminuindo e aumentando a resolução da imagem usando <em>pooling</em> e <em>upsampling</em> ou <em>unpooling</em>.</p>
<p align="center">
  <img src="parte-6/1/./img/51.png">
</p>
<p align="center">
Figura 51: Representação de uma ConvNet de segmentação semântica. Percebemos que existem dois tipos de
alteração da resolução da imagem: usando <i>pooling</i>, que diminui a resolução da imagem e <i>upsampling</i> que aumenta a
resolução da imagem.
</p>
<p>Com essa implementação, podemos promover o aprendizado da rede neural centrado em cada pixel
da imagem e, assim, classificar cada pixel em diferentes classes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="localização"><a class="header" href="#localização">Localização</a></h1>
<p>Dada uma imagem, desejamos localizar onde está determinado objeto na imagem. Para isso, temos
dois tipos de localização: localização com apenas um objeto na imagem e localização com mais de
um objeto.</p>
<p>Podemos verificar como funciona a localização de objetos em imagens na Figura 52</p>
<p align="center">
  <img src="parte-6/1/./img/52.png">
</p>
<p align="center">
Figura 52: Representação de uma localização de objetos. Percebemos que a partir de uma coordenada \( (x, y) \)
prevemos os valores da altura e da largura da <i>bounding box</i> que limita a figura do gato, neste caso.
</p>
<p>Para o problema de localização de objetos, trataremos esse problema como um problema de regressão,
que dado uma coordenada \( (x, y) \) de uma <em>bounding box</em>, desejamos prever as outras coordenadas dessa
<em>bounding box</em> de acordo com a classificação gerada pela ConvNet. Assim, o resultado da função de
localização será uma tupla do tipo \( (x, y, w, h) \) onde \( x \) e \( y \) são as coordenadas iniciais da
<em>bounding box</em> e \( w \) e \( h \) são, respectivamente a largura e a altura da <em>bounding box</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="detecção-de-objetos"><a class="header" href="#detecção-de-objetos">Detecção de Objetos</a></h1>
<p>Dada uma imagem que contém múltiplos objetos, desejamos classificar e localizar cada um desses
objetos presentes na imagem.</p>
<p>Podemos verificar na Figura 53 como funciona a detecção de objetos.</p>
<p align="center">
  <img src="parte-6/1/./img/53.png">
</p>
<p align="center">
Figura 53: Representação de detecção de objetos. Percebe-se que, para cada um dos objetos na imagem, existe uma
<i>bounding box</i> localizando cada um dos objetos e para cada uma dessas <i>bounding boxes</i>, sua classificação.
</p>
<p>Para isso, foram criados dois métodos principais de detecção de objetos em imagens <em>You Only Look Once</em>
(YOLO) e <em>Single Shot Detector</em> (SSD).</p>
<p>Para YOLO, o método é essencialmente regressão, o qual usa uma imagem para aprender as possibilidades
das classes de acordo com as coordenadas das <em>bounding boxes</em>. YOLO divide cada imagem
em uma rede \( SxS \) e cada rede prevê \( N \) <em>bounding boxes</em> e a confiança da precisão se cada
<em>bounding box</em> realmente está contornando um objeto (Figura 54). Então, são previstas um total de \( SxSxN \)
<em>bounding boxes</em> que, em sua maioria têm pontuações de confiança baixas e, portanto, podemos nos
livrar delas.</p>
<p align="center">
  <img src="parte-6/1/./img/54.png">
</p>
<p align="center">
Figura 54: Representação de um algoritmo de detecção de objetos usando o método YOLO. A partir de uma imagem
de entrada, a dividimos em uma rede de tamanho \( SxS \) e, através de um algoritmo de regressão, encontramos as
coordenadas das <i>bounding boxes</i> e a confiança de que cada uma delas possui um objeto.
</p>
<p>Por outro lado, SSD atinge um melhor equilíbrio entre rapidez e precisão. O SSD executa uma
ConvNet na imagem de entrada apenas uma vez e calcula um mapa de recursos (<em>feature map</em>). Agora,
executamos a convolução com um <em>kernel</em> 3x3 para prever as <em>bounding boxes</em> e as probabilidades de
categorização.</p>
<p align="center">
  <img src="parte-6/1/./img/55.png">
</p>
<p align="center">
Figura 55: Representação de um algoritmo de detecção de imagens usando SSD. A partir de uma imagem de entrada,
através de uma ConvNet prevemos as coordenadas das <i>bounding boxes</i> com suas respectivas probabilidades, através
de apenas uma passagem pela rede para cada objeto.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="segmentação-de-instâncias"><a class="header" href="#segmentação-de-instâncias">Segmentação de Instâncias</a></h1>
<p>Assim, como detecção de objetos, desejamos localizar e classificar objetos em uma dada imagem,
porém, além de apenas definir uma <em>bounding box</em> para cada um dos objetos, desejamos segmentar a
imagem de acordo com cada objeto encontrado e classificá-los, como pode-se perceber na Figura 56</p>
<p align="center">
  <img src="parte-6/1/./img/56.png">
</p>
<p align="center">
Figura 56: Comparação entre os diferentes tipos de detecção e segmentação de objetos em imagens.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redes-neurais-recorrentes-recurrent-neural-networks"><a class="header" href="#redes-neurais-recorrentes-recurrent-neural-networks">Redes neurais recorrentes <em>(Recurrent neural networks)</em></a></h1>
<ul>
<li>
<p><a href="parte-6/2/./6-2-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-2.html">Vetores de palavras (word embeddings)</a></p>
<ul>
<li>
<p><a href="parte-6/2/./6-2-2-1.html">One-hot encoding</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-2-2.html">Word2Vec</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-6/2/./6-2-3.html">Arquitetura de uma RNN</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-4.html">Aplicações</a></p>
<ul>
<li>
<p><a href="parte-6/2/./6-2-4-1.html">One-to-one</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-4-2.html">One-to-many</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-4-3.html">Many-to-one</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-4-4.html">Many-to-many</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-6/2/./6-2-5.html">Função Custo (Cost Function)</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-6.html">Backpropagation</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-7.html">Função de ativação e propriedades</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-8.html">Gradiente de desaparecimento e explosão</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-9.html">Long Short-Term Memory (LSTM)</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-10.html">Bidirectional &amp; Multi-layer RNNs</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-11.html">Attention</a></p>
</li>
<li>
<p><a href="parte-6/2/./6-2-12.html">Redes neurais convolucionais para NLP</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visão-geral-3"><a class="header" href="#visão-geral-3">Visão geral</a></h1>
<p>Redes neurais recorrentes, ou RNNs, são um tipo de rede neurais que usam dados sequenciais ou
dados de séries temporais. Os algoritmos mais conhecidos utilizados estão relacionados a tradução
de linguagens, processamento de linguagem natural (<em>Natural Language Processing</em> (NLP)),
reconhecimento de discursos e legenda de imagens.</p>
<p>Assim como <em>feedforward</em> e redes neurais convolucionais, RNNs utilizam de dados de treino para
aprender, ou seja, é um tipo de aprendizagem supervisionada. A principal diferença de uma RNN é
que ela possui uma espécie de memória que guarda as informações prévias de input que influenciam
nas próximas camadas da rede neural (Figura 57). Além disso, RNNs dependem dos elementos
posteriores dentro da sequência.</p>
<p align="center">
  <img src="parte-6/2/./img/57.png">
</p>
<p align="center">
Figura 57: Representação da comparação estrutural de umas rede neural recorrente e uma rede neural tradicional do
tipo <i>feedforward</i>.
</p>
<p>Para representação de uma RNN podemos utilizar dois métodos: <em>rolled</em> e <em>unrolled</em> mostradas na
Figura 58.</p>
<p align="center">
  <img src="parte-6/2/./img/58.png">
</p>
<p align="center">
Figura 58: Duas formas básicas de representação de uma RNN. A forma <i>rolled</i> representa a rede neural inteira,
focando apenas da saída. Na forma <i>unrolled</i> representa as camadas individuais com seus respectivos parâmetros.
</p>
<p>Outra importante diferença em uma RNN em relação a uma rede neural do tipo <em>feedforward</em> é que
no caso das RNNs, seus nodos compartilham dos mesmos parâmetros ao logo de cada camada da
rede, enquanto em uma rede <em>feedforward</em> possui diferentes parâmetros (pesos) em cada nodo.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vetores-de-palavras-word-embeddings"><a class="header" href="#vetores-de-palavras-word-embeddings">Vetores de palavras (<em>word embeddings</em>)</a></h1>
<p>Como foi mencionado, uma das principais utilizações de RNNs é para o processamento de linguagem
natural. Contudo, uma rede neural não consegue distinguir diferenças semânticas entre frases e
impactos de determinadas palavras apenas pelo contexto. Sabemos que uma rede neural trabalha
com valores numéricos e, para isso, devemos codificar essas palavras e frases de forma que o nosso
modelo possa reconhecer e processar essas diferenças entre elas.</p>
<p align="center">
  <img src="parte-6/2/./img/59.png">
</p>
<p align="center">
Figura 59: Representação de um espaço vetorial formado pelas <i>word embeddings</i>. Podemos verificar as similaridades
das palavras dentro de um mesmo círculo e a diferenças entre elas quando não pertencem ao mesmo grupo.
</p>
<p>Dessa forma, palavras podem ser diferenciadas em um espaço vetorial de acordo com o ângulo que
geram entre elas. Por exemplo, palavras com significados opostos tendem a gerar um ângulo maior,
enquanto palavras com significados similares geram um ângulo menor. Podemos verificar isso, na
Figura 60, a seguir.</p>
<p align="center">
  <img src="parte-6/2/./img/60.png">
</p>
<p align="center">
Figura 60: Representação da diferenciação de palavras através de vetores. Percebe-se que palavras com significado
similar, possuem um ângulo menor, como por exemplo, as palavras ’King’ e ’Queen’, enquanto palavras com
significados distintos como ’Woman’ e ’King’ possuem um ângulo maior.
</p>
<p>Nesta seção, serão apresentadas formas de pré-processamento de dados em forma de texto a fim de
alimentar a nossa rede neural. Para convertermos esse texto para valores numéricos, utilizaremos um
métodos chamados <em>one-hot encoding</em> e <em>word vectors</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="one-hot-encoding"><a class="header" href="#one-hot-encoding"><em>One-hot encoding</em></a></h1>
<p><em>One-hot encoding</em> representa cada palavra como um vetor de tamanho \( V \) , onde \( V \) é o número de
palavras únicas no nosso conjunto de dados. Cada palavra é codificada em um vetor binário no qual
o valor 1 é um índice único para cada palavra e o valor 0 para o restante das palavras. Para visualizar
isso melhor, podemos pensar nas duas frases: ”Redes Neurais são difíceis” e ”Redes Neurais são
fáceis”, como estão exemplificadas na Figura 61.</p>
<p align="center">
  <img src="parte-6/2/./img/61.png">
</p>
<p align="center">
Figura 61: Exemplo da codificação <i>one-hot</i> em um vocabulário de cinco palavras, ou seja,
V = 5 → [’Redes’, ’Neurais’, ’são’, ’difíceis’, ’fáceis’] .
</p>
<p>Essa codificação mapeia cada palavra a um vetor único em que cada posição representa uma palavra
distinta do vocabulário. Esse método converte qualquer palavra em valores numéricos de uma maneira
muito simples.</p>
<p>Todavia, com essa codificação temos dois problemas principais: uso de memória inutilizada e perda
semântica da frase. Primeiramente, teremos um vetor codificado para cada uma das palavras que
aumenta o tamanho de acordo com a complexidade do vocabulário, então teremos um vetor enorme,
composto basicamente por zeros, para representar uma só palavra o que pode levar a um uso excessivo
e desnecessário de memória. Além disso, esse tipo de codificação não guarda o significado semântico
de cada palavra. Para isso, gostaríamos de representar um vetor que representasse o valor semântico
de cada palavra e similaridade entre elas, por exemplo, as palavras ”fáceis” e ”difíceis”, gostaríamos
que fossem dois vetores completamente distintos, enquanto ”errado” e ”incorreto” gostaríamos que
fossem vetores semelhantes.</p>
<p>Para isso, utilizamos de outro método de codificação chamado <em>word2Vec</em> que resolve os dois
problemas citados acima.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word2vec"><a class="header" href="#word2vec">Word2Vec</a></h1>
<p>Em word2Vec, existem dois tipos de arquiteturas: <em>Continuous Bag Of Words</em> (CBOW) e <em>Skip Gram</em>.
Primeiro iremos discutir a arquitetura Skip Gram para depois discutirmos CBOW.</p>
<p>Iremos fazer com que os dados nos digam quais palavras estão ocorrendo próximo de uma outra
palavra. Usamos um método chamado ”janela de contexto” para nos dizer isso.</p>
<p>Se utilizarmos como exemplo a seguinte frase ”Deep Learning is very hard and fun”. Primeiro devemos
definir o tamanho da janela (<em>window size</em>), que pode ser, por exemplo igual a 2. Precisamos iterar
sobre todas as palavras presentes nos nossos dados - nesse caso é apenas uma frase. Como o tamanho
da janela é dois iremos considerar duas palavras antes e duas palavras depois da palavra que estamos
analisando, como percebe-se na Figura 62. E iremos repetir isso até que todas as palavras sejam
coletadas em forma de pares.</p>
<p align="center">
  <img src="parte-6/2/./img/62.png">
</p>
<p align="center">
Figura 62: Representação da formação de uma ”janela de contexto”.
</p>
<p>Com isso feito, podemos formar pares entre as palavras dos dados verificados de forma que a palavra
que estamos verificando esteja relacionada com uma palavra que pertence ao contexto dela, ou seja,
buscaremos pares ordenados do tipo (target word, context word). Para a frase do exemplo teremos
os seguintes pares.</p>
<ol>
<li>
<p>(Deep, Learning), (Deep, is)</p>
</li>
<li>
<p>(Learning, Deep), (Learning, is), (Learning, very)</p>
</li>
<li>
<p>(is, Deep), (is, Learning), (is, very), (is, hard)</p>
</li>
<li>
<p>(very, learning), (very, is), (very, hard), (very, and)</p>
</li>
<li>
<p>(hard, is), (hard, very), (hard, and), (hard, fun)</p>
</li>
<li>
<p>(and, very), (and, hard), (and, fun)</p>
</li>
<li>
<p>(fun, hard), (fun, and)</p>
</li>
</ol>
<p>Esses dados podem ser considerados os nossos dados de treino para <em>word2Vec</em>.</p>
<p>O modelo <em>Skip Gram</em> tenta prever o contexto de cada palavra dada uma palavra que irá ser focada.
Usamos uma rede neural para a previsão dessa tarefa. A entrada da rede neural será a versão
codificada <em>one-hot</em> das palavras, onde o tamanho \( V \) do vetor será o tamanho do vocabulário. A
arquitetura da rede está exemplificada na Figura 63, abaixo.</p>
<p align="center">
  <img src="parte-6/2/./img/63.png">
</p>
<p align="center">
Figura 63: Representação de uma rede neural para a codificação Skip Gram a partir da entrada (Deep, Learning),
onde ”Deep” é a <i>target word</i> e ”Learning” é a <i>context word</i>. A partir da target word, treinamos a rede
neural para prever a <i>context word</i>, sendo que a saída e a entrada da rede estão na forma <i>one-hot encoding</i>.
</p>
<p>Para CBOW, a única diferença é que nós tentamos prever a <em>target word</em> dada a <em>context word</em>,
então, basicamente, invertemos o modelo <em>Skip Gram</em> para gerarmos o modelo CBOW, como está
representado na Figura 64.</p>
<p align="center">
  <img src="parte-6/2/./img/64.png">
</p>
<p align="center">
Figura 64: Representação de uma rede neural para a codificação CBOW a partir da entrada (Deep, Learning), onde
”Deep” é a <i>target word</i> e ”Learning” é a <i>context word</i>. A partir da <i>context word</i>,
treinamos a rede neural para prever a <i>target word</i>, sendo que a saída e a entrada da rede estão
na forma <i>one-hot encoding</i>.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arquitetura-de-uma-rnn"><a class="header" href="#arquitetura-de-uma-rnn">Arquitetura de uma RNN</a></h1>
<p>Como foi explicado na Seção <a href="parte-6/2/./6-2-1.html">Visão geral</a>, uma RNN é uma classe de redes neurais que permite que as
saídas anteriores sejam usadas como entradas, embora tenham estados ocultos. Com isso podemos
determinar uma arquitetura básica desse tipo de rede neural e definir os seus parâmetros e suas
funções de ativação que podem ser, potencialmente, utilizadas.</p>
<p align="center">
  <img src="parte-6/2/./img/65.png">
</p>
<p>Na Figura 65 está representada uma estrutura básica de uma RNN. Nela, para cada pedaço de tempo
\( t \), a ativação \( a ^{&lt;t&gt;} \) e a saída \( y ^{&lt;y&gt;} \) são expressos da seguinte forma:</p>
<hr />
<p>\[
\large{} a ^{&lt;t&gt;} = g _1 (W _{aa} a ^{&lt;t-1&gt;} + W _{ax} x ^{&lt;t&gt;} + b _a)
\]</p>
<hr />
<p>\[
\large{} y ^{&lt;t&gt;} = g _2 (W _{ya} a ^{&lt;t&gt;} + b _y)
\]</p>
<hr />
<p>Onde \( W _{ax}, \ W _{aa}, \ W _{ya}, \ b _a, \ b_y \) são coeficientes que são temporariamente compartilhados
e \( g _1 \) e \( g _2 \) são as funções de ativação.</p>
<p>Os prós e contras de uma RNN estão descritos abaixo:</p>
<p align="center">
Tabela 5: Comparação entre os prós e contras de uma RNN
</p>
<table><thead><tr><th>Prós</th><th>Contras</th></tr></thead><tbody>
<tr><td>Possibilidade de processamento de entrada de qualquer tamanho</td><td>Computação lenta</td></tr>
<tr><td>O tamanho do modelo não aumenta de acordo com o tamanho da entrada</td><td>Dificuldade de acessar informações de tempo muito distante</td></tr>
<tr><td>A computação leva em consideração informações históricas</td><td>Não consegue levar em consideração nenhuma entrada futura para o atual estado</td></tr>
<tr><td>Os pesos são compartilhados ao longo do tempo</td><td></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aplicações-1"><a class="header" href="#aplicações-1">Aplicações</a></h1>
<p>Os modelos de RNN são muito utilizados, como foi mencionado, em áreas como processamento de
linguagem natural, reconhecimento de discurso e geração de música. As diferentes aplicações serão
representadas a seguir com suas respectivas arquiteturas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="one-to-one"><a class="header" href="#one-to-one"><em>One-to-one</em></a></h1>
<p>O tipo de RNN <em>one-to-one</em> é uma rede neural muito simples, na qual \( T _x = T _y = 1 \). Esse tipo de
RNN é muito utilizada em redes neurais tradicionais vistas anteriormente.</p>
<p align="center">
  <img src="parte-6/2/./img/66.png">
</p>
<p align="center">
Figura 66: Representação de uma RNN do tipo <i>one-to-one</i>.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="one-to-many"><a class="header" href="#one-to-many"><em>One-to-many</em></a></h1>
<p>O tipo de RNN <em>one-to-many</em> é uma rede neural, na qual \( T _x = 1 , \ T _y &gt; 1 \). Esse tipo de RNN é muito
utilizada em geração de música.</p>
<p align="center">
  <img src="parte-6/2/./img/67.png">
</p>
<p align="center">
Figura 67: Representação de uma RNN do tipo <i>one-to-many</i>.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="many-to-one"><a class="header" href="#many-to-one"><em>Many-to-one</em></a></h1>
<p>O tipo de RNN <em>many-to-one</em> é uma rede neural, na qual \( T _x &gt; 1 , \ T _y = 1 \). Esse tipo de RNN é muito
utilizada em classificação de sentimento.</p>
<p align="center">
  <img src="parte-6/2/./img/68.png">
</p>
<p align="center">
Figura 68: Representação de uma RNN do tipo <i>many-to-one</i>.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="many-to-many"><a class="header" href="#many-to-many"><em>Many-to-many</em></a></h1>
<p>O tipo de RNN <em>many-to-many</em> é uma rede neural, na qual \( T _x = T _y \). Esse tipo de RNN é muito
utilizada em nome de reconhecimento de entidade.</p>
<p align="center">
  <img src="parte-6/2/./img/69.png">
</p>
<p align="center">
Figura 69: Representação de uma RNN do tipo <i>many-to-many</i> com saídas e entradas em números iguais.
</p>
<p>A outra forma de representação desse tipo de RNN é na forma \( T _x \neq T _y \), também chamada de
<em>sequence-to-sequence</em>, que é realiza a codificação da entrada usando a arquitetura <em>many-to-one</em> e a
produção da saída realizada por uma única entrada do tipo one-to-many, como se pode perceber na
Figura 70. Esse tipo de RNN é muito utilizada em tradução de máquina.</p>
<p align="center">
  <img src="parte-6/2/./img/70.png">
</p>
<p align="center">
Figura 70: Representação de uma RNN do tipo <i>many-to-many</i> com saídas e entradas em números diferentes.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="função-custo-cost-function-3"><a class="header" href="#função-custo-cost-function-3">Função custo (<em>Cost function</em>)</a></h1>
<p>Como foi visto nas seções anteriores, a função custo de uma rede neural tem como objetivo calcular
o erro entre os valores de entrada a cada passo de tempo. A função custo está representada abaixo.</p>
<p>\[
\large{} \mathcal{L} _{\theta} (y, ŷ) = - \sum _{t+1} ^{T _y}
\mathcal{L} _{\theta} (y ^{&lt; t &gt;} , ŷ ^{&lt; t &gt;})
\]</p>
<p>onde \( \mathcal{L} _{\theta} (y, ŷ) _t \) é a função custo definida. Geralmente essa função
custo é a <em>cross entropy loss</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backpropagation"><a class="header" href="#backpropagation"><em>Backpropagation</em></a></h1>
<p>O algoritmo de <em>backpropagation</em> é feito a cada ponto em um período de tempo. No tempo \( T \),
da derivada da função custo \( \mathcal{L} \) a respeito de uma matriz de pesos \( W \) é expressa como segue:</p>
<p>\[
\large{} \frac{\partial \mathcal{L} ^{(T)}}{\partial W} =
\sum _{t=1} ^{T} \frac{\partial \mathcal{L} ^{(T)}}{\partial W} \Bigg| _{(t)}
\]</p>
<p>Para realizar essa computação, realizamos as atualizações dos parâmetros <em>feedforward</em>, a cada passo
de tempo, e ao computar a função custo, realizamos a <em>backpropagation</em>. Contudo, como esse tipo
de rede neural processa os dados sequencialmente de forma a computar cada derivada para cada
período de tempo. Esse algoritmo é chamado de <em>backpropagation through time</em> <a href="parte-6/2/../../referencias.html">[22]</a>.</p>
<p>Realizar o cálculo das derivadas de forma sequencial pode ser uma tarefa pouco eficiente. A computação
das derivadas parciais são realizadas de forma <em>end-to-end</em>, o que, muitas vezes pode causar
diversos problemas como desaparecimento e explosão do gradiente que serão explicados em detalhes
na Seção <a href="parte-6/2/./6-2-8.html">Gradiente de desaparecimento e explosão</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="funções-de-ativação-e-propriedades"><a class="header" href="#funções-de-ativação-e-propriedades">Funções de ativação e propriedades</a></h1>
<p>Como vimos nas seções anteriores, para cada transição de camada, precisamos de uma função de
ativação para atualizar os valores dos respectivos nodos da camada seguinte a partir dos nodos da
camada anterior. Para RNNs, temos basicamente três tipos de funções de ativação:</p>
<ol>
<li><em>Sigmoid</em>:</li>
</ol>
<p>\[
\large{} g(z) = \frac{1}{1 + e ^{-z}}
\]</p>
<ol start="2">
<li>tanh:</li>
</ol>
<p>\[
\large{} g(z) = \frac{e ^z - e ^{-z}}{e ^z + e ^{-z}}
\]</p>
<ol start="3">
<li><em>ReLU</em>:</li>
</ol>
<p>\[
\large{} g(z) = max(0, z)
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradiente-de-desaparecimento-e-explosão"><a class="header" href="#gradiente-de-desaparecimento-e-explosão">Gradiente de desaparecimento e explosão</a></h1>
<p>Esse fenômeno ocorre quando o gradiente, no contexto das RNNs, não consegue calcular valores
profundos da rede neural devido às sucessivas multiplicações. Muitas vezes essas multiplicações
podem ser exponenciais, decrementando ou incrementando, os valores dos parâmetros de acordo
com o número de camadas da rede neural.</p>
<p>O principal problema gerado pelo desaparecimento/explosão do gradiente é que a RNN não consegue
guardar as informações vistas em camadas muito anteriores da sequência, pois muitos valores estarão
zerados ou NaNs.</p>
<p>A fim de evitar o problema de explosão, usa-se a técnica de ”recorte do gradiente” (<em>gradient clipping</em>)
na qual limita o valor do gradiente calculado a fim de evitar explosões que eventualmente podem
ocorrer na <em>backpropagation</em>. Este método está representado na Figura 71.</p>
<p align="center">
  <img src="parte-6/2/./img/71.png">
</p>
<p align="center">
Figura 71: Representação da técnica de ”recorte do gradiente”. No eixo <i>y</i> está representado o valor do gradiente após
a realização do corte de acordo com o crescimento do valor do gradiente, representado no eixo <i>x</i>.
</p>
<p>Outra metodologia abordada para evitar o problema de dificuldade de memorização foi a criação de
portões de memorização, utilizados em arquiteturas que serão vistas na seção a seguir, como GRUs
e LSTMs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="long-short-term-memory-lstm"><a class="header" href="#long-short-term-memory-lstm"><em>Long Short-Term Memory</em> (LSTM)</a></h1>
<p>As arquiteturas de LSTMs são utilizadas para resolver os problemas de dificuldade de memorização
de longas sequências causadas pelo desaparecimento do gradiente.</p>
<p>A LSTM é uma RNN que memoriza os valores em intervalos arbitrários, sendo, assim, muito adequada
para processar e prever séries de valores temporais com intervalos de tempo de duração desconhecida.
Como pode-se verificar na Figura 72, a LSTM possui uma estrutura de cadeia que contém quatro
redes neurais e diferentes blocos de memória chamados ”células”.</p>
<p>Para cada período de tempo, a LSTM retorna dois valores: \( h _t \) e \( c _t \) que são, respectivamente, os
valores da hipótese de cada período e os valores da célula de cada período. Ambos valores possuem
o mesmo tamanho \( n \). A célula guarda as informações de longo prazo da rede neural, enquanto a
hipótese é o valor gerado de determinado período de tempo.</p>
<p align="center">
  <img src="parte-6/2/./img/72.png">
</p>
<p align="center">
Figura 72: Representação de uma arquitetura LSTM. Percebe-se que existem três <i>gates</i>
(<i>input</i>, <i>forget</i> e <i>output</i>) e uma célula, onde são realizadas as operações de memorização.
</p>
<p>Com isso, uma LSTM consegue ler, apagar e escrever informações da célula. Essas informações são
controladas através dos portões (<em>gates</em>) que, para cada período de tempo, eles podem estar fechados
(0) ou abertos (1). Esses portões são dinâmicos, ou seja, os seus valores são computados de acordo
com o contexto da sequência passada como entrada. A seguir, estão listados e definidos cada um
dos gates representados na Figura 72 acima.</p>
<ol>
<li><em>Forget Gate</em>: as informações que não são mais úteis no estado da célula são removidas através
do <em>forget gate</em>. Os valores de \( x ^{&lt; t &gt;} \) e \( a ^{&lt; t-1 &gt;} \) são alimentadas no <em>gate</em> e multiplicadas pelas
matrizes \( W \) , resultando em um valor binário após a passagem pela função de ativação. Caso
a saída seja 0, o valor é esquecido e caso seja 1 o valor é mantido para uso futuro.</li>
</ol>
<p>\[
\large{} f _t ^1 = \sigma (W _{f ^1} \cdot [a ^{&lt; t-1 &gt;} x ^{&lt; t &gt;}] + b _{f ^1})
\]</p>
<ol start="2">
<li><em>Input Gate</em>: as informações úteis para o estado da célula é feita pelo <em>input gate</em>. Primeiro,
a informação é regulada através da função sigmoide, filtrando os valores, e, depois um vetor
de valores criados usando a função \( tanh \) retorna valores entre -1 a 1, que contém todos os
valores possíveis para \( x ^{&lt; t &gt;} \) e \( a ^{&lt; t-1 &gt;} \). Em seguida, os valores do vetor e os valores regulados
são multiplicados para obter as informações úteis.</li>
</ol>
<p>\[
\large{} f _t ^1 = \sigma (W _{f ^2} \cdot [a ^{&lt; t-1 &gt;} x ^{&lt; t &gt;}] + b _{f ^1})
\odot tanh (W _{f ^2} \cdot [a ^{&lt; t-1 &gt;} x ^{&lt; t &gt;}] + b _{f ^2})
\]</p>
<ol start="3">
<li><em>Output Gate</em>: as informações úteis da célula atual que podem ser usadas nos estados seguintes
são extraídas através do <em>output gate</em>. Um vetor é, primeiramente, gerado aplicando a função
\( tanh \) na célula e, depois, essa informação é regulada através da função sigmoide filtrando os
valores a serem lembrados. Os valores do vetor e dos valores regulados são multiplicados e são
enviados como saída para a célula seguinte.</li>
</ol>
<p>\[
\large{} h' _t = \sigma (W _{h' _t} \cdot [a ^{&lt; t-1 &gt;} x ^{&lt; t &gt;}] + b _{h' _t}) \odot tanh (c _t)
\]</p>
<p>No entanto, LSTMs possuem um problema similar aos modelos de RNNs. Quando as sentenças são
muito grandes, LSTMs não funcionam muito bem. Isso acontece porque o valor de uma célula muito
anterior à célula atual decresce exponencialmente e isso potencializa a perda de informação e a falta
de controle do processo. Em geral, RNNs e LSTMs possuem três problemas:</p>
<ol>
<li>
<p>A computação sequencial inibe a paralelização. Em outras palavras não podemos paralelizar
tarefas em uma RNN ou em uma LSTM devido ao fato de que seu processamento é sequencial;</p>
</li>
<li>
<p>Sem modelagem explícita de dependências de longo e curto alcance;</p>
</li>
<li>
<p>A distância entre as posições é linear.</p>
</li>
</ol>
<p>Contudo, mesmo com a memorização mais eficiente, LSTMs não resolvem efetivamente o problema
do desaparecimento do gradiente, justamente por, ainda assim, se tratar de uma sequência.
Atualmente, esse problema pode ser resolvido através da utilização de arquiteturas de redes neurais
residuais, ou ResNets <a href="parte-6/2/../../referencias.html">[8]</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bidirectional--multi-layer-rnns"><a class="header" href="#bidirectional--multi-layer-rnns"><em>Bidirectional &amp; Multi-layer</em> RNNs</a></h1>
<p><em>Bidirectional</em> RNNs (BRNNs)são uma arquitetura que utilizam de duas RNNs independentes que
processam a mesma sequência, porém, uma delas processa essa sequência no sentido convencional
(<em>forward RNN</em>) - do início para o fim - e a outra processa no sentido contrário (<em>backward RNN</em>) - do
final para o início. As saídas dessas redes, no final do processamento, em geral, são concatenadas para
cada período de tempo. Na Figura 73 abaixo, está representada uma generalização da arquitetura
de uma BRNN.</p>
<p align="center">
  <img src="parte-6/2/./img/73.png">
</p>
<p align="center">
Figura 73: Representação de uma BRNN. Percebe-se que existem duas RNNs processando, independentemente duas
informações da mesma sequência de entrada em sentidos opostos.
</p>
<p>BRNNs são muito poderosas para classificações de sentimento, pois conseguem compreender o
contexto das frases extremamente bem.</p>
<p><em>Multi-layer</em> RNNs são uma arquitetura que possuem diversas camadas de RNNs empilhadas processando
a mesma informação. Esse tipo de arquitetura é muito poderosa, porém o custo computacional
para o processamento das informações é muito alto. São frequentemente usadas para problemas de
tradução de máquina. A Figura 74 abaixo exemplifica esse tipo de arquitetura.</p>
<p align="center">
  <img src="parte-6/2/./img/74.png">
</p>
<p align="center">
Figura 74: Exemplificação de uma arquitetura <i>Multi-layer</i> RNN.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attention"><a class="header" href="#attention">Attention</a></h1>
<p>Em problemas do tipo <em>seq2seq</em>, quando criamos um modelo de tradução de máquina que tem como
objetivo traduzir uma sentença geramos um problema chamado <em>bottleneck</em> (Figura 75), o qual toda
a informação da sentença que desejamos traduzir fica acumulado na camada da última palavra
da sentença, devido ao processamento sequencial de uma RNN, possibilitando grande perda de
informações passadas e dificultando a tradução.</p>
<p align="center">
  <img src="parte-6/2/./img/75.png">
</p>
<p align="center">
Figura 75: Exemplificação do problema de <i>bottleneck</i>, onde toda a informação da sequência que deseja ser traduzida
acaba ficando acumulada na camada da última palavra da sentença (retângulo laranja). Esse tipo de problema gera
impactos na tradução, principalmente para linguagens que não se relacionam linearmente.
</p>
<p>Para resolver o problema mencionado acima, foram criadas técnicas para prestar atenção a palavras
específicas e influenciar a tradução a cada período de tempo. Por exemplo, quando traduzimos
uma frase, prestamos atenção na palavra que estamos querendo traduzir e validamos a tradução
baseando-se nas palavras anteriores e futuras. Uma RNN pode realizar isso, através de uma técnica
chamada <em>attention</em>.</p>
<p>Em uma RNN, ao invés de codificar toda a sentença um estado interno da rede neural, cada palavra
corresponde a um estado interno que é passado até o estágio de decodificação, como está representado
na Figura 76.</p>
<p align="center">
  <img src="parte-6/2/./img/76.png">
</p>
<p align="center">
Figura 76: Representação de um processo de <i>encoding-decoding</i>. Percebe-se que os valores de \( x _i \) são os valores de
input de cada célula. Esses valores são codificados, gerando os valores \( c _i \), a fim de serem decodificados no final da
operação, gerando \( s _i \). Os valores gerados como saída baseado em cada um dos valores decodificados são os valores
de \( y _i \).
</p>
<p>Contudo, um dos problemas dessa arquitetura é que, como para cada valor codificado a partir dos
valores de entrada, devemos gerar um único vetor \( c \), esse processo pode acarretar na perda de
informações importantes devido ao <em>bottleneck</em>.</p>
<p>Para resolver esse problema, utilizamos o método de <em>Attention</em>. Uma RNN utilizando esse método
está representada na Figura 77.</p>
<p align="center">
  <img src="parte-6/2/./img/77.png">
</p>
<p align="center">
Figura 77: Representação de uma arquitetura de RNN utilizando <i>attention</i>. O módulo à esquerda representa o
mecanismo de <i>attention</i>.
</p>
<p>O modelo com <em>Attention</em> possui uma única camada de encoding, com 4 entradas \( x _i \) e com
4 saídas \( h _i \). O mecanismo de <em>attention</em> está localizado entre as camadas de <em>encoding</em> e <em>decoding</em>.
As entradas dessa camada são os vetores de saída \( h _i \) da camada de <em>encoding</em> e os vetores \( s _i \) dos estados do
<em>decoder</em>. E sua saída é uma sequência de vetores chamados vetores de contexto \( c _i \).</p>
<p>Os vetores de contexto possibilitam com que o <em>decoder</em> foque em determinadas partes da entrada
quando está tentando prever a saída. Para o cálculo desses vetores, realizamos uma soma ponderada
dos valores \( h _i \) gerados pela camada de <em>encoding</em> com os pesos \( a _i \) gerados a partir do cálculo do grau
de relevância da entrada \( x _i \) sobre o output \( y _i \) no tempo \( i \). Em outras palavras:</p>
<p>\[
\large{} c _i = \sum _{j=1} ^i a _{ij} h _j
\]</p>
<p>Os valores de \( a _{ij} \) são aprendidos por uma rede neural densa com uma camada de ativação <em>softmax</em>.</p>
<p>Para exemplificar o processo de <em>attention</em> e como relaciona cada um dos estados nos processos
de <em>encoding-decoding</em>, podemos pensar na tradução da frase &quot;L’accord sur la zone économique
européenne a été signé en août 1992.&quot;, em francês para o inglês, como está representado na Figura 78.</p>
<p align="center">
  <img src="parte-6/2/./img/78.png">
</p>
<p align="center">
Figura 78: Representação de um processo de tradução da frase "An &lt;unk&gt; dog runs through the grass in front of a
white fence" para o alemão. Os pixels mais amarelados representam o quanto de atenção foi prestado a cada etapa da
tradução.
</p>
<p>Como visto, o método de <em>attention</em> aumentou significantemente a performance técnicas de <em>Neural Machine Translation</em>
(NMT), devido ao fato de possibilitar que o <em>decoder</em> foque apenas em determinadas
partes da sequência. Além disso, resolve o problema <em>bottleneck</em>, pois possibilita que o decoder
olhe diretamente para a sentença que deve ser traduzidas.</p>
<p>Contudo, apesar da alta significância dessa técnica, ainda não conseguimos resolver o problema de
processarmos os dados de entrada em paralelo. Para uma entrada longa, o tempo de processamento
dessa técnica é aumentado significativamente.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redes-neurais-convolucionais-para-nlp"><a class="header" href="#redes-neurais-convolucionais-para-nlp">Redes neurais convolucionais para NLP</a></h1>
<p>Como foi visto na Seção <a href="parte-6/2/../1/6-1.html">Redes neurais convolucionais (<em>Convolutional neural networks</em>)</a>,
CNNs conseguem paralelizar atividades devido às camadas de convolução
que atuam como cópias de um mesmo neurônio trabalhando em paralelo. CNNs
(<a href="parte-6/2/../1/6-1.html">Redes neurais convolucionais (<em>Convolutional neural networks</em>)</a>) podem
ajudar a resolver esse problema.</p>
<p>No caso das CNNs, quando utilizadas para realização de problemas que envolvem sequências de
palavras, por exemplo, as entradas são palavras representadas por vetores de tamanho \( k \), e logo
após, um filtro de convolução é aplicado a fim de gerar novos valores a essas palavras. Para cada um
dos vetores gerados, escolhe-se o maior valor de cada vetor, a fim de aplicar o <em>pooling</em>. Na última
camada, aplica-se a função de ativação nos resultados (ver Figura 79).</p>
<p align="center">
  <img src="parte-6/2/./img/79.png">
</p>
<p align="center">
Figura 79: Representação de uma CNN voltada para a classificação de sentimento de uma sentença.
</p>
<p>A razão pela qual uma CNN pode funcionar em paralelo é que cada palavra de entrada pode ser
processada ao mesmo tempo e não depende necessariamente da palavra anterior já estar traduzida.
Além disso, a distância entre o <em>output</em> e cada <em>input</em> de uma CNN está na ordem de \( log(N) \), o que
é muito melhor que uma RNN cuja distância está na ordem linear \( N \).</p>
<p>Com isso, seria interessante se pudéssemos realizar tarefas de forma paralela como as CNNs realizam
juntamente com a efetividade da compreensão semântica do mecanismo de <em>attention</em>. Com base
nisso, uma nova arquitetura foi desenvolvida chamada <em>transformers</em> que será detalhada na seção
seguinte.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers"><a class="header" href="#transformers"><em>Transformers</em></a></h1>
<p>O artigo publicado em 2017 chamado &quot;Attention is all you need&quot;<a href="parte-6/3/../../referencias.html">[21]</a> mudou a forma de como o
mecanismo de attention era visto. <em>Transformers</em> começaram a ser aplicados em tarefas de NLP e
agora estão sendo utilizados em diversas outras áreas de pesquisa como CV.</p>
<p>Como visto nas seções anteriores, arquiteturas de RNNs, como por exemplo LSTMs, processam os
dados sequencialmente através das suas diversas <em>hidden layers</em>, empilhando as informações. No caso
dos <em>transformers</em>, as informações são processadas paralelamente, o que acarreta na perda de ordem
das sequências.</p>
<p>Um transformer possui uma arquitetura similar aos modelos previamente vistos. Ele consiste num
conjunto de encoders e decoders, como podemos visualizar na Figura 80, a seguir.</p>
<p align="center">
  <img src="parte-6/3/./img/80.png">
</p>
<p align="center">
Figura 80: Representação de uma estrutura básica de um <i>transformer</i>. Percebe-se que a partir de uma dada entrada,
as sequências
</p>
<p>A revolução dos <em>transformers</em> começou com a ideia de mudar a representação da entrada dos dados a
fim de alimentar toda a camada de <em>input</em> de uma vez só através de um conjunto em forma de <em>tokens</em>,
exemplificado na Figura 81. Esse conjunto de entrada pode ser escrito como \( X = x _1 , x _2 , \dots , x _N \),
onde \( x \in \mathbb{R} ^{N \times d _{in}} \) onde os elementos da sequência \( x _i \) referem-se aos <em>tokens</em>.</p>
<p align="center">
  <img src="parte-6/3/./img/81.png">
</p>
<p align="center">
Figura 81: Exemplificação do processo de tokenização. Percebe-se que dado um conjunto fixo de uma sequência de
palavras, dividimos cada uma dessas palavras em <i>tokens</i> a fim de serem processamos como um conjunto.
</p>
<p>Após a tokenização, projetamos essas palavras em espaço geométrico distribuído, ou seja, como visto
na Seção <a href="parte-6/3/../2/6-2-2.html">Vetores de palavras</a>, <em>word embeddings</em>, capturando o sentido semântico das palavras.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="positional-encoding"><a class="header" href="#positional-encoding"><em>Positional encoding</em></a></h1>
<p>Quando processamos as palavras em forma de tokens, o nosso modelo perde a noção de ordem das
palavras devido ao fato de processar esse conjunto de uma vez só. Com isso, antes de processarmos
as informações das sequências na camada de <em>self-attention</em>, precisamos fazer com que o modelo passe
a compreender ordem novamente.</p>
<p>Por exemplo, se duas palavras iguais aparecem em lugares diferentes na mesma sentença, provavelmente
o sentido semântico delas é diferente. Para esse tipo de problema, o método de <em>positional encoding</em>
é muito eficiente, pois irá tratar palavras iguais de maneiras diferentes dependendo da ordem em que elas aparecem.</p>
<p>Na Figura 82 abaixo está representada a camada de positional encoding.</p>
<p align="center">
  <img src="parte-6/3/./img/82.png">
</p>
<p align="center">
Figura 82: Representação da camada de <i>positional encoding</i>. Percebe-se que, dada uma sequência, ela é processada
através de tokenização e <i>embedding</i> e enviada para uma etapa onde acontece a soma entre uma função sinusoidal e o
resultado da camada anterior, gerando uma nova representação semântica para cada uma das palavras.
</p>
<p>A função sinusoidal é utilizada para capturar informações relativas às posições das palavras e, com
isso, diferenciar semanticamente palavras iguais que possuem sentidos semânticos diferentes. Abaixo,
está descrita a equação de <em>positional encoding</em> e será discutida a seguir.</p>
<p>\[
\large{} PE(pos, 2i) = \sin \Big( \frac{pos}{10000 ^{\frac{2i}{d}}} \Big)
\]</p>
<p>A equação de <em>positional encoding</em> recebe dois argumentos: \( pos \) e \( i \). O primeiro argumento
\( pos \) representa a posição que a palavra que está sendo prestada atenção ocupa. A variável \( d \) é o tamanho
do vetor de <em>embedding</em> gerado. O segundo argumento \( i \) representa a \( i \)-ésima posição do vetor de
<em>embedding</em>.</p>
<p>\[
\large{} PE(pos, 2i + 1) = \cos \Big( \frac{pos}{10000 ^{\frac{2i}{d}}} \Big)
\]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encoder"><a class="header" href="#encoder"><em>Encoder</em></a></h1>
<ul>
<li>
<p><a href="parte-6/3/./6-3-2-1.html">Self-Attention</a></p>
</li>
<li>
<p><a href="parte-6/3/./6-3-2-2.html">Multi-head attention e geração de saída</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="self-attention"><a class="header" href="#self-attention"><em>Self-Attention</em></a></h1>
<p>Segundo a definição de Ashish Vaswani et al. <a href="parte-6/3/../../referencias.html">[21]</a> do Google Brain, &quot;<em>Self-attention</em>,
às vezes chamado de <em>intra-attention</em>, é um mecanismo de atenção relacionado a diferentes posições de uma única
sequência que tem como objetivo computar uma representação para a sequência&quot;.</p>
<p><em>Self-attention</em> nos possibilita encontra correlações entre diferentes palavras de entrada indicando a
estrutura contextual e sintática da sentença.</p>
<p>Na prática, o <em>transformer</em> possui três representações de <em>attention</em> formadas por matrizes denominadas
<em>Queries</em>, <em>Keys</em> e <em>Values</em> resultantes da camada de <em>embedding</em>. Essa representação é chamada de
<em>Multi-head attention</em> (Figura 83).</p>
<p align="center">
  <img src="parte-6/3/./img/83.png">
</p>
<p align="center">
Figura 83: Representação de <i>multi-head attention</i> em um <i>transformer</i>. Os símbolos Q, K e V representam,
respectivamente as matrizes <i>Queries</i>, <i>Keys</i> e <i>Values</i>.
</p>
<p>O princípio básico da criação dessa camada é o descobrimento de relações semânticas nas sentenças.
Após a camada de <em>embedding</em>, os vetores expressam a relação entre cada uma das palavras e, a
partir desse contexto, a camada de <em>multi-head attention</em> tem como objetivo relacionar a palavra
que estamos buscando (Q) com as palavras que possam estar relacionadas com ela (K) e, por fim,
influenciar a decisão de acordo com a palavra que gostaríamos que fosse retornada (V).</p>
<p>Essas matrizes, inicialmente, são todas iguais quando passadas como entrada para a <em>linear layer</em> (por
isso o nome <em>self-attention</em>). Essa <em>linear layer</em>, tem como objetivo diferenciar cada uma dessas três
matrizes Q, K e V multiplicando cada uma delas por três matrizes de pesos da distintos
\( W _Q , W _K , W _V \), um para cada uma das <em>linear layers</em>.</p>
<p>Para comparar a palavra que estamos buscando Q com os seus possíveis relacionamentos K, reali-
zamos uma operação vetorial que calcula o cosseno do ângulo entre essas palavras. Esse ângulo,
como vimos na <a href="parte-6/3/../2/6-2-2.html">Vetores de palavras (<em>word embeddings</em>)</a> representa o quão relacionadas
semanticamente essas palavras estão. Genericamente, o cálculo do cosseno entre dois vetores se dá por</p>
<p>\[
\large{} \cos (A,B) = \frac{A \cdot B}{|A| |B|}
\]</p>
<p>Com isso em mente, a primeira tarefa que devemos realizar é encontrar os relacionamentos entre
as matrizes Q e K. Para isso realizamos o produto entre essas matrizes (numerador da equação
acima) a fim de gerar um filtro de atenção e dividimos pela raiz quadrada tamanho da sentença
\( \sqrt{d _k} \) (denominador da equação acima). Por fim, os resultados dessa matriz é passado para uma camada
de ativação <em>softmax</em> (Seção <a href="parte-6/3/../../parte-3/2/3-2-1-6.html">Camada de ativação: Hyperbolic tangent</a>) a fim de
gerar valores de probabilidade dos relacionamentos entre as palavras. Essas operações irão gerar um filtro
de atenção que irá prestar atenção em uma parte específica da sentença baseando-se no relacionamento entre
cada uma das palavras.</p>
<p>Para facilitar a compreensão, podemos fazer um paralelo com a atuação de um filtro de atenção
sobre uma imagem. O que acontece, neste caso é que ignoramos todos os pixels da imagem original
que não interferem no resultado que desejamos alcançar. A Figura 84 abaixo, representa a aplicação
do filtro de atenção gerado sobre uma imagem original. O resultado dessa aplicação é uma imagem
em que somente o que realmente interessa está aparecendo.</p>
<p align="center">
  <img src="parte-6/3/./img/84.png">
</p>
<p align="center">
Figura 84: Exemplificação da aplicação de um filtro de atenção sobre os valores dos pixels de uma imagem.
</p>
<p>Nesse sentido, de acordo com a imagem acima, <em>attention filter</em> é o filtro gerado através das multi-
plicações de matrizes, escalonamento e <em>softmax</em> realizadas com as matrizes Q e V e <em>original image</em>
é a entrada do mecanismo de <em>self-attention</em>, ou seja, a matriz V. Portanto, para a gerar a imagem
filtrada, realizamos, basicamente, uma multiplicação entre o filtro de atenção gerado pelas matrizes
Q e K e o conteúdo de entrada V.</p>
<p>Com isso, temos, finalmente, a equação desse mecanismo de <em>self-attention</em>, descrita abaixo.</p>
<hr />
<p>\[
\large{} Attention (Q,K,V) = softmax \Big( \frac{QK ^T}{\sqrt{d _k}} \Big) V
\]</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-head-attention-e-geração-da-saída"><a class="header" href="#multi-head-attention-e-geração-da-saída"><em>Multi-head attention</em> e geração da saída</a></h1>
<p>Na seção anterior, definimos apenas uma das cabeças do mecanismo de <em>multi-head attention</em>. Agora,
precisamos gerar a saída para as outras cabeças.</p>
<p>Essas outras cabeças são necessárias para focar em outros elementos da sentença, ou, no caso do
exemplo, em outros elementos da imagem.</p>
<p align="center">
  <img src="parte-6/3/./img/85.png">
</p>
<p align="center">
Figura 85: Exemplificação do mecanismo de <i>multi-head attention</i> com os filtros aplicados em diferentes na mesma
imagem, porém, focando em diferentes elementos.
</p>
<p>Definimos o mecanismo de <em>multi-head attention</em> da seguinte forma:</p>
<hr />
<p>\[
\large{} MultiHead (Q,K,V) = Concat(head _1 , \dots , head _h) W ^O
\]</p>
<hr />
<p>onde \( W ^O \) são os pesos da <em>linear layer</em> de saída e
\( head _i = Attention (QW _i ^Q , KW _i ^K , VW _i ^V) \).</p>
<p>No final da camada de <em>multi-head attention</em> concatenamos essas três matrizes geradas com o filtro
aplicado. Depois, através de conecções residuais (<em>residual skip connections</em> <a href="parte-6/3/../../referencias.html">[8]</a>),
normalizamos a matriz de saída através do método de <em>layer normalization (LN)</em> <a href="parte-6/3/../../referencias.html">[3]</a>
e geramos o saída através de uma <em>linear layer</em>.
Na Figura 86 a seguir, podemos visualizar a estrutura de um <em>encoder</em> de um <em>transformer</em>.</p>
<p align="center">
  <img src="parte-6/3/./img/86.png">
</p>
<p align="center">
Figura 86: Representação da estrutura de um <i>encoder</i> de um <i>transformer</i>.
</p>
<p>Por fim, para gerar o resultado da saída do modelo, devemos passar esses dados gerados na camada
de <em>encoding</em> para a camada de <em>decoding</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="decoder"><a class="header" href="#decoder"><em>Decoder</em></a></h1>
<p>O decoder consiste nos mesmos componentes do <em>encoder</em> com exceção de dois. Como visto
anteriormente:</p>
<ol>
<li>
<p>A sequência de saída é computada da mesma forma através de <em>embedding</em> e <em>positional encoding</em></p>
</li>
<li>
<p>Esses vetores são passados para o primeiro bloco de <em>decoder</em></p>
</li>
</ol>
<p>Cada bloco de <em>decoder</em> contém as seguintes estruturas:</p>
<ol>
<li>
<p>Uma camada de <em>Mask Multi-head Attention</em></p>
</li>
<li>
<p>Uma camada de normalização juntamente com uma conexão residual</p>
</li>
<li>
<p>Uma nova camada de <em>multi-head attention</em> conhecida como <em>Encoder-Decoder attention</em></p>
</li>
<li>
<p>Uma nova camada de normalização seguida por conexões residuais</p>
</li>
<li>
<p>Uma <em>linear layer</em> com uma terceira conexão residual</p>
</li>
</ol>
<p>Assim, a estrutura do <em>decoder</em> está representada a seguir.</p>
<p align="center">
  <img src="parte-6/3/./img/87.png">
</p>
<p align="center">
Figura 87: Representação da estrutura de um <i>decoder</i> de um <i>transformer</i>.
</p>
<p>Na saída, <em>&quot;Output Probabilities&quot;</em> representa a probabilidade do próximo <em>token</em> da sequência.</p>
<p>Nas seções seguintes, serão detalhadas as subestruturas do <em>decoder</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="masked-multi-head-attention"><a class="header" href="#masked-multi-head-attention"><em>Masked Multi-head attention</em></a></h1>
<p>Como o nosso objetivo, neste caso, é prever a próxima palavra da sequência, a camada de
<em>masked multi-head attention</em> tem como objetivo ”ensinar” o <em>decoder</em> a prever a melhor palavra a ser retornada.
Para isso, atualizamos os valores da matriz de atenção de forma que o modelo preste atenção apenas
nas palavras que foram vistas anteriormente a fim de gerar a melhor que se relacione semanticamente
com elas. Em outras palavras:</p>
<hr />
<p>\[
\large{} MaskedAttention (Q,K,V) = softmax \Big( \frac{QK ^T + M}{\sqrt{d _k}} \Big) V
\]</p>
<hr />
<p>onde a matriz M é uma matriz de zeros e \( - \infty \).</p>
<p>Os zeros irão se tornar os valores do produto \( QK ^T \) e os \( - \infty \) irão se tornar zero.
Esse tipo de máscara evita que o modelo preste atenção nas palavras futuras da sentença, focando nas
palavras anteriores, que é o que realmente interessa.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encoder-decoder-attention"><a class="header" href="#encoder-decoder-attention"><em>Encoder-Decoder attention</em></a></h1>
<p>Nesta etapa, a matriz gerada pela camada de <em>encoding</em> é passada como entrada para um mecanismo
de <em>multi-head attention</em> juntamente com a matriz gerada pelo bloco <em>masked multi-head attention</em>.</p>
<p>A intuição a respeito dessa camada é que essa camada combina a sentença de entrada e saída, em
outras palavras, ela irá ser treinada para associar a sentença de entrada com a sentença de saída
correspondente.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="modelos-generativos-generative-models"><a class="header" href="#modelos-generativos-generative-models">Modelos generativos (Generative models)</a></h1>
<ul>
<li>
<p><a href="parte-6/4/./6-4-1.html">Visão geral</a></p>
</li>
<li>
<p><a href="parte-6/4/./6-4-2.html">Aplicações</a></p>
</li>
<li>
<p><a href="parte-6/4/./6-4-3.html">Auto-Regressive Models</a></p>
<ul>
<li>
<p><a href="parte-6/4/./6-4-3-1.html">PixelRNN</a></p>
</li>
<li>
<p><a href="parte-6/4/./6-4-3-2.html">PixelCNN</a></p>
</li>
</ul>
</li>
<li>
<p><a href="parte-6/4/./6-4-4.html">Variational Autoencoders (VAE)</a></p>
</li>
<li>
<p><a href="parte-6/4/./6-4-5.html">Generative Adversarial Networks (GANs)</a></p>
<ul>
<li><a href="parte-6/4/./6-4-5-1.html">Treinamento: Jogo de dois jogadores</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visão-geral-4"><a class="header" href="#visão-geral-4">Visão geral</a></h1>
<p>Tanto o mundo real quanto o mundo digital é composto por uma quantidade enorme de informações
e tudo isso está disponível de forma acessível. Todavia, é um problema desenvolver modelos e
algoritmos que possam analisar e entender essa quantidade absurda de dados.</p>
<p>Nesse sentido, os modelos generativos são uma das abordagens mais promissoras no subconjunto
de <em>unsupervised learning</em>. Para treinar esse tipo de modelo, coletamos uma grande quantidade de
dados de algum domínio (e.g. imagens, frases, sons, etc.) que chamaremos de \( p _{data}(x) \) e, então,
treinamos um modelo para gerar dados semelhantes aos de entrada, ou seja, geraremos \( p _{model}(x) \)
tal que é semelhante a \( p _{data}(x) \).</p>
<p>A principal ideia é que as redes neurais que usamos como modelos generativos têm um número de
parâmetros significativamente menor do que a quantidade de dados em que os treinamos, de modo
que os modelos são forçados a descobrir e internalizar com eficiência a essência dos dados para
gerá-los.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aplicações-2"><a class="header" href="#aplicações-2">Aplicações</a></h1>
<p>Com modelos generativos podemos criar amostras extremamente realistas como por exemplo modelos
de arte, imagens, tarefas como <em>super-resolution</em> e colorização.</p>
<p align="center">
  <img src="parte-6/4/./img/88.png">
</p>
<p align="center">
Figura 88: Exemplos de modelos generativos. Na primeira imagem estão representados modelos de arte gerados a
partir de ambientes. Na segunda imagem estão representadas imagens em alta resolução geradas a partir de
identificação de rostos. Na terceira imagem estão representados exemplos de colorização de imagens a partir de uma
”croqui” e imagens de treino.
</p>
<p>Além disso, modelos generativos podem ser aplicados em outras áreas de <em>unsupervised learning</em>, como
por exemplo simulações e planejamentos usando <em>reinforcement learning</em>.</p>
<p>Com isso, temos três formas de modelarmos esse tipo de distribuição de modelos generativos: através
de <em>Auto-regressive Models</em>, <em>Auto-encoders</em> e GANs. Iremos discutir, separadamente, nas seções
seguintes modelos como PixelRNN, PixelCNN, <em>Variational Autoencoder</em> e GANs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="auto-regressive-models"><a class="header" href="#auto-regressive-models">Auto Regressive Models</a></h1>
<p>Antes mesmo de definirmos <em>Generative Adversarial Networks</em> (GANs) é importante mencionar que a
principal diferença entre uma GAN e <em>Auto-regressive models</em> é que uma GAN aprende uma distribui-
ção implícita dos dados, enquanto o último aprende uma distribuição explícita governada por uma
estrutura de modelo imposta.</p>
<p>As principais vantagens da utilização de <em>Auto-regressive models</em> estão listadas a seguir:</p>
<ol>
<li>
<p><strong>Fornece uma maneira de calcular a probabilidade:</strong> esses modelos tem a vantagem de
retornar probabilidades explícitas das densidades, tornando-o simples de aplicar em domínios
como compressão e planejamento e explorações baseadas em probabilidade.</p>
</li>
<li>
<p><strong>O treino é estável:</strong> existe um algoritmo estável que treina um <em>Auto-regressive models</em></p>
</li>
<li>
<p><strong>Funciona tanto para dados discretos quanto contínuos</strong></p>
</li>
</ol>
<p>Um dos problemas mais conhecidos de <em>unsupervised learning</em> é o de modelar a distribuição de imagens
naturais. Para resolver esse problema, precisamos de um modelo tratável e escalonável. PixelRNN e
PixelCNN fazem parte da classe de <em>Auto-regressive models</em> que atendem essas condições.</p>
<p>Esses tipos de modelos são, preferencialmente usados no preenchimento de imagens. A razão para
isso é porque eles têm um desempenhos melhor do que outros modelos generativos nesse tipo de
problema.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pixelrnn"><a class="header" href="#pixelrnn">PixelRNN</a></h1>
<p>Através da utilização de modelos probabilísticos de densidade, como por exemplo Distribuição Normal,
consegue gerar imagens começando através de um canto e calculando qual é o valor do próximo pixel
que mais faz sentido de acordo com os pixels previamente gerados e com um valor de probabilidade.</p>
<p>Para processar a relação entre geração do pixel e valor de probabilidade, utilizamos de modelos que
funcionam bem com sequências, como por exemplo, Redes Neurais recorrentes (RNNs).</p>
<p align="center">
  <img src="parte-6/4/./img/89.png">
</p>
<p align="center">
Figura 89: Geração de pixels utilizando PixelRNN. A partir de um pixel do canto da imagem, o modelo começa a
gerar, sequencialmente outros pixels baseados nos valores de probabilidade e nos valores dos pixels previamente
gerados.
</p>
<p>A rede neural &quot;varre&quot; a imagem gerando, linha a linha e pixel a pixel a cada período de tempo, o que
pode ser, muitas vezes, muito demorado. Os valores dos pixels são gerados baseando-se nos valores
de probabilidade gerados através de uma distribuição que é escrita a partir do produto condicional
das distribuições e os valores gerados são compartilhados através da imagem.</p>
<p>O objetivo, então é calcular a probabilidade \( p(x) \) para cada pixel da imagem de tamanho \( nxn \).
Assim, a probabilidade pode ser escrita da seguinte forma:</p>
<p>\[
\large{} p(x) = \prod _{i=1} ^{n ^2} p(x _i|x _1, \dots , x _{i-1})
\]</p>
<p>A equação acima é a probabilidade do \( i \)-ésimo pixel dada a probabilidade de todos os pixels pre-
viamente gerados. Essa geração se dá linha a linha e pixel por pixel. Além disso, cada pixel \( x _i \) é
juntamente determinado por todos os três canais de cores RGB. Assim, a probabilidade condicional
do \( i \)-ésimo pixel se torna:</p>
<p>\[
\large{} p(x _{i,R}|X _{&lt;i}) p (x _{i,G}|X _{&lt;i}, x _{i,R})
p (x _{i,B}|X _{&lt;i}, x _{i,R}, x _{i,G})
\]</p>
<p>Portanto, cada cor é condicionada sobre as outras cores e os pixels previamente gerados.</p>
<p>Contudo, um dos principais problemas desse tipo de implementação é a velocidade. Como geramos
cada pixel sequencialmente, isso pode ser extremamente lento de acordo com a complexidade da
saída. Para resolver esse problema de otimização, podemos utilizar um método similar e paralelizável
utilizando ConvNets, chamado PixelCNN.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pixelcnn"><a class="header" href="#pixelcnn">PixelCNN</a></h1>
<p>Como vimos, a utilização do método PixelRNN, pode, muitas vezes, ser mais lento que o desejável,
para isso, podemos adicionar camadas de convolução no nosso modelo. PixelCNN usa esse tipo
de camada com o objetivo de paralelizar as operações de geração de pixel, preservando a resolução
espacial da imagem gerada.</p>
<p>Da mesma forma que PixelRNN, PixelCNN gera a imagem a partir de um canto da imagem, porém,
agora, utilizando de uma ConvNet para paralelizar esse processo.</p>
<p align="center">
  <img src="parte-6/4/./img/90.png">
</p>
<p align="center">
Figura 90: Geração de uma imagem através do método PixelCNN. Podemos perceber que os valores dos pixels são
gerados sequencialmente através de uma camada de convolução. Os pixels previamente gerados estão representados
em cinza e os valores de probabilidade são calculados paralelamente através da convolução.
</p>
<p>Como a geração dos pixels continua sequencial, isso pode ser um ponto negativo que, muitas vezes,
pode tornar a geração da imagem, ainda muito lenta. Para isso, nas próximas seções iremos discutir
métodos mais eficientes para realizar esse procedimento.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="variational-autoencoders-vae"><a class="header" href="#variational-autoencoders-vae"><em>Variational Autoencoders</em> (VAE)</a></h1>
<p>Como vimos até agora, PixelCNNs definem a função de densidade travável e otimiza a probabilidade
de dados de treinamento. De outra forma, VAEs definem funções de probabilidade intratáveis de
acordo com um valor \( z \) que é chamado de espaço latente e será definido posteriormente.</p>
<p>Dessa forma, um VAE é um <em>Autoencoder</em> cuja distribuição de codificações é regularizada durante
o treinamento, a fim de garantir que seu espaço latente tenha boas propriedade, o que nos permite
gerar novos dados</p>
<p>Em geral, <em>Autoencoders</em> são redes neurais para redução de dimensionalidade. A estrutura básica de
um <em>Autoencoder</em> está representada na Figura 91 Essas redes neurais tem como objetivo aprender o
melhor esquema de codificação-decodificação usando um processo de otimização iterativo.</p>
<p align="center">
  <img src="parte-6/4/./img/91.png">
</p>
<p align="center">
Figura 91: Através de dados de entrada <b>x</b> treinamos o nosso modelo de forma que possamos reconstruir os dados
originais através do <i>Autoencoder</i>. Dessa forma, <b>z</b> são os dados de entrada codificados através de algoritmos de
redução de dimensionalidade e <b>x̂</b> é o conjunto de dados decodificado a partir de <b>z</b>.
</p>
<p>Assim, a arquitetura geral de um <em>Autoencoder</em> cria um gargalo de dados que garante que apenas a
parte estruturada principal da informação possa ser reconstruída. Os codificadores e decodificadores
são transformações lineares simples que podem ser expressas como matrizes, da mesma forma que o
algoritmo PCA funciona e no final, adicionamos não-linearidade.</p>
<p><em>Autoencoders</em> são, portanto, arquiteturas de codificador-decodificador que podem ser treinadas uti-
lizando um algoritmo de otimização como, por exemplo, gradiente descendente.</p>
<p>Finalmente, podemos definir um VAE. VAE é uma arquitetura composta por um codificador e um
decodificador treinada para minimizar o erro de reconstrução entre os dados decodificados e os dados
iniciais. Assim, podemos treinar o modelo da seguinte forma:</p>
<ol>
<li>
<p>A entrada é codificada como distribuição do espaço latente \( z \)</p>
</li>
<li>
<p>Um ponto do espaço latente é amostrado a partir dessa distribuição</p>
</li>
<li>
<p>O ponto amostrado é decodificado e o erro de reconstrução pode ser calculado</p>
</li>
<li>
<p><em>Backpropagation</em> utilizando o erro de reconstrução</p>
</li>
</ol>
<p>Na Figura 92 a seguir, está esquematizado o processo de treino do modelo, sabendo que \( x \) representa
os dados de entrada, \( z \) o espaço latente e \( d(z) \) a reconstrução da entrada.</p>
<p align="center">
  <img src="parte-6/4/./img/92.png">
</p>
<p align="center">
Figura 92
</p>
<p>Com isso, a função custo que é minimizada ao treinar um VAE é composta por um ”termo de
reconstrução” que torna o esquema de codificação-decodificação mais eficiente e um ”termo de re-
gularização” que regulariza a organização do espaço latente, tornando-a mais próxima da Distribuição
Normal Padrão. Esse termo é expresso como a divergência de Kullback-Leibler, que sua definição
não faz parte do escopo deste curso.</p>
<p>A seguir, na Figura 93 está esquematizada uma estrutura básica de um VAE com a definição da
sua função custo, que são dependentes dos valores de entrada \( x \), da distribuição do espaço latente
\( N(\mu _x , \sigma _x ^2) \) e da aproximação gerada a partir do espaço latente \( x̂ = d(z) \).</p>
<p align="center">
  <img src="parte-6/4/./img/93.png">
</p>
<p align="center">
Figura 93
</p>
<p>Portanto, através de VAEs podemos utilizar amostras usadas no espaço latente para gerar dados
similares através do decodificador. A figura abaixo mostra os dados gerados a partir de uma rede
decodificadora de um VAE treinada a partir da base de dados MNIST. Além disso, um dos pontos
negativos da utilização de VAEs é que as amostras geradas são embaçadas e de baixa qualidade.
Portanto para a melhoria da qualidade utilizamos GANs, que serão descritas a seguir.</p>
<p align="center">
  <img src="parte-6/4/./img/94.png">
</p>
<p align="center">
Figura 94: Nesta imagem estão representados os dígitos gerados por uma rede VAE através do decodificador. A
imagem tem esse padrão devido a forma bidimensional da Distribuição Normal. Como se pode perceber, os dígitos
distintos estão em diferentes regiões da imagem devido ao espaço latente.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generative-adversarial-networks-gans"><a class="header" href="#generative-adversarial-networks-gans"><em>Generative Adversarial Networks</em> (GANs)</a></h1>
<p>Até agora, trabalhamos com funções de densidade de probabilidade explícita, como foram vistas em
PixelCNNs e VAEs. No caso das GANs, não trabalhamos com funções explícitas de probabilidade,
usamos aproximações probabilísticas baseadas na Teoria Dos Jogos, aprendendo a gerar dados a
partir da distribuição de treinamento através do jogo de dois jogadores.</p>
<p>Com GANs, o que desejamos é a partir de uma entrada inicializada aleatoriamente, através de
uma rede neural geradora, geramos uma amostra baseada na distribuição de treino, como mostra a
Figura 95.</p>
<p align="center">
  <img src="parte-6/4/./img/95.png">
</p>
<p align="center">
Figura 95
</p>
<p>A seguir iremos descrever o processo de treino de GANs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="treinamento-jogo-de-dois-jogadores"><a class="header" href="#treinamento-jogo-de-dois-jogadores">Treinamento: Jogo de dois jogadores</a></h1>
<p>A forma que iremos treinar e fazer com que o nosso modelo aprenda a geração de dados é através da
visualização desse problema como um jogo de dois jogadores. Existem dois jogadores: a rede neural
geradora e a rede neural discriminadora.</p>
<p>A rede neural geradora (ou <em>Generator network</em>) tenta enganar o discriminador gerando imagens de
aparência real. E a rede neural discriminadora (ou <em>Discriminator network</em>) tenta distinguir imagens
reais e falsas. A representação desse ”jogo” mostrada a seguir na Figura 96.</p>
<p align="center">
  <img src="parte-6/4/./img/96.png">
</p>
<p align="center">
Figura 96: A partir de uma imagem inicializada aleatoriamente z, uma rede neural gerado, representada em verde,
gera imagens falsas, porém muito semelhantes às reais e uma rede neural discriminadora, representada em azul, tenta
distinguir se as imagens dadas como entrada para essa rede neural são falsas ou reais.
</p>
<p>Utilizamos o algoritmo Minimax para realizar o treinamento de uma GAN, cuja função principal está
descrita a seguir.</p>
<p>\[
\large{} \underset{\theta _g}{min} \ \underset{\theta _d}{max}
\Big[ \mathbb{E} _{x \sim p _{data}} \log \
D _{\theta _d}(x) \ +
\mathbb{E} _{z \sim p _z} \log (1 - D _{\theta _d} (G _{\theta _g}(z))) \Big]
\]</p>
<p>onde:</p>
<p>\( D _{\theta _d}(x): \) Saída do discriminador para dados reais \( x \)</p>
<p>\( D _{\theta _d} (G _{\theta _g}(z))): \) Saída do discrminador para dados falsos gerados
\( G(z) \)</p>
<p>Com essa equação, desejamos maximizar o objetivo do discriminador \( \theta _d \) , de forma que \( D(x) \)
é próximo de 1 (imagem real) e \( D(G(z)) \) é próximo de 0 (imagem falsa). E desejamos minimizar o
objetivo do gerador \( \theta _g \) de forma que \( D(G(z)) \) é próximo de 1 (discriminador é enganado a pensar
que \( G(z) \) é real).</p>
<p>Com isso, utilizamos métodos de maximizar e minimizar os discriminadores e geradores, respectiva-
mente. Usamos o método de gradiente ascendente para o discriminador</p>
<p>\[
\large{} \underset{\theta _d}{max} \ \mathbb{E} _{z \sim p _z} \log (1 - D _{\theta _d} (G _{\theta _g}(z)))
\]</p>
<p>Com isso, podemos descrever o seguinte algoritmo para o treinamento de GANs:</p>
<hr />
<p><strong>Algorithm 13</strong> Algoritmo de treino de uma GAN</p>
<hr />
<p>1: <strong>procedure</strong></p>
<p>2:   <strong>for</strong> número de iteração de treino <strong>do</strong></p>
<p>3:    <strong>for</strong> \( i=1 \) <strong>to</strong> \( k \) <strong>do</strong></p>
<p>4:     Crie amostras <em>minibatch</em> de \( m \) amostras inicializadas aleatoriamente
\( \{z ^{(1)}, \dots , z ^{(m)} \} \) a partir de \( p _g (z) \)</p>
<p>5:     Crie amostras <em>minibatch</em> de \( m \) exemplos \( \{ x ^{(i)}, \dots , x ^{(m)} \} \)
a partir da geração de dados pela distribuição \( p _{data}(x) \)</p>
<p>6:     Atualize o discriminador pelo gradiente ascendente</p>
<p>7:     \( \nabla _{\theta _d} \frac{1}{m} \sum _{i=1} ^m
\Big[ \log D _{\theta _d}(x ^{(i)}) + \log (1 - D _{\theta _d}(G _{\theta _g}(z ^{(i)}))) \Big] \)</p>
<p>8:    <strong>end for</strong></p>
<p>9:   
Crie amostras <em>minibatch</em> de \( m \) amostras inicializadas aleatoriamente \( \{z ^{(1)}, \dots , z ^{(m)} \} \)
a partir de \( p _g (z) \)</p>
<p>10:    Atualize o gerador pelo gradiente ascendente</p>
<p>11:    \( \nabla _{\theta _g} \frac{1}{m} \sum _{i=1} ^{m}
\log (D _{\theta _d} (G _{\theta _g}(z ^{(i)}))) \)</p>
<p>12:   <strong>end for</strong></p>
<p>13: <strong>end procedure</strong></p>
<hr />
<p>Após o treinamento, podemos usar a rede neural geradora para gerar novas imagens. Podemos
perceber na Figura 97 alguns exemplos de imagens geradas por uma GAN.</p>
<p align="center">
  <img src="parte-6/4/./img/97.png">
</p>
<p align="center">
Figura 97: Imagens geradas a partir do treinamento de uma GAN. As imagens contornadas em amarelo são imagens
do conjunto de treino e, portanto as imagens da mesma linha são as vizinhas mais próximas.
</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-q-learning"><a class="header" href="#deep-q-learning"><em>Deep Q-Learning</em></a></h1>
<p>Como foi visto na Seção <a href="parte-6/5/../../parte-4/4/4-4.html">Aprendizado por reforço (<em>Reinforcement learning</em>)</a>,
através do método de <em>Q-learning</em>, atualizamos os valores das políticas do
agente maximizando os valores de Q de acordo com o estado futuro do ambiente e da ação tomada,
de maneira recursiva, usando a equação de Bellman.</p>
<p>Contudo, apesar do grande poder desse método, de acordo com a complexidade do ambiente, ele
pode ser tornar extremamente ineficiente. Para resolver esse problema, usamos um método que
engloba <em>Q-learning</em> e redes neurais profundas, chamado <em>Deep Q-learning</em> (DQN).</p>
<p>De maneira geral, ao invés de trabalharmos com uma tabela para atualizar os valores de Q de decisão
de ação do agente, utilizamos uma rede neural. Essa rede neural funciona através de uma função
de aproximação, chamada de aproximador, denotado por  \( Q(s, a; \theta) \), em que \( \theta \)
representa os pesos treináveis da rede, para aproximar os valores de Q de forma ótima.</p>
<p>Assim, a equação de Bellman, vista na Seção <a href="parte-6/5/../../parte-4/4/4-4-6.html"><em>Q-Learning</em></a> é utilizada como
a função custo, que deve ser minimizada. Em outras palavras, minimizamos a diferença entre a igualdade
do valor Q que deve ser atualizado em relação a soma da recompensa com o desconto do máximo valor de Q
dos estados futuros. Ou seja,</p>
<hr />
<p>\[
\large{} Cost = \Big[ Q(s,;\theta) -
\Big( r(s,a) + \gamma \ \underset{a}{max} \ Q(s',a;\theta) \Big) \Big] ^2
\]</p>
<hr />
<p>onde \( Q(s,;\theta) \) é chamado de <em>Q-target</em>, os parâmetros da rede neural.</p>
<p>O treinamento desse tipo de rede neural acontece durante o momento de <em>exploration</em> e <em>exploitation</em>,
atualizando os valores de Q já vistos e que serão descobertos futuramente. Para o treino, portanto,
selecionamos \( b \) estados já visitados, juntamente com os seus respectivos valores, de maneira aleatória,
e usamos esses valores como <em>input</em> e <em>target</em>, respectivamente.</p>
<p>Diferentemente do método de <em>Q-learning</em> tradicional, DQN prevê os \( N \) possíveis valores de Q para um
determinado estado. Na Figura 98 abaixo, está representada essa diferença entre os dois métodos.</p>
<p align="center">
  <img src="parte-6/5/./img/98.png">
</p>
<p align="center">
Figura 98: Representação dos dois métodos de <i>Q-learning</i>. Na figura do topo, a partir de uma determinada ação
tomada em um estado de ambiente, gera-se um valor de Q referente a este par estado-ação. Na figura de baixo, dado
um estado, a rede neural calcula todos os valores de Q referentes a todas as ações possíveis a serem tomadas,
selecionando o melhor valor para aquele estado.
</p>
<p>Com isso, podemos listar as etapas que envolvem o aprendizado de uma rede neural.</p>
<ol>
<li>
<p>Passar como entrada estado atual \( s \) do ambiente para a rede neural. Essa rede neural irá
retornar os valores Q de todas as possíveis ações a serem tomadas pelo agente no estado \( s \).</p>
</li>
<li>
<p>Selecionar uma ação usando a política \( \epsilon \)<em>-greedy</em>. Com a probabilidade \( \epsilon \),
selecionar uma ação aleatória \( a \) (<em>exploration</em>) e com a probabilidade \( 1 - \epsilon \)
escolher a ação que corresponde o valor máximo de Q, tal que \( a = argmax(Q(s,a;\theta)) \)</p>
</li>
<li>
<p>Realizar a ação tomada \( a \) no estado \( s \) e mover para o próximo estado \( s' \)
para receber a recompensa.</p>
</li>
<li>
<p>Após, tomamos uma decisão aleatória de transição de estado e calculamos o custo</p>
</li>
<li>
<p>Minimizar o erro da função custo utilizando algum método de otimização, como por exemplo,
gradiente descendente</p>
</li>
<li>
<p>A cada \( C \) iterações, copiar os pesos da rede neural atual para uma rede neural <em>target</em> a fim de
salvar os pesos</p>
</li>
<li>
<p>Repetir esses passos \( M \) episódios</p>
</li>
</ol>
<p>Com DQN, temos uma ferramenta extremamente forte para resolução de problemas de aprendizado
de RL. Agora, com ambientes mais complexos e com mais interações, o agente consegue realizar uma
melhor valoração das possibilidades de ações que podem ser tomadas baseadas no retorno da rede
neural. Atualmente, jogos de Atari, Go e StarCraft já atingiram níveis humanos de comportamento
utilizando DQN e outros métodos de IA.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="busca-de-monte-carlo"><a class="header" href="#busca-de-monte-carlo">Busca de Monte Carlo</a></h1>
<p>O método de Busca em Árvore de Monte Carlo (do inglês, Monte Carlo Tree Search (MCTS)) é
extremamente poderoso para diversas aplicações que envolvem IA e teoria dos jogos. Uma das
principais aplicações desse método na literatura foi, em 2016, no artigo publicado pela DeepMind
<a href="parte-6/6/../../referencias">19</a>, no qual atingem níveis humanos no jogo Go utilizando técnicas de DQN, vistas na Seção
<a href="parte-6/6/../5/6-5.html"><em>Deep Q-Learning</em></a> e MCTS.</p>
<p align="center">
  <img src="parte-6/6/./img/99.png">
</p>
<p align="center">
Figura 99: Representação de uma árvore de Monte Carlo gerada a partir de um estado de jogo do jogo Go. Cada
nodo da árvore representa uma possibilidade de jogada baseada no estado de jogo imediatamente anterior.
</p>
<p>O principal objetivo de MCTS é valorar estados intermediários do ambiente de forma que não seja
necessário atingir estados finais para encontrar a melhor possibilidade de recompensa.</p>
<p>Para introduzir esse método, será definido, primeiramente um método <em>uninformed search</em>, que é mais
simples e ineficiente, para depois descrever o método MCTS.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="uninformed-search"><a class="header" href="#uninformed-search">Uninformed Search</a></h1>
<p>É um algoritmo de busca genérica o qual percorre todos os nodos da árvore a fim de valorar os
estados intermediários.</p>
<p>Para realizar essa valoração, temos buscas em profundidade e em largura, como pode ser visto na
Figura 100 abaixo.</p>
<p align="center">
  <img src="parte-6/6/./img/100.png">
</p>
<p align="center">
Figura 100: Representação das buscas em profundidade e largura. Percebe-se que todos os nodos da árvore são
acessados durante a busca.
</p>
<p>Percebe-se que uma árvore com \( b \) ramos por nível e profundidade \( d \) teria \( b ^d \) número de nós folha.
Por exemplo, no jogo Go, cujo número de ramos por nível é 250, no nível 5 da árvore, teríamos
\( 250 ^5 = 976.562.500.000 \) possibilidades de estados.</p>
<p>O maior problema desse tipo de busca é que, quanto maior a complexidade do problema, maior a
profundidade e largura da árvore, o que torna o método extremamente ineficiente.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monte-carlo-tree-search"><a class="header" href="#monte-carlo-tree-search">Monte Carlo <em>Tree Search</em></a></h1>
<p>MCTS é um método de busca baseado no método de <em>uninformed search</em>, porém, mais inteligente.
MCTS usa simulação de Monte Carlo <a href="parte-6/6/../../referencias.html">[17]</a> para acumular estimativas de valor para orientar em qual
direção ou trajetória da árvore devemos seguir para maximizar a recompensa na árvore de busca. Em
outras palavras, MCTS presta mais atenção em nodos que são mais promissores, fazendo não seja
necessário acessar todos os nodos da árvore para valorar um estado.</p>
<p>Basicamente, esse método consiste em quatro etapas bem separadas: seleção, expansão, simulação
e <em>backup</em>.</p>
<p>No passo de simulação, utilizamos <em>tree policy</em> para construir um caminho da raiz até o nodo folha
mais promissor. <em>Tree policy</em> é utilizada para selecionar a melhor ação a partir de um estado. No
AlphaGo, o cálculo dessa política se dá através do valor UCB, que calcula o grau de confiança de
uma determinada ação ser a mais promissora. Abaixo está representado o cálculo desse valor.</p>
<p>\[
\large{} UCB (node _i) = V _i + 2 \sqrt{ \frac{\ln N}{n _i} }
\]</p>
<p>onde, \( V _i \) é a média de recompensa de todos os valores abaixo de \( node _i \). \( N \) é o número de vezes que
o pai de \( node _i \) foi visitado e \( n _i \) é o número de vezes que \( node _i \) foi visitado.</p>
<p>Com isso, percebe-se que quanto mais um nodo \( i \) é visitado, menor é o valor UCB, diminuindo
a probabilidade desse nodo ser selecionado novamente. Assim, usa-se técnicas de <em>exploration</em> e
<em>exploitation</em>, vistas na Seção <a href="parte-6/6/../../parte-4/4/4-4.html">Aprendizado por reforço (<em>Reinforcement learning</em>)</a>
para a valoração dos estados através do valor UCB.</p>
<p>Na etapa de expansão, apenas selecionamos um nodo de forma aleatória para ser explorado.</p>
<p>Na etapa de simulação, simulamos uma ou mais jogadas para verificar a recompensa acumulada
de cada ação. Por exemplo, para cada simulação teríamos valores acumulados referentes à vitória,
derrota ou empate.</p>
<p>Na etapa de <em>backup</em> usamos os valores de recompensa acumulados das ações tomadas e estados atingidos na etapa de
simulação, propagando esses valores em direção ao nodo raiz a fim de possibilitar
a realização da melhor transição de estado.</p>
<p>A Figura 101 abaixo exemplifica de forma conjunta cada uma dessas etapas.</p>
<p align="center">
  <img src="parte-6/6/./img/101.png">
</p>
<p align="center">
  Figura 101: Exemplificação das etapas de seleção, expansão, simulação e backup
</p>
<p>Com isso, percebe-se que MCTS é uma ferramenta extremamente poderosa, especialmente quando
utilizada juntamente com métodos como DQN. MCTS valora as recompensas de cada estado sem
necessariamente percorrer toda a árvore para decidir a melhor jogada e DQN computa o melhor valor
Q de um determinado estado atingido por determinada ação.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="referências"><a class="header" href="#referências">Referências</a></h1>
<p>[1] <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">Vinod Nair Alex Krizhevsky e Geoffrey Hinton. THE CIFAR-10 DATABASE. 2012.</a></p>
<p>[2] <a href="http://cs230.stanford.edu/">Kian Katanforoosh Andrew Ng. Stanford University CS230 Deep Learning.</a></p>
<p>[3] <a href="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros e Geoffrey E. Hinton. Layer Normalization. 2016.</a> [stat.ML].</p>
<p>[4] Richard Bellman. Dynamic Programming. Dover Publications, 1957. isbn: 9780486428093.</p>
<p>[5] <a href="http://web.stanford.edu/class/cs234/index.html">Emma Brunskill. Stanford University CS234 Reinforcement Learning.</a></p>
<p>[6]
<a href="http://web.stanford.edu/class/cs224n/index.html#schedule">John Hewitt Christopher Manning. Stanford University CS224n: Natural Language Processing with Deep Learning.</a></p>
<p>[7] <a href="http://cs231n.stanford.edu/">Serena Yeung Fei-Fei Li Justin Johnson. Stanford University CS231n: Convolutional Neural Networks for Visual Recognition.</a></p>
<p>[8] <a href="https://arxiv.org/abs/1512.03385">Kaiming He et al. Deep Residual Learning for Image Recognition. 2015</a> [cs.CV]</p>
<p>[9] <a href="http://yann.lecun.com/exdb/mnist/">Yann LeCun. THE MNIST DATABASE of handwritten digits. 1998</a></p>
<p>[10] <a href="https://arxiv.org/abs/0712.3329">Shane Legg e Marcus Hutter. Universal Intelligence: A Definition of Machine Intelligence. 2007.</a></p>
<p>[11]
<a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">DeepMind &amp; University College London. Reinforcement learning course 2020.</a></p>
<p>[12] Tom M. Mitchell. Machine Learning. New York: McGraw-Hill, 1997. isbn: 978-0-07-042807-2.</p>
<p>[13] <a href="http://cs229.stanford.edu/">Andrew Ng. Stanford University CS229 Machine Learning.</a></p>
<p>[14] <a href="https://www.coursera.org/learn/machine-learning?">Andrew Ng. Stanford University Machine Learning.</a></p>
<p>[15] Alec Radford et al. “Language Models are Unsupervised Multitask Learners”. Em: (2019).</p>
<p>[16] <a href="https://arxiv.org/abs/1811.12808">Sebastian Raschka. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. 2020.</a> [cs.LG].</p>
<p>[17] <a href="https://doi.org/10.1109/WSC.2008.4736059">Samik Raychaudhuri. “Introduction to Monte Carlo simulation”. Em: 2008 Winter Simulation Conference. 2008, pp. 91–100.</a></p>
<p>[18] <a href="https://doi.org/10.1147/rd.33.0210">A. L. Samuel. “Some Studies in Machine Learning Using the Game of Checkers”. Em: IBM Journal of Research and Development 3.3 (1959), pp. 210–229.</a></p>
<p>[19] <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">David Silver et al. “Mastering the game of Go with deep neural networks and tree search”. Em: Nature 529 (2016), pp. 484–503.</a></p>
<p>[20] <a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF">Sander Dieleman Viorica Patraucean James Martens Marta Garnelo Felix Hill Alex Graves Andriy Mnih Mihaela Rosca Irina Higgins Jeff Donahue Iason Gabriel Chongli Qin Thore Graepel Wojtek Czarnecki. DeepMind &amp; University College London Deep learning lecture series 2020.</a></p>
<p>[21] <a href="https://arxiv.org/abs/1706.03762">Ashish Vaswani et al. Attention Is All You Need. 2017.</a> [cs.CL].</p>
<p>[22] <a href="https://doi.org/10.1109/CDC.1989.70114">P.J. Werbos. “Neural networks for control and system identification”. Em: Proceedings of the 28th IEEE Conference on Decision and Control, 1989, 260–265 vol.1.</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contatos"><a class="header" href="#contatos">Contatos</a></h1>
<p>Caso queira mandar um feedback sobre o curso, uma sugestão de incrementação, ou reportar algum erro,
você pode entrar em contato conosco por email (<a href="mailto:pet@inf.ufrgs.br">pet@inf.ufrgs.br</a>).</p>
<h2 id="redes-sociais"><a class="header" href="#redes-sociais">Redes sociais</a></h2>
<p>LinkedIn: <a href="https://www.linkedin.com/company/petcompufrgs">https://www.linkedin.com/company/petcompufrgs</a></p>
<p>Facebook: <a href="https://www.facebook.com/PETCompUFRGS">https://www.facebook.com/PETCompUFRGS</a></p>
<p>Instagram: <a href="https://www.instagram.com/petcompufrgs/">https://www.instagram.com/petcompufrgs</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
