# Regressão linear regularizada

Podemos utilizar o conceito de regularização para evitar problemas de _overfitting_ no método de
regressão linear.

É possível modificar o método gradiente descendente de forma que consigamos regularizar a
atualização do valor de \\( \theta \\) conforme visto na Seção [Gradiente Descendente (_Gradient Descent_)](../4/2-4.md):

---

**Algorithm 5** Algoritmo Gradiente Descendente Para Regressão Linear Regularizado

---

1: **procedure**

2: &emsp; **repeat**

3: &emsp;&emsp;
\\(
  \large{} \theta _0 := \theta _0 - \frac{\alpha}{m} \sum _{i=1} ^m
    \Big( h _{\theta}(x ^{(i)}) - y ^{(i)} \Big) \cdot x _0 ^{(i)}
\\)

4: &emsp;&emsp;
\\(
  \large{} \theta _j := \theta _j - \Big[ \Big( \frac{1}{m} \sum _{i=1} ^m
    (h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)} \Big) + \frac{\lambda}{m} \theta _j \Big]
\\)
&emsp; \\( \rhd j \in 1,2, \dots , n \\)

5: &emsp; **until** \\( convergir \\)

6: **end procedure**

---

Podemos perceber que atualizamos o valor de \\( \theta _0 \\) separadamente a fim de focarmos apenas nos termos
de maior grau.

Além disso, podemos representar o mesmo algoritmo através da equação normal - descrita na Seção
[Equação Normal (Normal Equation)](../7/2-7.md) - da seguinte forma:

\\[
  \large{} \theta = (X ^T X + \lambda \cdot L) ^{-1} X ^T y
\\]

\\[
  \large{} onde \\ L =
    \begin{bmatrix}
      0 && 0 && 0 && 0 && \dots && 0 \\\\
      0 && 1 && 0 && 0 && \dots && 0 \\\\
      0 && 0 && 1 && 0 && \dots && 0 \\\\
      0 && 0 && 0 && 1 && \dots && 0 \\\\
      0 && 0 && 0 && 0 && \ddots && 0 \\\\
      0 && 0 && 0 && 0 && \dots && 1
    \end{bmatrix}
\\]
