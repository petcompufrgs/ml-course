# Regressão logística regularizada

Podemos regularizar a regressão logística da mesma forma que regularizamos a função \\( J \\) para a
regressão linear. Com isso, podemos evitar casos de _overfitting_ no método de regressão logística.

Assim, podemos escrever a função \\( J(\theta) \\) da seguinte forma:

---

\\[
  J(\theta) =- \frac{1}{m} \sum _{i=1} ^m \Big[ y ^{(i)} \log (h _{\theta}(x ^{(i)})) +
    (1-y ^{(i)}) \log (1-h _{\theta}(x ^{(i)})) \Big] + \frac{\lambda}{2m} \sum _{j=1} ^n \theta _j ^2
\\]

---

Com isso, da mesma forma que na regressão linear, podemos escrever o algoritmo gradiente
descendente com a regularização da função \\( J \\).

---

**Algorithm 6** Algoritmo Gradiente Descendente Para Regressão Logística Regularizado

---

1: **procedure**

2: &emsp; **repeat**

3: &emsp;&emsp;
\\( \large{} \theta _0 := \theta _0 - \frac{\alpha}{m} \sum _{i=1} ^m
  \Big( h _{\theta}(x ^{(i)}) - y ^{(i)} \Big) \cdot x _0 ^{(i)} \\)

4: &emsp;&emsp;
\\( \large{} \theta _j := \theta _j - \alpha \Big[ \frac{1}{m} \sum _{i=1} ^m
  (h _{\theta}(x ^{(i)})-y ^{(i)}) \cdot x _j ^{(i)} + \frac{\lambda}{m} \theta _j \Big] \\)
  &emsp; \\( \rhd j \in 1,2, \dots , n \\)

5: &emsp; **until** \\( convergir \\)

6: **end procedure**

---
