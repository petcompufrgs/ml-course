# Gradiente Descendente com múltiplas variáveis

Da mesma forma apresentada na Seção [Função Custo (_Cost Function_)](../3/2-3.md), teremos as
seguintes predefinições das funções que serão utilizadas a fim de minimizar o custo da função
\\( J \\) com múltiplas variáveis de entrada.

- Hipótese:
\\( h _{\theta}(x) = \theta _0 + \theta _1 x _1 + \theta _2 x _2 + \theta _3 x _3 + \dots + \theta _n x _n \\);

- Parâmetros:
\\( \theta _0, \theta _1, \dots , \theta _n \\) (vetor (n+1)-dimensional);

- Função Custo:
\\( J(\theta _0, \theta _1, \dots , \theta _n)= \frac{1}{2m} \sum _{i=1} ^m (h _{\theta}(x _i) - y _i) ^2 \\)

O algoritmo gradiente descendente, em geral, tem a mesma estrutura para os diferentes problemas.
Nós apenas temos que iterar sobre os \\( n \\) parâmetros de entrada, atualizando-os simultaneamente.

---

**Algorithm 3** Algoritmo Gradiente Descendente Para Múltiplas Variáveis

---

1: **procedure**

2: &emsp; **repeat**

3: &emsp;&emsp; \\( \theta _j := \theta _j - \alpha \frac{1}{m} \sum _{i=1} ^m
  (h _{\theta}(x ^{(i)}) - y ^{(i)}) \cdot x _j ^{(i)} \\)
  &emsp; \\( \rhd \\) para \\( j := 0 \dots n \\)

4: &emsp; **until** \\( convergir \\)

5: **end procedure**

---

Podemos otimizar o algoritmo gradiente descendente colocando todos os parâmetros no mesmo
intervalo. Esse tipo de operação é muito importante para evitar que o gradiente trabalhe com
valores muito grandes que possam causar "explosão", ou muito pequenos que possam ser zero, o
que pode se tornar um problema para o treinamento do algoritmo de aprendizado.

Para isso, utilizaremos da técnica de normalização média (_mean normalization ou feature scaling_).
_Feature scaling_ envolve dividir os dados de entrada por um intervalo (desvio padrão, ou uma subtração
entre o maior e o menor valor das variáveis de entrada) resultando em um valor próximo de 1. _Mean normalization_
envolve subtrair o valor médio das variáveis de entrada de cada variável de entrada.
Dessa forma, teremos a seguinte fórmula:

---

\\[
  \large{} x _i = \frac{x _i - \mu _i}{s _i}
\\]

---

onde \\( \mu _i \\) é a média de todos os parâmetros de entrada e \\( s _i \\) é a variação dos valores de entrada
\\( max - min \\), ou o desvio padrão.

Com isso, teremos os valores de entrada em um mesmo intervalo, sem valores discrepantes para o
cálculo da função custo e da otimização pelo gradiente descendente. Com os valores normalizados,
evitamos diversos problemas que foram acima mencionados e o custo computacional para o cálculo
das derivadas parciais é diminuído.

Para ter certeza que o método do gradiente descendente está funcionando corretamente, o valor de
\\( J(\theta) \\) deve diminuir a cada iteração e convergir para um valor próximo de zero. Caso o valor de
\\( J(\theta) \\) não esteja convergindo, podemos tentar um valor menor de \\( \alpha \\).
