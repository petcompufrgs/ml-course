# Gradiente Descendente para Regressão Logística

Da mesma forma que o método de regressão linear, o algoritmo do gradiente descendente funciona
iterando sobre os valores de \\( \theta \\) e derivando a função custo \\( J \\) em relação a \\( \theta \\).

Assim, podemos descrever o método através do seguinte algoritmo:

---

**Algorithm 4** Algoritmo Gradiente Descendente Para Regressão Logística

---

1: **procedure**

2: &emsp; **repeat**

3: &emsp;&emsp;
\\( \theta _j := \theta _j - \frac{\alpha}{m} \sum _{i=1} ^m (h _{\theta}(x ^{(i)})-y ^{(i)}) \cdot x _j ^{(i)} \\)
&emsp; \\( \rhd \\) Atualiza simultaneamente todos \\( \theta _j\\)

4: &emsp; **until** \\( convergir \\)

5: **end procedure**

---

ou através da forma vetorizada:

\\[
  \large{} \theta := \theta - \frac{\alpha}{m} X ^T (g(X \theta)- \overset{\rightarrow}y)
\\]

Dessa forma, podemos utilizar dos métodos vistos anteriormente para implementar o algoritmo de
regressão logística. Contudo, ainda podemos utilizar de técnicas de computação numérica para
otimizar os algoritmos. Como por exemplo _Conjugate gradient_, _BFGS_ e _L-BFGS_ que podemos
utilizar a fim de otimizar o método de gradiente descendente.
