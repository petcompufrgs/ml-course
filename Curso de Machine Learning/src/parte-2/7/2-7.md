# Equação Normal (_Normal Equation_)

É um outro método de minimização da função \\( J \\), assim como o método de gradiente descendente.
Essa forma de implementação, muitas vezes pode otimizar o tempo de processamento da função
de minimização através de derivadas da função \\( J \\) a respeito aos \\( \theta ' _j  \\)s igualando-os a zero. Isso nos
permite encontrar o valor ótimo para \\( \theta \\) sem iterações. Fórmula da equação normal é dada abaixo:

\\[
  \large{} \theta = (X ^T X) ^{-1} X ^T y
\\]

onde \\( X \\) é uma matriz na qual a coluna zero tem todos os elementos iguais a 1.

Dessa forma, podemos comparar as duas formas de implementação que temos: _Gradient Descent_ e
_Normal Equation_.

<p align="center">
Tabela 1: Comparação entre os métodos Gradient Descent e Normal Equation
</p>

| Gradient Descent                                  | Normal Equation                                                   |
|--------------------------------------------------:|:------------------------------------------------------------------|
| É necessário definir o valor de \\( \alpha \\)    | Não é necessário definir o valor de \\( \alpha \\)                |
| Muitas iterações são necessárias                  | Não são necessárias iterações                                     |
| \\( O(kn ^2) \\)                                  | \\( O(n ^3) \\) pois precisa calcular a inversa de \\( X ^T X \\) |
| Funciona bem quando o valor de \\( n \\) é grande | Lento quando o valor de \\( n \\) é grande                        |
