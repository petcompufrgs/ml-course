# Organização do conhecimento

Como foi visto até agora, nós temos três tipos de estruturas básicas em uma arquitetura de uma
rede neural:

- Número de unidades de entrada (_input units_): dimensão do vetor de entradas \\( x \\);

- Número de unidades de saída (_output units_): dimensão do vetor de saída (classes);

- Número de unidades intermediárias (_hidden units_) por camada: são definidas a partir da
complexidade do problema e, geralmente, quando temos mais de uma camada, cada uma delas
devem ter o mesmo número de unidades.

A partir dessas definições mencionadas, podemos, novamente, estrutural a lógica que deve ser seguida
ao realizarmos o treino de uma rede neural. Abaixo, estão divididos em passos o algoritmo de treino
utilizando _backpropagation_.

1. Aleatoriamente inicializar os pesos \\( \Theta _{ij} ^{(l)} \\) (Seção [Inicialização aleatória](./3-2-6.md));

2. Implementar o método de _forward propagation_ para computar os valores da função hipótese
\\( h _{\Theta}(x ^{(i)}) \\) para todos os valores de \\( x ^{(i)} \\) (Seção [Redes Neurais: Aprendizado](./3-2.md));

3. Implementar a função custo (Seção [Função Custo (Cost Function)](./3-2-2.md));

4. Implementar o método de *backpropagation* para computar os valores das derivadas
parciais \\( \frac{\partial}{\partial \Theta} \\) (Seção [Backpropagation Algorithm](./3-2-3.md));

5. Utilizar o método de verificação do gradiente para confirmar que o método de _backpropagation_
está funcionando corretamente. Após executar uma vez, desabilitamos a verificação
(Seção [Verificação do gradiente](./3-2-5.md));

6. Utilizar o algoritmo de gradiente descendente ou algum outro método de minimização mais
otimizado para minimizarmos a função custo utilizando os valores de teta (Seção [Otimizadores](./3-2-4.md)).
