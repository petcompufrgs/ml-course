# _Backpropagation Algorithm_

O algoritmo de _Backpropagation_ é, sem dúvida, o algoritmo mais importante para as redes neurais.
É com esse algoritmo que as redes neurais aprendem, essencialmente.

Como foram vistos nas seções anteriores, existem algoritmos que são usados com o objetivo de
minimizar a função custo. Para redes neurais, usa-se um algoritmo chamado _Backpropagation_ que
tem o mesmo objetivo: minimizar a função custo, ou seja \\( \underset{\Theta}{min} J(\Theta) \\).
A minimização ocorre após realizarmos o processo de _forward propagation_ usando o método de gradiente
descendente - visto na Seção [Gradiente Descendente (Gradient Descent)](../../parte-2/4/2-4.md) -,
atualizando os valores das _hidden layers_ de acordo com os valores retornados da função custo.

Em geral, o algoritmo de _backpropagation_ acontece em duas fases principais que serão discutidas em
detalhes a seguir. Essas fases são:

1. **_Forward pass_**: nossas entradas são passadas através da rede e as previsões de saída são
obtidas. Nessa fase, calculamos a função custo e computamos as funções de ativação para
cada transição de camada.

2. **_Backward pass_**: calculamos o gradiente da função custo na camada final (_output layer_) e
usamos esse gradiente para aplicar recursivamente a regra da cadeia para atualizar os pesos da
rede neural.
