# _Backpropagation_

O algoritmo de _backpropagation_ é feito a cada ponto em um período de tempo. No tempo \\( T \\),
da derivada da função custo \\( \mathcal{L} \\) a respeito de uma matriz de pesos \\( W \\) é expressa como segue:

\\[
  \large{} \frac{\partial \mathcal{L} ^{(T)}}{\partial W} =
    \sum _{t=1} ^{T} \frac{\partial \mathcal{L} ^{(T)}}{\partial W} \Bigg| _{(t)}
\\]

Para realizar essa computação, realizamos as atualizações dos parâmetros _feedforward_, a cada passo
de tempo, e ao computar a função custo, realizamos a _backpropagation_. Contudo, como esse tipo
de rede neural processa os dados sequencialmente de forma a computar cada derivada para cada
período de tempo. Esse algoritmo é chamado de _backpropagation through time_ [[22]](../../referencias.md).

Realizar o cálculo das derivadas de forma sequencial pode ser uma tarefa pouco eficiente. A computação
das derivadas parciais são realizadas de forma _end-to-end_, o que, muitas vezes pode causar
diversos problemas como desaparecimento e explosão do gradiente que serão explicados em detalhes
na Seção [Gradiente de desaparecimento e explosão](./6-2-8.md).
