# Visão geral

Tanto o mundo real quanto o mundo digital é composto por uma quantidade enorme de informações
e tudo isso está disponível de forma acessível. Todavia, é um problema desenvolver modelos e
algoritmos que possam analisar e entender essa quantidade absurda de dados.

Nesse sentido, os modelos generativos são uma das abordagens mais promissoras no subconjunto
de _unsupervised learning_. Para treinar esse tipo de modelo, coletamos uma grande quantidade de
dados de algum domínio (e.g. imagens, frases, sons, etc.) que chamaremos de \\( p _{data}(x) \\) e, então,
treinamos um modelo para gerar dados semelhantes aos de entrada, ou seja, geraremos \\( p _{model}(x) \\)
tal que é semelhante a \\( p _{data}(x) \\).

A principal ideia é que as redes neurais que usamos como modelos generativos têm um número de
parâmetros significativamente menor do que a quantidade de dados em que os treinamos, de modo
que os modelos são forçados a descobrir e internalizar com eficiência a essência dos dados para
gerá-los.
