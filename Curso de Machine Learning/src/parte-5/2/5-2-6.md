# AdaGrad

Outro método de otimização é chamado de _Adaptive Gradient Algorithm_ (Adagrad). Esse método
adapta a taxa de aprendizado aos parâmetros, realizando atualizações menores nos parâmetros,
diminuindo o "barulho" das atualizações.

Como não precisamos definir uma taxa de aprendizado \\( \alpha \\), a convergência é mais rápida e confiável. O
método adapta as atualizações e a taxa de aprendizado automaticamente de acordo com os níveis da
superfície, impossibilitando que as atualizações sejam muito oscilantes em superfícies mais instáveis.

Essa atualização, ocorre da seguinte forma:

\\[
  \large{} \theta _{t+1,i} = \theta _{t,i} - \frac{\eta}{\sqrt{G _{t,ii}} + \epsilon} \cdot g _{t,i}
\\]

Atualizamos os parâmetros \\( \theta \\), multiplicando a taxa de aprendizado \\( \eta \\) pelo gradiente do ponto, e
dividimos esse valor pela raiz quadrada dos somatórios dos quadrados dos gradientes para cada
iteração \\( t \\). Além disso, \\( \epsilon \cong 10 ^{-7} \\).
