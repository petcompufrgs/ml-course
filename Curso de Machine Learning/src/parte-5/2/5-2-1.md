# _Batch Normalization_

Quando falarmos em normalização, estamos nos referindo à compactação de uma gama diversa
de números em uma gama fixa, como foi visto, por exemplo, na Seção [Distribuição Gaussiana](../1/5-1-2.md)
quando falamos de Distribuição Normal.

Por exemplo, quando temos entradas com uma variância muito alta, desejamos normalizar esses
valores a fim de gerarmos uma distribuição mais simples de trabalhar, colocando os valores em um
mesmo alcance. Isso se chama, _input normalization_.

Redes neurais profundas, geralmente possuem uma sequência de camadas que, muitas vezes, somam
milhares de neurônios. De camada a camada, esses valores podem se distinguir através das sequencias
operações, dificultando que as camadas da rede neural consigam fazer o trabalho de manter esses
valores bem distribuídos e estáveis.

Outro problema é quando a escala de diferentes parâmetros da rede neural está vinculada às atualizações
do gradiente, o que pode levar ao problema de explosão e desaparecimento dos gradientes.
Esses problemas estão relacionados, respectivamente ao crescimento descontrolado e infinito dos
valores do, ou à queda descontrolada tendendo à zero dos valores do gradiente.

Geralmente, para evitar esse tipo de problema, desejamos desacoplar a escala dos parâmetros da
rede neural das atualizações do gradiente e, para isso, podemos usar funções de ativação mais
robustas e estáveis, como por exemplo, ReLU (Seção
[Camada de ativação: Rectified Linear layer (ReLU)](../../parte-3/2/3-2-1-5.md)), escolhendo taxas de aprendizado
ótimas e inicializando os parâmetros das redes neurais cuidadosamente. Todavia, quanto mais tarefas
adicionamos, aumentamos a complexidade da rede neural, o que, muitas vezes, pode ser um problema.

Para solucionar esses problemas, utilizamos um método chamado _Batch Normalization_. Esse método
tem como objetivo evitar gradientes instáveis, reduzir os efeitos da inicialização da rede neural na
convergência do algoritmo de otimização e possibilitar o uso de taxas de aprendizado mais rápidas
para acelerar a convergência do algoritmo de otimização do gradiente.

Para isso, devemos normalizar cada camada intermediária da rede neural de acordo com as entradas
de cada camada, calculando a média e o desvio padrão de cada uma delas.

Assim, podemos finalmente descrever o algoritmo de normalização _Batch Normalization_.

---

**Algorithm 9** Algoritmo _Batch Normalization_ (BN)

---

1: **procedure** (Valores de \\( x \\) sobre um _mini-batch_: \\( \mathcal{B} = \{ x _1 , \dots , m \} \\)),
Parâmetros a serem aprendidos: \\( \gamma , \beta \\)

2: &emsp; \\( \large{} \mu _{\mathcal{B}} \leftarrow \frac{1}{m} \sum _{i=1} ^m x ^i \\)
&emsp;&emsp; \\( \rhd \\) Média _mini-batch_

3: &emsp; \\( \large{} \sigma _{\mathcal{B}} ^2 \leftarrow \frac{1}{m} \sum _{i=1} ^m (x _i - \mu _{\mathcal{B}}) ^2 \\)
&emsp;&emsp; \\( \rhd \\) Variância _mini-batch_

4: &emsp; \\( \large{} x̂ _i \leftarrow \frac{x _i - \mu _{\mathcal{B}}}{\sqrt{\sigma _{\mathcal{B}} ^2 + \epsilon}} \\)
&emsp;&emsp; \\( \rhd \\) Normalização

5: &emsp; \\( \large{} y _i = \gamma x̂ _i + \beta \equiv BN _{\gamma , \beta} (x _i) \\)
&emsp;&emsp; \\( \rhd \\) Escala e _shift_

6: **end procedure**

---

No algoritmo acima, as primeiras três equações calculam a média e o desvio padrão e depois normalizam
as entradas, respectivamente. O valor de \\( \epsilon \\) é um número que ajuda na estabilidade numérica,
evitando que seja efetuada uma divisão por zero. A principal característica desse algoritmo é que a
normalização acontece para cada um dos inputs no _batch_ separadamente.

Na última equação, temos dois novos parâmetros, \\( \gamma \\) e \\( \beta \\), que representam respectivamente o
_scaling_ e o _shift_ dos valores. Isso limita o alcance dos valores, facilitando com que a próxima camada trabalhe
com eles de forma mais simples. Esses valores são aprendidos pela rede neural e, assim, podemos
ajustar esses valores de acordo com a distribuição.
