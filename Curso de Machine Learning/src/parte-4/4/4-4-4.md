# _Markov Decision Processes_ (MDPs)

MDP é um dos tópicos mais importantes para o entendimento de RL. MDP é um método que resolve
a maioria dos problemas de RL com tomadas de decisões discretas. Com esse método, um agente
atinge uma política ótima para receber o maior número de recompensas ao longo do processo de
exploração do ambiente.

Como foi apresentado na seção anterior na qual discutimos as definições de Propriedade e Cadeia
de Markov, uma Cadeia de Markov trabalha com as transições entre os estados e as probabilidades
de transição relacionadas aos mesmos. MDP difere da Cadeia de Markov pois, agora, os estados
não dependem apenas do estado imediatamente posterior, mas sim, de todas as ações que foram
executadas a partir do estado atual.

Com isso, o objetivo principal do MDP é treinar um agente a fim de encontrar a melhor política
possível acumulando o maior número de recompensas a partir de tomadas de decisões em um ou
mais estados. Podemos definir, formalmente MDP da seguinte forma:

MDP é uma 5-upla \\( (S,A,P,R, \gamma) \\), onde:

- \\( S: \\) conjunto de estados;

- \\( A: \\) conjunto de ações;

- \\( P(s, a, s'): \\) probabilidade de uma ação \\( a \\) no estado \\( s \\) levar ao estado
\\( s' \\) no tempo \\( t+1 \\);

- \\( R(s, a, s'): \\) recompensa recebida imediatamente após a transição do estado \\( s \\) para o
\\( s' \\) a partir de uma ação \\( a \\);

- \\( \gamma \\): fator de desconto o qual é usado para gerar uma recompensa relativa. Esse valor está entre
0 e 1 e quantifica a diferença de importância de recompensas imediatas e recompensas futuras.

Em outras palavras, MDP pode ser escrito como

\\[
  \large{} \sum _{t=0} ^{t= \infty} \gamma ^t r(x(t), a(t))
\\]
