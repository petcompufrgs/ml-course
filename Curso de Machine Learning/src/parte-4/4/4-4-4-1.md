# Busca pela política ótima com MDP

Com MDP podemos fazer com que o nosso agente selecione a decisão ótima para determinado estado
de ambiente. Iremos maximizar a recompensa do agente ao longo do tempo a fim de fazer com que
ele atinja a política ótima, i.e. determinaremos qual é a melhor ação a ser tomada em cada estado.

Para determinar a melhor ação a ser tomada, iremos usar a equação de Bellman
(_Bellman Optimality Equation_ [[4]](../../referencias.md)) que nos possibilita estimar o valor ótimo
para cada estado. A equação estima o valor de um estado computando as recompensas esperadas que cada estado pode gerar.

Abaixo está definida a equação de Bellman recursiva

\\[
  \large{} V ^*(s) = max _a \sum _{s'} P(s,a,s')[R(s,a,s') + \gamma V(s')]
\\]

Onde:

- \\( P(s,a,s') \\) é a probabilidade da transição do estado \\( s \\) para o \\( s' \\) a partir da escolha
da ação \\( a \\);

- \\( R(s,a,s') \\) é a recompensa imediata do estado \\( s \\) para o \\( s' \\) quando o agente escolhe a
ação \\( a \\);

- \\( \gamma \\) é o fator de desconto (recursivo).
