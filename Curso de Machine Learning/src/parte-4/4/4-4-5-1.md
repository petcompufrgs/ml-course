# Valoração de Monte-Carlo

O método de Monte-Carlo faz com que o agente aprenda interagindo com o ambiente e coletando
diversas amostras. Essa coleção de amostras estão relacionadas as distribuições de probabilidades
mencionadas \\( R(s,a,s') \\) e \\( R(s,a,s') \\).

Entretanto, Valoração de MC é uma forma de aprendizado baseado em testes, em outras palavras,
um MDP sem a tupla \\( P   \\) pode aprender por tentativa e erro, por meio de muitas repetições.

Nesse cenário, cada "tentativa" é chamado de episódio, e ele termina quando o estado final do MDP
é atingido. No final, todos os valores de recompensa são atualizados na recompensa \\( G _t \\), final de cada
estado. Abaixo está representada a equação de Monte-Carlo que atualiza o valor de cada estado.

\\[
  \large{} V(s _t) \leftarrow V(S _t) + \alpha [G _t - V(S _t)]
\\]

Onde:

- \\( V(S _t) \\) é o valor do estado que desejamos estimar. Esse valor pode ser inicializado
aleatoriamente ou baseado em alguma estratégia;

- \\( G _t \\) pode ser calculado da seguinte forma, onde T é o tempo de terminação:

\\[
  \large{} G _t = R _{t+1} + \gamma R _{t+2} + \gamma ^2 R _{t+3} + \dots + \gamma ^{T-1} R _T
\\]

- \\( \alpha \\) é um parâmetro que influencia a convergência.

Sendo assim, temos diversas formas de atualizarmos os valores dos estados do ambiente. Essas
atualizações são significantes para determinar a política do agente e otimizar a escolha da ação.
Essas formas estão relacionadas ao momento de visita de cada estado, ou seja, como devemos
atualizar o valor de um estado na primeira visita e em cada vez que atingimos o mesmo estado.
