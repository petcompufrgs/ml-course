# Otimizadores

Em algumas implementações do gradiente descendente, podemos encontrar diferentes formas de
otimizações. A seguir estão listadas algumas formas de implementações.

- Gradient Descent;

- Stochastic Gradient Descent;

- Mini-Batch Gradient Descent;

- Momentum;

- Nesterov Accelerated Gradient;

- AdaGrad;

- Adam;

Esses sistemas de otimização serão abordados e detalhados em seções seguintes.
