# Funções de ativação e propriedades

Como vimos nas seções anteriores, para cada transição de camada, precisamos de uma função de
ativação para atualizar os valores dos respectivos nodos da camada seguinte a partir dos nodos da
camada anterior. Para RNNs, temos basicamente três tipos de funções de ativação:

1. _Sigmoid_:

\\[
  \large{} g(z) = \frac{1}{1 + e ^{-z}}
\\]

2. tanh:

\\[
  \large{} g(z) = \frac{e ^z - e ^{-z}}{e ^z + e ^{-z}}
\\]

3. _ReLU_:

\\[
  \large{} g(z) = max(0, z)
\\]
