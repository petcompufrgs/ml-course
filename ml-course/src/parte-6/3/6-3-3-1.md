# _Masked Multi-head attention_

Como o nosso objetivo, neste caso, é prever a próxima palavra da sequência, a camada de
_masked multi-head attention_ tem como objetivo ”ensinar” o _decoder_ a prever a melhor palavra a ser retornada.
Para isso, atualizamos os valores da matriz de atenção de forma que o modelo preste atenção apenas
nas palavras que foram vistas anteriormente a fim de gerar a melhor que se relacione semanticamente
com elas. Em outras palavras:

---

\\[
  \large{} MaskedAttention (Q,K,V) = softmax \Big( \frac{QK ^T + M}{\sqrt{d _k}} \Big) V
\\]

---

onde a matriz M é uma matriz de zeros e \\( - \infty \\).

Os zeros irão se tornar os valores do produto \\( QK ^T \\) e os \\( - \infty \\) irão se tornar zero.
Esse tipo de máscara evita que o modelo preste atenção nas palavras futuras da sentença, focando nas
palavras anteriores, que é o que realmente interessa.
