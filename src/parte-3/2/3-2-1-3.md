# _Cross entropy loss_

_Cross entropy loss_ tem como objetivo calcular o quão discrepante do valor esperado está o valor
produzido como saída na rede neural. Como foi visto na Seção [Função Custo (Cost Function)](../../parte-2/9/2-9-1.md).
Essa função tem a seguinte estrutura:

---

\\[
  \large{}
    \mathcal{L} _{CE}(y,g(z))= - \frac{1}{m} \Big[ \sum _{j=1} ^m y \log (g(z)) +
      (1-y) \log (1-g(z)) \Big]
\\]

---

Essa função tem o mesmo objetivo da função custo anteriormente vista e que será detalhada na
Seção [Função Custo (Cost Function)](./3-2-2.md).
