# TD _Learning_

Como vimos na seção anterior, o método de Monte-Carlo exige que nós esperamos até o fim do
episódio para determinar o valor do estado \\( V(S _t) \\). No método de *Temporal-Difference*, esperamos
até o próximo passo a ser tomado. Em outras palavras, no tempo \\( t+1 \\), o método TD utilizado da
recompensa observada \\( R _{t+1} \\) e, imediatamente, gera um valor, chamado *TD target*
\\( R(t+1)+V(S _{t+1}) \\), atualizando o valor do estado \\( V(S _t) \\) através do erro,
chamado *TD error* \\( R(t+1)+V(S _{t+1})-V(S _t) \\).

Com isso, podemos definir a equação \\( TD(\lambda) \\) na qual substituímos o valor de *TD target* por
\\( G _t \\) da seguinte forma:

\\[
  \large{} V(S _t) \leftarrow V(S _t) + \alpha [G _t ^{(\lambda)} - V(S _t)]
\\]

onde:

\\[
  \large{} G _t ^{(\lambda)} = (1- \lambda) \sum _{n=1} ^{\infty} \lambda ^{n-1} G _t ^{n}
\\]

Assim, com essas duas equações, Monte-Carlo(MC) e *Temporal-Difference*(TD) podemos definir um
método de aprendizado por reforço que é uma combinação das duas, chamado de \\( Q-Learning \\).
