# _Q-Learning_

*Q-Learning* é uma combinação entre os métodos de MC e TD vistos na seção anteriores. Nesse
método, a cada passo, tomamos uma decisão gananciosa que tem como objetivo maximizar o valor
da variável \\( Q(S _{t+1},a) \\), como está representado abaixo:

---

\\[
  \large{} Q(S _t,a) \leftarrow Q(S _t,a) + \alpha [R _{t+1} + \gamma max _a Q(S _{t+1},a) - Q(S _t,a)]
\\]

---

Para a implementação desse método, precisamos de dois fatores principais: a atualização do valor
de \\( Q \\) - chamado de _Q-value_ e um lugar para salvar esses valores, chamado de _Q-table_.

Em _Q-learning_, a cada passo tomamos a melhor decisão possível para o agente, ou seja, a decisão
que terá o melhor impacto na atualização de _Q-value_. Esse tipo de tomada de decisão se chama
\\( \epsilon \\)*-greedy-policy*, onde \\( 0 \< \epsilon \< 1 \\) é o grau de ganância do nosso agente,
em outras palavras, quanto mais alto o seu valor, maior a quantidade de ações realizadas aleatoriamente.

No início da exploração, o agente toma decisões puramente aleatórias a fim de conhecer o ambiente
no qual ele está inserido. E a cada tomada de decisão, atualiza-se o valor de cada estado _Q-value_
na tabela *Q-table*. Portanto, é comum começar a exploração com um valor alto de \\( \epsilon \\), como, por
exemplo, igual a 1, a fim de fazer com que o agente tome decisões 100% aleatórias.

Ao longo da exploração, o agente começa a conhecer melhor o ambiente, então podemos diminuir o
valor de \\( \epsilon \\) gradativamente ao longo do treino.

Contudo, a utilização desse tipo de implementação não é escalável, ou seja, para modelos extremamente
complexos, é extremamente ineficiente a valoração dos estados e a busca pela política ótima.
Para resolver esse tipo de problema, métodos derivados foram desenvolvidos utilizando redes neurais
e o principal deles se chama _Deep Q-Learning_ que será discutido na Seção [_Deep Q-Learning_](../../parte-6/5/6-5.md).
